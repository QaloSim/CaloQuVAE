{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9efbc629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dkraft/.local/lib/python3.10/site-packages/wandb/apis/public.py:3105: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n",
      "\u001b[1m[21:21:35.841]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mCaloQuVAE                                         \u001b[0mLoading configuration.\n",
      "2025-08-27 21:21:36,498 dwave.cloud \u001b[1;95mINFO \u001b[1;0m MainThread Log level for 'dwave.cloud' namespace set to 0\n",
      "\u001b[1m[21:21:36.498]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud                                       \u001b[0mLog level for 'dwave.cloud' namespace set to 0\n"
     ]
    }
   ],
   "source": [
    "# this notebook is for plotting with the ATLAS \"work in progress\" or \"simulation internal\" labels\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "from hydra.utils import instantiate\n",
    "from hydra import initialize, compose\n",
    "import hydra\n",
    "import wandb\n",
    "\n",
    "from data.dataManager import DataManager\n",
    "from model.modelCreator import ModelCreator\n",
    "from omegaconf import OmegaConf\n",
    "from scripts.run import setup_model, load_model_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10bcc231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dkraft/.local/lib/python3.10/site-packages/wandb/apis/public.py:3105: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import parse_version\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=\"config\")\n",
    "config=compose(config_name=\"config.yaml\")\n",
    "wandb.init(tags = [config.data.dataset_name], project=config.wandb.project, entity=config.wandb.entity, config=OmegaConf.to_container(config, resolve=True), mode='disabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "168ef51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cuda:3']\n"
     ]
    }
   ],
   "source": [
    "# check which dev:\n",
    "devids = [\"cuda:{0}\".format(x) for x in list(config.gpu_list)]\n",
    "dev = torch.device(devids[0])\n",
    "print(devids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad4ea0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[21:24:20.447]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0mLoading other dataset: CaloChallenge2\n",
      "\u001b[1m[21:24:20.449]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0mKeys: ['incident_energies', 'showers']\n",
      "\u001b[1m[21:24:26.334]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0mdict_keys(['incident_energies', 'showers'])\n",
      "\u001b[1m[21:24:26.335]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0m<torch.utils.data.dataloader.DataLoader object at 0x7f9fd4379c60>: 79999 events, 157 batches\n",
      "\u001b[1m[21:24:26.336]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0m<torch.utils.data.dataloader.DataLoader object at 0x7f9fd4379de0>: 10001 events, 10 batches\n",
      "\u001b[1m[21:24:26.336]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0m<torch.utils.data.dataloader.DataLoader object at 0x7f9fd4379cf0>: 9999 events, 10 batches\n",
      "\u001b[1m[21:24:26.337]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mmodel.modelCreator                                \u001b[0m::Creating Model\n",
      "\u001b[1m[21:24:27.066]\u001b[0m \u001b[1;93mWARN \u001b[1;0m  \u001b[1mmodel.rbm.zephyr                                  \u001b[0mQPU is offline. Setting a hard-coded zephyr. Check to see you're pinging the correct chip_id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dkraft/CaloQuVAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[21:24:27.345]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mscripts.run                                       \u001b[0mRequesting GPUs. GPU list :[3]\n",
      "\u001b[1m[21:24:27.347]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mscripts.run                                       \u001b[0mMain GPU : cuda:3\n",
      "\u001b[1m[21:24:27.347]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mscripts.run                                       \u001b[0mCUDA available\n",
      "\u001b[1m[21:24:27.461]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mmodel.modelCreator                                \u001b[0mLoading state\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n",
      "encoder._networks.0.seq1.0.conv.weight True\n",
      "encoder._networks.0.seq1.0.conv.bias True\n",
      "encoder._networks.0.seq1.1.weight True\n",
      "encoder._networks.0.seq1.1.bias True\n",
      "encoder._networks.0.seq1.2.weight True\n",
      "encoder._networks.0.seq1.3.conv.weight True\n",
      "encoder._networks.0.seq1.3.conv.bias True\n",
      "encoder._networks.0.seq1.4.weight True\n",
      "encoder._networks.0.seq1.4.bias True\n",
      "encoder._networks.0.seq1.5.weight True\n",
      "encoder._networks.0.seq1.6.conv.weight True\n",
      "encoder._networks.0.seq1.6.conv.bias True\n",
      "encoder._networks.0.seq1.7.weight True\n",
      "encoder._networks.0.seq1.7.bias True\n",
      "encoder._networks.0.seq1.8.weight True\n",
      "encoder._networks.0.seq2.0.conv.weight True\n",
      "encoder._networks.0.seq2.0.conv.bias True\n",
      "encoder._networks.0.seq2.1.weight True\n",
      "encoder._networks.0.seq2.1.bias True\n",
      "encoder._networks.0.seq2.2.weight True\n",
      "encoder._networks.0.seq2.3.conv.weight True\n",
      "encoder._networks.0.seq2.3.conv.bias True\n",
      "encoder._networks.0.seq2.4.weight True\n",
      "encoder._networks.1.seq1.0.conv.weight True\n",
      "encoder._networks.1.seq1.0.conv.bias True\n",
      "encoder._networks.1.seq1.1.weight True\n",
      "encoder._networks.1.seq1.1.bias True\n",
      "encoder._networks.1.seq1.2.weight True\n",
      "encoder._networks.1.seq1.3.conv.weight True\n",
      "encoder._networks.1.seq1.3.conv.bias True\n",
      "encoder._networks.1.seq1.4.weight True\n",
      "encoder._networks.1.seq1.4.bias True\n",
      "encoder._networks.1.seq1.5.weight True\n",
      "encoder._networks.1.seq1.6.conv.weight True\n",
      "encoder._networks.1.seq1.6.conv.bias True\n",
      "encoder._networks.1.seq1.7.weight True\n",
      "encoder._networks.1.seq1.7.bias True\n",
      "encoder._networks.1.seq1.8.weight True\n",
      "encoder._networks.1.seq2.0.conv.weight True\n",
      "encoder._networks.1.seq2.0.conv.bias True\n",
      "encoder._networks.1.seq2.1.weight True\n",
      "encoder._networks.1.seq2.1.bias True\n",
      "encoder._networks.1.seq2.2.weight True\n",
      "encoder._networks.1.seq2.3.conv.weight True\n",
      "encoder._networks.1.seq2.3.conv.bias True\n",
      "encoder._networks.1.seq2.4.weight True\n",
      "encoder._networks.2.seq1.0.conv.weight True\n",
      "encoder._networks.2.seq1.0.conv.bias True\n",
      "encoder._networks.2.seq1.1.weight True\n",
      "encoder._networks.2.seq1.1.bias True\n",
      "encoder._networks.2.seq1.2.weight True\n",
      "encoder._networks.2.seq1.3.conv.weight True\n",
      "encoder._networks.2.seq1.3.conv.bias True\n",
      "encoder._networks.2.seq1.4.weight True\n",
      "encoder._networks.2.seq1.4.bias True\n",
      "encoder._networks.2.seq1.5.weight True\n",
      "encoder._networks.2.seq1.6.conv.weight True\n",
      "encoder._networks.2.seq1.6.conv.bias True\n",
      "encoder._networks.2.seq1.7.weight True\n",
      "encoder._networks.2.seq1.7.bias True\n",
      "encoder._networks.2.seq1.8.weight True\n",
      "encoder._networks.2.seq2.0.conv.weight True\n",
      "encoder._networks.2.seq2.0.conv.bias True\n",
      "encoder._networks.2.seq2.1.weight True\n",
      "encoder._networks.2.seq2.1.bias True\n",
      "encoder._networks.2.seq2.2.weight True\n",
      "encoder._networks.2.seq2.3.conv.weight True\n",
      "encoder._networks.2.seq2.3.conv.bias True\n",
      "encoder._networks.2.seq2.4.weight True\n",
      "decoder.moduleLayers.0._layers.1.conv.weight True\n",
      "decoder.moduleLayers.0._layers.1.conv.bias True\n",
      "decoder.moduleLayers.0._layers.2.weight True\n",
      "decoder.moduleLayers.0._layers.2.bias True\n",
      "decoder.moduleLayers.0._layers.3.weight True\n",
      "decoder.moduleLayers.0._layers.4.conv.weight True\n",
      "decoder.moduleLayers.0._layers.4.conv.bias True\n",
      "decoder.moduleLayers.0._layers.5.weight True\n",
      "decoder.moduleLayers.0._layers.5.bias True\n",
      "decoder.moduleLayers.0._layers.6.weight True\n",
      "decoder.moduleLayers.0._layers2.0.conv.weight True\n",
      "decoder.moduleLayers.0._layers2.0.conv.bias True\n",
      "decoder.moduleLayers.0._layers2.1.weight True\n",
      "decoder.moduleLayers.0._layers2.1.bias True\n",
      "decoder.moduleLayers.0._layers2.2.weight True\n",
      "decoder.moduleLayers.0._layers2.3.conv.weight True\n",
      "decoder.moduleLayers.0._layers2.3.conv.bias True\n",
      "decoder.moduleLayers.0._layers2.4.weight True\n",
      "decoder.moduleLayers.0._layers2.4.bias True\n",
      "decoder.moduleLayers.0._layers2.5.weight True\n",
      "decoder.moduleLayers.0._layers2.6.conv.weight True\n",
      "decoder.moduleLayers.0._layers2.6.conv.bias True\n",
      "decoder.moduleLayers.0._layers2.7.conv.weight True\n",
      "decoder.moduleLayers.0._layers2.7.conv.bias True\n",
      "decoder.moduleLayers.0._layers2.8.weight True\n",
      "decoder.moduleLayers.0._layers3.0.conv.weight True\n",
      "decoder.moduleLayers.0._layers3.0.conv.bias True\n",
      "decoder.moduleLayers.0._layers3.1.weight True\n",
      "decoder.moduleLayers.0._layers3.1.bias True\n",
      "decoder.moduleLayers.0._layers3.3.to_qkv.weight True\n",
      "decoder.moduleLayers.0._layers3.3.to_out.0.weight True\n",
      "decoder.moduleLayers.0._layers3.3.to_out.0.bias True\n",
      "decoder.moduleLayers.0._layers3.3.to_out.1.weight True\n",
      "decoder.moduleLayers.0._layers3.3.to_out.1.bias True\n",
      "decoder.moduleLayers.0._layers3.4.conv.weight True\n",
      "decoder.moduleLayers.0._layers3.4.conv.bias True\n",
      "decoder.moduleLayers.0._layers3.5.weight True\n",
      "decoder.moduleLayers.0._layers3.5.bias True\n",
      "decoder.moduleLayers.0._layers3.7.to_qkv.weight True\n",
      "decoder.moduleLayers.0._layers3.7.to_out.0.weight True\n",
      "decoder.moduleLayers.0._layers3.7.to_out.0.bias True\n",
      "decoder.moduleLayers.0._layers3.7.to_out.1.weight True\n",
      "decoder.moduleLayers.0._layers3.7.to_out.1.bias True\n",
      "decoder.moduleLayers.0._layers3.8.conv.weight True\n",
      "decoder.moduleLayers.0._layers3.8.conv.bias True\n",
      "decoder.moduleLayers.0._layers3.9.conv.weight True\n",
      "decoder.moduleLayers.0._layers3.9.conv.bias True\n",
      "decoder.moduleLayers.1._layers.1.conv.weight True\n",
      "decoder.moduleLayers.1._layers.1.conv.bias True\n",
      "decoder.moduleLayers.1._layers.2.weight True\n",
      "decoder.moduleLayers.1._layers.2.bias True\n",
      "decoder.moduleLayers.1._layers.3.weight True\n",
      "decoder.moduleLayers.1._layers.4.conv.weight True\n",
      "decoder.moduleLayers.1._layers.4.conv.bias True\n",
      "decoder.moduleLayers.1._layers.5.weight True\n",
      "decoder.moduleLayers.1._layers.5.bias True\n",
      "decoder.moduleLayers.1._layers.6.weight True\n",
      "decoder.moduleLayers.1._layers2.0.conv.weight True\n",
      "decoder.moduleLayers.1._layers2.0.conv.bias True\n",
      "decoder.moduleLayers.1._layers2.1.weight True\n",
      "decoder.moduleLayers.1._layers2.1.bias True\n",
      "decoder.moduleLayers.1._layers2.2.weight True\n",
      "decoder.moduleLayers.1._layers2.3.conv.weight True\n",
      "decoder.moduleLayers.1._layers2.3.conv.bias True\n",
      "decoder.moduleLayers.1._layers2.4.weight True\n",
      "decoder.moduleLayers.1._layers2.4.bias True\n",
      "decoder.moduleLayers.1._layers2.5.weight True\n",
      "decoder.moduleLayers.1._layers2.6.conv.weight True\n",
      "decoder.moduleLayers.1._layers2.6.conv.bias True\n",
      "decoder.moduleLayers.1._layers2.7.conv.weight True\n",
      "decoder.moduleLayers.1._layers2.7.conv.bias True\n",
      "decoder.moduleLayers.1._layers2.8.weight True\n",
      "decoder.moduleLayers.1._layers3.0.conv.weight True\n",
      "decoder.moduleLayers.1._layers3.0.conv.bias True\n",
      "decoder.moduleLayers.1._layers3.1.weight True\n",
      "decoder.moduleLayers.1._layers3.1.bias True\n",
      "decoder.moduleLayers.1._layers3.3.to_qkv.weight True\n",
      "decoder.moduleLayers.1._layers3.3.to_out.0.weight True\n",
      "decoder.moduleLayers.1._layers3.3.to_out.0.bias True\n",
      "decoder.moduleLayers.1._layers3.3.to_out.1.weight True\n",
      "decoder.moduleLayers.1._layers3.3.to_out.1.bias True\n",
      "decoder.moduleLayers.1._layers3.4.conv.weight True\n",
      "decoder.moduleLayers.1._layers3.4.conv.bias True\n",
      "decoder.moduleLayers.1._layers3.5.weight True\n",
      "decoder.moduleLayers.1._layers3.5.bias True\n",
      "decoder.moduleLayers.1._layers3.7.to_qkv.weight True\n",
      "decoder.moduleLayers.1._layers3.7.to_out.0.weight True\n",
      "decoder.moduleLayers.1._layers3.7.to_out.0.bias True\n",
      "decoder.moduleLayers.1._layers3.7.to_out.1.weight True\n",
      "decoder.moduleLayers.1._layers3.7.to_out.1.bias True\n",
      "decoder.moduleLayers.1._layers3.8.conv.weight True\n",
      "decoder.moduleLayers.1._layers3.8.conv.bias True\n",
      "decoder.moduleLayers.1._layers3.9.conv.weight True\n",
      "decoder.moduleLayers.1._layers3.9.conv.bias True\n",
      "decoder.moduleLayers.2._layers.1.conv.weight True\n",
      "decoder.moduleLayers.2._layers.1.conv.bias True\n",
      "decoder.moduleLayers.2._layers.2.weight True\n",
      "decoder.moduleLayers.2._layers.2.bias True\n",
      "decoder.moduleLayers.2._layers.3.weight True\n",
      "decoder.moduleLayers.2._layers.4.conv.weight True\n",
      "decoder.moduleLayers.2._layers.4.conv.bias True\n",
      "decoder.moduleLayers.2._layers.5.weight True\n",
      "decoder.moduleLayers.2._layers.5.bias True\n",
      "decoder.moduleLayers.2._layers.6.weight True\n",
      "decoder.moduleLayers.2._layers2.0.conv.weight True\n",
      "decoder.moduleLayers.2._layers2.0.conv.bias True\n",
      "decoder.moduleLayers.2._layers2.1.weight True\n",
      "decoder.moduleLayers.2._layers2.1.bias True\n",
      "decoder.moduleLayers.2._layers2.2.weight True\n",
      "decoder.moduleLayers.2._layers2.3.conv.weight True\n",
      "decoder.moduleLayers.2._layers2.3.conv.bias True\n",
      "decoder.moduleLayers.2._layers2.4.weight True\n",
      "decoder.moduleLayers.2._layers2.4.bias True\n",
      "decoder.moduleLayers.2._layers2.5.weight True\n",
      "decoder.moduleLayers.2._layers2.6.conv.weight True\n",
      "decoder.moduleLayers.2._layers2.6.conv.bias True\n",
      "decoder.moduleLayers.2._layers2.7.conv.weight True\n",
      "decoder.moduleLayers.2._layers2.7.conv.bias True\n",
      "decoder.moduleLayers.2._layers2.8.weight True\n",
      "decoder.moduleLayers.2._layers3.0.conv.weight True\n",
      "decoder.moduleLayers.2._layers3.0.conv.bias True\n",
      "decoder.moduleLayers.2._layers3.1.weight True\n",
      "decoder.moduleLayers.2._layers3.1.bias True\n",
      "decoder.moduleLayers.2._layers3.3.to_qkv.weight True\n",
      "decoder.moduleLayers.2._layers3.3.to_out.0.weight True\n",
      "decoder.moduleLayers.2._layers3.3.to_out.0.bias True\n",
      "decoder.moduleLayers.2._layers3.3.to_out.1.weight True\n",
      "decoder.moduleLayers.2._layers3.3.to_out.1.bias True\n",
      "decoder.moduleLayers.2._layers3.4.conv.weight True\n",
      "decoder.moduleLayers.2._layers3.4.conv.bias True\n",
      "decoder.moduleLayers.2._layers3.5.weight True\n",
      "decoder.moduleLayers.2._layers3.5.bias True\n",
      "decoder.moduleLayers.2._layers3.7.to_qkv.weight True\n",
      "decoder.moduleLayers.2._layers3.7.to_out.0.weight True\n",
      "decoder.moduleLayers.2._layers3.7.to_out.0.bias True\n",
      "decoder.moduleLayers.2._layers3.7.to_out.1.weight True\n",
      "decoder.moduleLayers.2._layers3.7.to_out.1.bias True\n",
      "decoder.moduleLayers.2._layers3.8.conv.weight True\n",
      "decoder.moduleLayers.2._layers3.8.conv.bias True\n",
      "decoder.moduleLayers.2._layers3.9.conv.weight True\n",
      "decoder.moduleLayers.2._layers3.9.conv.bias True\n",
      "decoder.moduleLayers.3._layers.1.conv.weight True\n",
      "decoder.moduleLayers.3._layers.1.conv.bias True\n",
      "decoder.moduleLayers.3._layers.2.weight True\n",
      "decoder.moduleLayers.3._layers.2.bias True\n",
      "decoder.moduleLayers.3._layers.3.weight True\n",
      "decoder.moduleLayers.3._layers.4.conv.weight True\n",
      "decoder.moduleLayers.3._layers.4.conv.bias True\n",
      "decoder.moduleLayers.3._layers.5.weight True\n",
      "decoder.moduleLayers.3._layers.5.bias True\n",
      "decoder.moduleLayers.3._layers.6.weight True\n",
      "decoder.moduleLayers.3._layers2.0.conv.weight True\n",
      "decoder.moduleLayers.3._layers2.0.conv.bias True\n",
      "decoder.moduleLayers.3._layers2.1.weight True\n",
      "decoder.moduleLayers.3._layers2.1.bias True\n",
      "decoder.moduleLayers.3._layers2.2.weight True\n",
      "decoder.moduleLayers.3._layers2.3.conv.weight True\n",
      "decoder.moduleLayers.3._layers2.3.conv.bias True\n",
      "decoder.moduleLayers.3._layers2.4.weight True\n",
      "decoder.moduleLayers.3._layers2.4.bias True\n",
      "decoder.moduleLayers.3._layers2.5.weight True\n",
      "decoder.moduleLayers.3._layers2.6.conv.weight True\n",
      "decoder.moduleLayers.3._layers2.6.conv.bias True\n",
      "decoder.moduleLayers.3._layers2.7.conv.weight True\n",
      "decoder.moduleLayers.3._layers2.7.conv.bias True\n",
      "decoder.moduleLayers.3._layers2.8.weight True\n",
      "decoder.moduleLayers.3._layers3.0.conv.weight True\n",
      "decoder.moduleLayers.3._layers3.0.conv.bias True\n",
      "decoder.moduleLayers.3._layers3.1.weight True\n",
      "decoder.moduleLayers.3._layers3.1.bias True\n",
      "decoder.moduleLayers.3._layers3.3.to_qkv.weight True\n",
      "decoder.moduleLayers.3._layers3.3.to_out.0.weight True\n",
      "decoder.moduleLayers.3._layers3.3.to_out.0.bias True\n",
      "decoder.moduleLayers.3._layers3.3.to_out.1.weight True\n",
      "decoder.moduleLayers.3._layers3.3.to_out.1.bias True\n",
      "decoder.moduleLayers.3._layers3.4.conv.weight True\n",
      "decoder.moduleLayers.3._layers3.4.conv.bias True\n",
      "decoder.moduleLayers.3._layers3.5.weight True\n",
      "decoder.moduleLayers.3._layers3.5.bias True\n",
      "decoder.moduleLayers.3._layers3.7.to_qkv.weight True\n",
      "decoder.moduleLayers.3._layers3.7.to_out.0.weight True\n",
      "decoder.moduleLayers.3._layers3.7.to_out.0.bias True\n",
      "decoder.moduleLayers.3._layers3.7.to_out.1.weight True\n",
      "decoder.moduleLayers.3._layers3.7.to_out.1.bias True\n",
      "decoder.moduleLayers.3._layers3.8.conv.weight True\n",
      "decoder.moduleLayers.3._layers3.8.conv.bias True\n",
      "decoder.moduleLayers.3._layers3.9.conv.weight True\n",
      "decoder.moduleLayers.3._layers3.9.conv.bias True\n",
      "decoder.subdecs.0.weight True\n",
      "decoder.subdecs.0.bias True\n",
      "decoder.subdecs.1.weight True\n",
      "decoder.subdecs.1.bias True\n",
      "decoder.subdecs.2.weight True\n",
      "decoder.subdecs.2.bias True\n",
      "prior._weight_dict.01 False\n",
      "prior._weight_dict.02 False\n",
      "prior._weight_dict.03 False\n",
      "prior._weight_dict.12 False\n",
      "prior._weight_dict.13 False\n",
      "prior._weight_dict.23 False\n",
      "prior._bias_dict.0 False\n",
      "prior._bias_dict.1 False\n",
      "prior._bias_dict.2 False\n",
      "prior._bias_dict.3 False\n",
      "prior._weight_mask_dict.01 False\n",
      "prior._weight_mask_dict.02 False\n",
      "prior._weight_mask_dict.03 False\n",
      "prior._weight_mask_dict.12 False\n",
      "prior._weight_mask_dict.13 False\n",
      "prior._weight_mask_dict.23 False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[21:24:27.903]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mmodel.modelCreator                                \u001b[0mLoading weights from file : /fast_scratch_1/caloqvae/jtoledo/wandb/run-20250721_152921-c22o2vxp/files/ae_separate_249.pth\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights for module =  _hit_smoothing_dist_mod\n",
      "Loading weights for module =  _bce_loss\n",
      "Loading weights for module =  encoder\n",
      "Loading weights for module =  decoder\n",
      "Loading weights for module =  prior\n"
     ]
    }
   ],
   "source": [
    "# loading in the model\n",
    "new_model = False\n",
    "if new_model:\n",
    "    self = setup_model(config)\n",
    "    # self.model = self.model.double()  # sets all model parameters to float64\n",
    "else:\n",
    "    #self = load_model_instance(config.config_path)\n",
    "    self = load_model_instance(config)\n",
    "    # self.model = self.model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3958926d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[21:25:22.862]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mengine.engine                                     \u001b[0mEpoch: 0 - Average Val Loss: 15010.9894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'val_ae_loss': 13458.0109375,\n",
       " 'val_hit_loss': 1565.1702026367188,\n",
       " 'val_entropy': -121.91909790039062,\n",
       " 'val_loss': 15010.98935546875}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# uncomment this to run evaluate vae and show plots:\n",
    "self.evaluate_vae(self.data_mgr.val_loader, epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "702f9f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to plot with ATLAS labels:\n",
    "def plot_histograms(ax, target, recon, sampled, xlabel, ylabel, title, bins=30, log_scale=True):\n",
    "    max_value = max(target.max(), recon.max(), sampled.max())\n",
    "    min_value = min(target.min(), recon.min(), sampled.min())\n",
    "    if min_value == max_value:\n",
    "        print(\"Warning: min and max values are the same, adjusting to avoid division by zero.\")\n",
    "        max_value += 0.1\n",
    "    binning = np.arange(min_value, max_value, (max_value - min_value) / bins)\n",
    "\n",
    "    ax.hist(target, histtype=\"stepfilled\", bins=binning, density=True, alpha=0.7, label='Ground Truth', color='b', linewidth=2.5)\n",
    "    ax.hist(recon, histtype=\"step\", bins=binning, density=True, label='Reconstruction', color='c', linewidth=2.5)\n",
    "    ax.hist(sampled, histtype=\"step\", bins=binning, density=True, label='Sampled', color='orange', linewidth=2.5)\n",
    "\n",
    "    ax.set_xlabel(xlabel, fontsize=16)\n",
    "    ax.set_ylabel(ylabel, fontsize=16)\n",
    "    ax.set_title(title, fontsize=18)\n",
    "    ax.set_yscale('log' if log_scale else 'linear')\n",
    "    ax.set_ylim(top=ax.get_ylim()[1] * 1.7)\n",
    "    ax.grid(True)\n",
    "    #ax.legend(fontsize=15, loc='lower center')\n",
    "    ax.legend(fontsize=15, loc='lower left')\n",
    "\n",
    "    # ATLAS label\n",
    "    top = ax.get_ylim()[1]\n",
    "    left = ax.get_xlim()[0]\n",
    "    right = ax.get_xlim()[1]\n",
    "    height = top - ax.get_ylim()[0]\n",
    "    width = right - left\n",
    "    \n",
    "    ax.text(0.05, 0.93, 'ATLAS', fontsize=19,\n",
    "        fontweight='bold', style='italic',\n",
    "        transform=ax.transAxes, horizontalalignment='left')\n",
    "\n",
    "    ax.text(0.22, 0.93, 'Work in Progress', fontsize=18,\n",
    "        transform=ax.transAxes, horizontalalignment='left')\n",
    "\n",
    "    fig = ax.get_figure()\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "831863d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer specific deposited energy:\n",
    "\n",
    "# adding ATLAS label to the plots:\n",
    "# cfg = self._config\n",
    "# layer_cell_count = cfg.data.r * cfg.data.phi\n",
    "# relevant_layers = cfg.data.relevantLayers\n",
    "\n",
    "# for i, layer_num in enumerate(relevant_layers):\n",
    "#     idx_prev = i * layer_cell_count\n",
    "#     idx = (i + 1) * layer_cell_count\n",
    "\n",
    "#     target_layer = self.showers[:, idx_prev:idx]\n",
    "#     recon_layer = self.showers_recon[:, idx_prev:idx]\n",
    "#     sampled_layer = self.showers_prior[:, idx_prev:idx]\n",
    "\n",
    "#     target_energy = torch.sum(target_layer, dim=1).numpy() / 1000  # convert to GeV\n",
    "#     recon_energy = torch.sum(recon_layer, dim=1).numpy() / 1000\n",
    "#     sampled_energy = torch.sum(sampled_layer, dim=1).numpy() / 1000\n",
    "\n",
    "#     fig = plt.figure(figsize=(7, 6))\n",
    "#     ax = plt.gca()\n",
    "#     plot_histograms(ax, target_energy, recon_energy, sampled_energy,\n",
    "#                     xlabel='Deposited Energy (GeV)', ylabel='Density',\n",
    "#                     title=f'Layer {layer_num} - Deposited Energy')\n",
    "#     fig.savefig(f'PosterPlots/layer{layer_num}_deposited_energy.png', dpi=300)\n",
    "#     plt.closefig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56afe2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# energy conditioned sparsity:\n",
    "\n",
    "# incident_np = self.incident_energy.view(-1).numpy()\n",
    "# target_sparsity = ((self.showers == 0).sum(dim=1) / self.showers.shape[1]).numpy()\n",
    "# recon_sparsity = ((self.showers_recon == 0).sum(dim=1) / self.showers_recon.shape[1]).numpy()\n",
    "# sampled_sparsity = ((self.showers_prior == 0).sum(dim=1) / self.showers_prior.shape[1]).numpy()\n",
    "\n",
    "# # energy bin centers for ATLAS\n",
    "# energy_bin_centers = [2 ** i for i in range(8, 23)] \n",
    "\n",
    "# for i, energy_center in enumerate(energy_bin_centers):\n",
    "#     e_low = 2 ** (np.log2(energy_center) - 0.5)\n",
    "#     e_high = 2 ** (np.log2(energy_center) + 0.5)\n",
    "\n",
    "#     mask = (incident_np >= e_low) & (incident_np < e_high)\n",
    "#     if mask.sum() == 0:\n",
    "#         continue\n",
    "\n",
    "#     fig = plt.figure(figsize=(7, 6))\n",
    "#     ax = plt.gca()\n",
    "#     plot_histograms(\n",
    "#         ax,\n",
    "#         target_sparsity[mask],\n",
    "#         recon_sparsity[mask],\n",
    "#         sampled_sparsity[mask],\n",
    "#         xlabel='Sparsity',\n",
    "#         ylabel='Density',\n",
    "#         title=f'Sparsity ~ {e_low / 1000:.1f} - {e_high / 1000:.1f} GeV'\n",
    "#     )\n",
    "#     fig.savefig(f'PosterPlots/sparsity_energybin_{i}.png', dpi=300)\n",
    "#     plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01a3378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer specific incident energy:\n",
    "\n",
    "# layer_cell_count = cfg.data.r * cfg.data.phi\n",
    "# relevant_layers = cfg.data.relevantLayers\n",
    "# incident = self.incident_energy.view(-1).numpy()\n",
    "# epsilon = 1e-7  # avoid division errors\n",
    "\n",
    "# for i, layer_num in enumerate(relevant_layers):\n",
    "#     idx_prev = i * layer_cell_count\n",
    "#     idx = (i + 1) * layer_cell_count\n",
    "\n",
    "#     target_layer = self.showers[:, idx_prev:idx]\n",
    "#     recon_layer = self.showers_recon[:, idx_prev:idx]\n",
    "#     sampled_layer = self.showers_prior[:, idx_prev:idx]\n",
    "\n",
    "#     target_energy = torch.sum(target_layer, dim=1).numpy()\n",
    "#     recon_energy = torch.sum(recon_layer, dim=1).numpy()\n",
    "#     sampled_energy = torch.sum(sampled_layer, dim=1).numpy()\n",
    "\n",
    "#     target_ratio = target_energy / (incident + epsilon)\n",
    "#     recon_ratio = recon_energy / (incident + epsilon)\n",
    "#     sampled_ratio = sampled_energy / (incident + epsilon)\n",
    "\n",
    "#     fig = plt.figure(figsize=(7, 6))\n",
    "#     ax = plt.gca()\n",
    "#     plot_histograms(\n",
    "#         ax,\n",
    "#         target_ratio,\n",
    "#         recon_ratio,\n",
    "#         sampled_ratio,\n",
    "#         xlabel='Deposited / Incident Energy',\n",
    "#         ylabel='Density',\n",
    "#         title=f'Layer {layer_num} - Incidence Ratio'\n",
    "#     )\n",
    "#     fig.savefig(f'PosterPlots/layer{layer_num}_incidence_ratio.png', dpi=300)\n",
    "#     #plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa78b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding in RBM plotting with the ATLAS label\n",
    "\n",
    "def plot_rbm_histogram(rbm_post, rbm_prior, rbm_prior_qpu=None, save_path=None):\n",
    "    energy_post = rbm_post.numpy()\n",
    "    energy_prior = rbm_prior.numpy()\n",
    "\n",
    "    if rbm_prior_qpu is not None:\n",
    "        energy_qpu = rbm_prior_qpu.numpy()\n",
    "        minVal = min(energy_post.min(), energy_prior.min(), energy_qpu.min())\n",
    "        maxVal = max(energy_post.max(), energy_prior.max(), energy_qpu.max())\n",
    "    else:\n",
    "        minVal = min(energy_post.min(), energy_prior.min())\n",
    "        maxVal = max(energy_post.max(), energy_prior.max())\n",
    "\n",
    "    binwidth = (maxVal - minVal) / 30\n",
    "    bins = np.arange(minVal, maxVal + binwidth, binwidth)\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    ax = plt.gca()\n",
    "\n",
    "    ax.hist(energy_post, bins=bins, linewidth=2.5, color=\"b\", density=True,\n",
    "            log=True, label=\"RBM Posterior\", alpha=0.7)\n",
    "    \n",
    "    ax.hist(energy_prior, bins=bins, color=\"orange\", density=True,\n",
    "            histtype='step', linewidth=2.5, label=\"RBM Prior\")\n",
    "\n",
    "    if rbm_prior_qpu is not None:\n",
    "        ax.hist(energy_qpu, bins=bins, color=\"m\", density=True,\n",
    "                histtype='step', linewidth=2.5, label=\"RBM QPU\")\n",
    "\n",
    "    ax.set_xlabel(\"RBM Energy\", fontsize=16)\n",
    "    ax.set_ylabel(\"Density\", fontsize=16)\n",
    "    ax.set_title(\"RBM Energy Distribution\", fontsize=16)\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.grid(True)\n",
    "    ax.legend(fontsize=14, loc='lower center')\n",
    "    ax.set_ylim(top=ax.get_ylim()[1] * 1.8)\n",
    "    \n",
    "    # adding ATLAS style label\n",
    "    ax.text(0.05, 0.93, 'ATLAS', fontsize=19, fontweight='bold', style='italic',\n",
    "            transform=ax.transAxes, horizontalalignment='left')\n",
    "    ax.text(0.20, 0.93, 'Work in Progress', fontsize=18,\n",
    "            transform=ax.transAxes, horizontalalignment='left')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        fig.savefig(save_path, dpi=300)\n",
    "        #plt.close(fig)\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61ae3e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling function to plot:\n",
    "#fig_rbm = plot_rbm_histogram(self.RBM_energy_post, self.RBM_energy_prior,\n",
    "#                             save_path=\"PosterPlots/rbm_energy_histogram.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9759c697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
