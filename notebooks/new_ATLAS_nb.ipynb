{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357a7ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "from hydra.utils import instantiate\n",
    "from hydra import initialize, compose\n",
    "import hydra\n",
    "\n",
    "import wandb\n",
    "\n",
    "from data.dataManager import DataManager\n",
    "from model.modelCreator import ModelCreator\n",
    "from omegaconf import OmegaConf\n",
    "from scripts.run import setup_model, load_model_instance\n",
    "\n",
    "from utils.plots import vae_plots\n",
    "from utils.rbm_plots import plot_rbm_histogram\n",
    "import os\n",
    "import h5py\n",
    "from utils.data_exploration import overall_plots\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b4c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare combined, positive, fine datasets across layers and eta\n",
    "\n",
    "base_dir = '/fast_scratch_1/caloqvae/data/atlas_july31'\n",
    "# List top-level keys (groups or datasets)\n",
    "# print(\"Combined keys:\", list(file_combined.keys()))\n",
    "# print(\"Positive keys:\", list(file_positive.keys()))\n",
    "for eta in range(0, 135, 5):\n",
    "    print(f\"Processing eta: {eta:03d}\")\n",
    "    file_names = [\"dataset_combined.hdf5\", \"dataset_combined_positive.hdf5\", \"dataset_combined_fine.hdf5\"]\n",
    "    file_combined = h5py.File(f\"{base_dir}/eta_{eta:03d}/eta_{eta:03d}_regular_binning/{file_names[0]}\", 'r')\n",
    "    file_positive = h5py.File(f\"{base_dir}/eta_{eta:03d}/eta_{eta:03d}_regular_binning/{file_names[1]}\", 'r')\n",
    "    file_fine = h5py.File(f\"{base_dir}/eta_{eta:03d}/eta_{eta:03d}_regular_binning/{file_names[2]}\", 'r')\n",
    "\n",
    "\n",
    "    valid_layers_combined = []\n",
    "    valid_layers_positive = []\n",
    "    valid_layers_fine = []\n",
    "    f_combined = {key: torch.tensor(np.array(file_combined[key])) for key in file_combined.keys()}\n",
    "    f_positive = {key: torch.tensor(np.array(file_positive[key])) for key in file_positive.keys()}\n",
    "    f_fine = {key: torch.tensor(np.array(file_fine[key])) for key in file_fine.keys()}\n",
    "    for layer in range(24):\n",
    "        key = f'energy_layer_{layer}'\n",
    "        if key in f_combined:\n",
    "            if (f_combined[key].sum(dim=1) != 0).any(): #check if layers contain events with nonzero entries\n",
    "                valid_layers_combined.append(layer)\n",
    "        if key in f_positive:\n",
    "            if (f_positive[key].sum(dim=1) != 0).any():\n",
    "                valid_layers_positive.append(layer)\n",
    "        if key in f_fine:\n",
    "            if (f_fine[key].sum(dim=1) != 0).any():\n",
    "                valid_layers_fine.append(layer)\n",
    "\n",
    "    print(f\"using these layers in combined: {valid_layers_combined}\")\n",
    "    print(f\"using these layers in positive: {valid_layers_positive}\")\n",
    "    print(f\"using these layers in fine: {valid_layers_fine}\")\n",
    "\n",
    "\n",
    "    # plot number of non-zero events in each layer\n",
    "    num_events_combined = []\n",
    "    num_events_positive = []\n",
    "    num_events_fine = []\n",
    "    for layer in valid_layers_combined:\n",
    "        key = f'energy_layer_{layer}'\n",
    "        if key in f_combined and key in f_positive and key in f_fine:\n",
    "            shape_combined = f_combined[key].shape\n",
    "            shape_positive = f_positive[key].shape\n",
    "            shape_fine = f_fine[key].shape\n",
    "            if shape_combined != shape_positive:\n",
    "                print(f\"Shape mismatch for layer {layer}: combined {shape_combined}, positive {shape_positive}\")\n",
    "            else:\n",
    "                print(f\"Layer {layer} has the same shape in both datasets: {shape_combined}\")\n",
    "            print(f\"Layer {layer} in combined dataset has {f_combined[key].sum(dim=1).nonzero().shape[0]} non-zero events.\")\n",
    "            print(f\"Layer {layer} in positive dataset has {f_positive[key].sum(dim=1).nonzero().shape[0]} non-zero events.\")\n",
    "            num_events_fine.append(f_fine[key])\n",
    "            num_events_combined.append(f_combined[key])\n",
    "            num_events_positive.append(f_positive[key])\n",
    "            # print dimensions of the layer\n",
    "            print(f\"Layer {layer} dimensions in combined dataset: {f_combined[key].shape}\")\n",
    "            print(f\"Layer {layer} dimensions in positive dataset: {f_positive[key].shape}\")\n",
    "        else:\n",
    "            print(f\"Layer {layer} is missing in one of the datasets.\")\n",
    "    # Plot number of non-zero events in each layer\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(np.array(valid_layers_combined)-0.3, [ (layer.sum(dim=1) != 0).sum().item() for layer in num_events_combined], width=0.3, label='Combined Dataset')\n",
    "    plt.bar(np.array(valid_layers_positive), [ (layer.sum(dim=1) != 0).sum().item() for layer in num_events_positive], width=0.3, label='Positive Dataset')\n",
    "    plt.bar(np.array(valid_layers_fine)+0.3, [ (layer.sum(dim=1) != 0).sum().item() for layer in num_events_fine], width=0.3, label='Fine Dataset')\n",
    "    plt.xlabel('Layer')\n",
    "    plt.ylabel('Number of Non-Zero Events')\n",
    "    plt.title(f'Number of Non-Zero Events per Layer for eta {eta:03d}')\n",
    "    plt.xticks(valid_layers_combined)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8955b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_base = \"/fast_scratch_1/caloqvae/data/atlas_july31\"\n",
    "output_dir_base = \"/fast_scratch_1/caloqvae/data/atlas_july31_cat\"\n",
    "\n",
    "for eta in range(0, 135, 5):\n",
    "    print(f\"Processing eta {eta}\")\n",
    "    input_dir = input_dir_base + f\"/eta_{eta:03d}\" + f\"/eta_{eta:03d}_regular_binning\"\n",
    "    output_dir = output_dir_base + f\"/eta_{eta:03d}\" + f\"/eta_{eta:03d}_regular_binning\"\n",
    "    print(f\"Output directory: {output_dir}\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    file_name = f\"{input_dir}/dataset_combined.hdf5\"\n",
    "    file_positive_name = f\"{input_dir}/dataset_combined_positive.hdf5\"\n",
    "    file_fine_name = f\"{input_dir}/dataset_combined_fine.hdf5\"\n",
    "    file_combined = h5py.File(file_name, 'r')\n",
    "    file_positive = h5py.File(file_positive_name, 'r')\n",
    "    file_fine = h5py.File(file_fine_name, 'r')\n",
    "    valid_layers_combined = []\n",
    "    valid_layers_positive = []\n",
    "    valid_layers_fine = []\n",
    "    f_combined = {key: torch.tensor(np.array(file_combined[key])) for key in file_combined.keys()}\n",
    "    f_positive = {key: torch.tensor(np.array(file_positive[key])) for key in file_positive.keys()}\n",
    "    f_fine = {key: torch.tensor(np.array(file_fine[key])) for key in file_fine.keys()}\n",
    "    for layer in range(24):\n",
    "        key = f'energy_layer_{layer}'\n",
    "        if key in f_combined:\n",
    "            if (f_combined[key].sum(dim=1) != 0).any(): #check if layers contain events with nonzero entries\n",
    "                valid_layers_combined.append(layer)\n",
    "        if key in f_positive:\n",
    "            if (f_positive[key].sum(dim=1) != 0).any():\n",
    "                valid_layers_positive.append(layer)\n",
    "        if key in f_fine:\n",
    "            if (f_fine[key].sum(dim=1) != 0).any():\n",
    "                valid_layers_fine.append(layer)\n",
    "    print(f\"using these layers in combined: {valid_layers_combined}\")\n",
    "    print(f\"using these layers in positive: {valid_layers_positive}\")\n",
    "    print(f\"using these layers in fine: {valid_layers_fine}\")\n",
    "\n",
    "    # concatenate across layers for all 3 datasets\n",
    "    showers = torch.cat([f_combined[f'energy_layer_{layer}'] for layer in valid_layers_combined], dim=1)\n",
    "    showers_positive = torch.cat([f_positive[f'energy_layer_{layer}'] for layer in valid_layers_positive], dim=1)\n",
    "    showers_fine = torch.cat([f_fine[f'energy_layer_{layer}'] for layer in valid_layers_fine], dim=1)\n",
    "\n",
    "    showers_scaled = (showers * f_combined['incident_energy'].unsqueeze(1)).numpy()\n",
    "    showers_positive_scaled = (showers_positive * f_positive['incident_energy'].unsqueeze(1)).numpy()\n",
    "    showers_fine_scaled = (showers_fine * f_fine['incident_energy'].unsqueeze(1)).numpy()\n",
    "\n",
    "    incident_energy_combined = f_combined['incident_energy'].numpy()\n",
    "    incident_energy_positive = f_positive['incident_energy'].numpy()\n",
    "    incident_energy_fine = f_fine['incident_energy'].numpy()\n",
    "\n",
    "    with h5py.File(f\"{output_dir}/dataset_combined_cat.hdf5\", 'w') as f_out:\n",
    "        f_out.create_dataset('showers', data=showers_scaled)\n",
    "        f_out.create_dataset('incident_energy', data=incident_energy_combined)\n",
    "\n",
    "    with h5py.File(f\"{output_dir}/dataset_positive_cat.hdf5\", 'w') as f_out:\n",
    "        f_out.create_dataset('showers', data=showers_positive_scaled)\n",
    "        f_out.create_dataset('incident_energy', data=incident_energy_positive)\n",
    "\n",
    "    with h5py.File(f\"{output_dir}/dataset_fine_cat.hdf5\", 'w') as f_out:\n",
    "        f_out.create_dataset('showers', data=showers_fine_scaled)\n",
    "        f_out.create_dataset('incident_energy', data=incident_energy_fine)\n",
    "    print(f\"Saved combined dataset to {output_dir}/dataset_combined_cat.hdf5\")\n",
    "    print(f\"Saved positive dataset to {output_dir}/dataset_positive_cat.hdf5\")\n",
    "    print(f\"Saved fine dataset to {output_dir}/dataset_fine_cat.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13309ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/fast_scratch_1/caloqvae/data/atlas_july31\"\n",
    "output_dir_base = \"/fast_scratch_1/caloqvae/data/atlas_july31_rebuilt\"\n",
    "\n",
    "\n",
    "def load_showers_and_incident_energy(path, valid_layers):\n",
    "    with h5py.File(path, 'r') as file:\n",
    "        data = {key: torch.tensor(file[key][:]) for key in file.keys()}\n",
    "        incident_energy = data[\"incident_energy\"]\n",
    "\n",
    "        combined = torch.cat([data[f\"energy_layer_{l}\"] for l in valid_layers], dim=1)\n",
    "        showers = combined * incident_energy.unsqueeze(1)\n",
    "        return showers.numpy(), incident_energy.numpy()\n",
    "\n",
    "def get_global_valid_layers(split_paths):\n",
    "    global_valid_layers = set()\n",
    "    for path in split_paths:\n",
    "        with h5py.File(path, 'r') as file:\n",
    "            data = {key: torch.tensor(file[key][:]) for key in file.keys()}\n",
    "            for l in range(24):\n",
    "                key = f\"energy_layer_{l}\"\n",
    "                if key in data and (data[key].sum(dim=1) != 0).any():\n",
    "                    global_valid_layers.add(l)\n",
    "    return sorted(global_valid_layers)\n",
    "\n",
    "eta_dirs = [f\"eta_{i:03d}\" for i in range(0, 135, 5)]\n",
    "file_names = [\"dataset_combined.hdf5\", \"dataset_combined_positive.hdf5\", \"dataset_combined_fine.hdf5\"]\n",
    "\n",
    "for eta_tag in tqdm(eta_dirs):\n",
    "    regular_dir = os.path.join(base_path, eta_tag, f\"{eta_tag}_regular_binning\")\n",
    "    global_valid_layers_dict = {file_name: set() for file_name in file_names}\n",
    "    showers_dict = {file_name: [] for file_name in file_names}\n",
    "    incident_energy_dict = {file_name: [] for file_name in file_names}\n",
    "\n",
    "    split_paths = {file_name: [os.path.join(regular_dir, str(i), file_name) for i in range(20)] for file_name in file_names}\n",
    "    for file_name, paths in split_paths.items():\n",
    "        global_valid_layers_dict[file_name] = get_global_valid_layers(paths)\n",
    "    for i in range(20):\n",
    "        for file_name in file_names:\n",
    "            split_path = os.path.join(regular_dir, str(i), file_name)\n",
    "            if os.path.exists(split_path):\n",
    "                showers, incident_energy = load_showers_and_incident_energy(split_path, global_valid_layers_dict[file_name])\n",
    "                if showers is not None:\n",
    "                    showers_dict[file_name].append(showers)\n",
    "                    incident_energy_dict[file_name].append(incident_energy)\n",
    "    output_dir = os.path.join(output_dir_base, eta_tag, f\"{eta_tag}_regular_binning\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for file_name in file_names:\n",
    "        shapes = [s.shape[1] for s in showers_dict[file_name]]\n",
    "        if len(set(shapes)) > 1:\n",
    "            print(f\"Inconsistent shape for {file_name} at {eta_tag}: {len(set(shapes))} unique shapes instead of 1\")\n",
    "            continue\n",
    "        showers_dict[file_name] = np.concatenate(showers_dict[file_name], axis=0)\n",
    "        incident_energy_dict[file_name] = np.concatenate(incident_energy_dict[file_name], axis=0)\n",
    "        with h5py.File(os.path.join(output_dir, file_name.split(\".\")[0]+\"_rebuilt.hdf5\"), 'w') as f_out:\n",
    "            f_out.create_dataset('showers', data=showers_dict[file_name])\n",
    "            f_out.create_dataset('incident_energy', data=incident_energy_dict[file_name])\n",
    "    print(f\"Rebuilt dataset saved for {eta_tag} in {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e005918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils.data_exploration\n",
    "importlib.reload(utils.data_exploration)\n",
    "from utils.data_exploration import get_global_valid_layers, load_showers_and_incident_energy\n",
    "base_path = \"/fast_scratch_1/caloqvae/data/atlas_july31_rebuilt\"\n",
    "\n",
    "#print shower shapes across eta\n",
    "eta_dirs = [f\"eta_{i:03d}\" for i in range(0, 135, 5)]\n",
    "file_names = [\"dataset_combined.hdf5\", \"dataset_combined_positive.hdf5\", \"dataset_combined_fine.hdf5\"]\n",
    "for eta_tag in tqdm(eta_dirs):\n",
    "    regular_dir = os.path.join(base_path, eta_tag, f\"{eta_tag}_regular_binning\")\n",
    "    for file_name in file_names:\n",
    "        combined_path = os.path.join(regular_dir, file_name.split(\".\")[0] + \"_rebuilt.hdf5\")\n",
    "        with h5py.File(combined_path, 'r') as f:\n",
    "            showers = f['showers'][:]\n",
    "            incident_energy = f['incident_energy'][:]\n",
    "            print(f\"Showers shape for {file_name} at {eta_tag}: {showers.shape}\")\n",
    "            print(f\"Incident energy shape for {file_name} at {eta_tag}: {incident_energy.shape}\")\n",
    "        if os.path.exists(os.path.join(regular_dir, file_name+\"_rebuilt.hdf5\")):\n",
    "            os.remove(os.path.join(regular_dir, file_name+\"_rebuilt.hdf5\"))  # Clean up after checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff16246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils.data_exploration\n",
    "importlib.reload(utils.data_exploration)\n",
    "from utils.data_exploration import compare_datasets, load_showers_and_incident_energy\n",
    "\n",
    "combined_path = \"/fast_scratch_1/caloqvae/data/atlas_july31_cat\"\n",
    "rebuilt_path = \"/fast_scratch_1/caloqvae/data/atlas_july31_rebuilt\"\n",
    "for eta in range(0, 135, 5)[:1]:\n",
    "    eta_tag = f\"eta_{eta:03d}\"\n",
    "    print(f\"Comparing datasets for {eta_tag}\")\n",
    "    combined_file = os.path.join(combined_path, eta_tag, f\"{eta_tag}_regular_binning\", \"dataset_positive_cat.hdf5\")\n",
    "    rebuilt_file = os.path.join(rebuilt_path, eta_tag, f\"{eta_tag}_regular_binning\", \"dataset_combined_positive_rebuilt.hdf5\")\n",
    "    if os.path.exists(combined_file) and os.path.exists(rebuilt_file):\n",
    "        combined_showers, combined_incident_energy = load_showers_and_incident_energy(combined_file, None)\n",
    "        rebuilt_showers, rebuilt_incident_energy = load_showers_and_incident_energy(rebuilt_file, None)\n",
    "        compare_datasets(combined_incident_energy, combined_showers, rebuilt_incident_energy, rebuilt_showers)\n",
    "    else:\n",
    "        print(f\"One of the files does not exist for {eta_tag}: {combined_file}, {rebuilt_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c511bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31_cat\"\n",
    "for eta in tqdm(range(0, 135, 5)):\n",
    "    file_names = [\"dataset_combined_cat.hdf5\", \"dataset_positive_cat.hdf5\", \"dataset_fine_cat.hdf5\"]\n",
    "    for file_name in file_names:\n",
    "        with h5py.File(f\"{base_dir}/eta_{eta:03d}/eta_{eta:03d}_regular_binning/{file_name}\", 'r') as file:\n",
    "            showers = torch.from_numpy(file['showers'][:])\n",
    "            incident_energy = torch.from_numpy(file['incident_energy'][:]).squeeze()\n",
    "\n",
    "        row_max = torch.amax(showers, dim=1)\n",
    "        bad_mask = row_max >= incident_energy\n",
    "        good_mask = ~bad_mask\n",
    "\n",
    "        n_bad = int(bad_mask.sum())\n",
    "        n_good = int(good_mask.sum())\n",
    "\n",
    "        print(f\"File: {file_name}, Eta: {eta}, Bad events: {n_bad}, Good events: {n_good}\")\n",
    "\n",
    "        clean_showers = showers[good_mask]\n",
    "        clean_incident_energy = incident_energy[good_mask]\n",
    "\n",
    "        with h5py.File(f\"{base_dir}/eta_{eta:03d}/eta_{eta:03d}_regular_binning/{file_name.split('.')[0]}_clean.hdf5\", 'w') as f_out:\n",
    "            f_out.create_dataset('showers', data=clean_showers.numpy())\n",
    "            f_out.create_dataset('incident_energies', data=clean_incident_energy.numpy())\n",
    "        if os.path.exists(f\"{base_dir}/eta_{eta:03d}/eta_{eta:03d}_regular_binning/{file_name}_clean\"):\n",
    "            os.remove(f\"{base_dir}/eta_{eta:03d}/eta_{eta:03d}_regular_binning/{file_name}_clean\")\n",
    "        print(f\"Saved cleaned dataset to {base_dir}/eta_{eta:03d}/eta_{eta:03d}_regular_binning/{file_name.split('.')[0]}_clean.hdf5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e14e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot number of bad events in combined, positive, and fine for each eta\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31_cat\"\n",
    "n_bad_combined = []\n",
    "n_bad_positive = []\n",
    "n_bad_fine = []\n",
    "\n",
    "for eta in range(0, 135, 5):\n",
    "    file_names = [\"dataset_combined_cat.hdf5\", \"dataset_positive_cat.hdf5\", \"dataset_fine_cat.hdf5\"]\n",
    "    for file_name in file_names:\n",
    "        file = h5py.File(f\"{base_dir}/eta_{eta:03d}/eta_{eta:03d}_regular_binning/{file_name}\", 'r')\n",
    "        showers = torch.from_numpy(file['showers'][:])\n",
    "        incident_energy = torch.from_numpy(file['incident_energy'][:]).squeeze()\n",
    "\n",
    "        row_max = torch.amax(showers, dim=1)\n",
    "        bad_mask = row_max >= incident_energy\n",
    "        n_bad = int(bad_mask.sum())\n",
    "\n",
    "        if \"combined\" in file_name:\n",
    "            n_bad_combined.append(n_bad)\n",
    "        elif \"positive\" in file_name:\n",
    "            n_bad_positive.append(n_bad)\n",
    "        elif \"fine\" in file_name:\n",
    "            n_bad_fine.append(n_bad)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "num_bins = len(n_bad_combined)\n",
    "eta_centres = np.arange(num_bins) * 5\n",
    "bin_width = 5/3\n",
    "plt.bar(eta_centres - bin_width, n_bad_combined, width=bin_width, label='Combined Dataset')\n",
    "plt.bar(eta_centres, n_bad_positive, width=bin_width, label='Positive Dataset')\n",
    "plt.bar(eta_centres + bin_width, n_bad_fine, width=bin_width, label='Fine Dataset')\n",
    "plt.xlabel('Eta')\n",
    "plt.ylabel('Number of Bad Events')\n",
    "plt.title('Number of Bad Events in Datasets by Eta')\n",
    "plt.legend()\n",
    "print(len(n_bad_combined), len(n_bad_positive), len(n_bad_fine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f591f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils.data_exploration\n",
    "importlib.reload(utils.data_exploration)\n",
    "from utils.data_exploration import overall_plots\n",
    "from utils.data_exploration import check_negatives\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31_cat\"\n",
    "n_bad_combined = []\n",
    "n_bad_positive = []\n",
    "n_bad_fine = []\n",
    "\n",
    "for eta in range(0, 135, 5)[:1]:\n",
    "    print(f\"Processing eta: {eta:03d}\")\n",
    "    showers = torch.from_numpy(h5py.File(f\"{base_dir}/eta_{eta:03d}/eta_{eta:03d}_regular_binning/dataset_combined_cat.hdf5\", 'r')['showers'][:])\n",
    "    incident_energy = torch.from_numpy(h5py.File(f\"{base_dir}/eta_{eta:03d}/eta_{eta:03d}_regular_binning/dataset_combined_cat.hdf5\", 'r')['incident_energy'][:])\n",
    "    showers_pos = torch.from_numpy(h5py.File(f\"{base_dir}/eta_{eta:03d}/eta_{eta:03d}_regular_binning/dataset_positive_cat.hdf5\", 'r')['showers'][:])\n",
    "    incident_energy_pos = torch.from_numpy(h5py.File(f\"{base_dir}/eta_{eta:03d}/eta_{eta:03d}_regular_binning/dataset_positive_cat.hdf5\", 'r')['incident_energy'][:])\n",
    "    showers_fine = torch.from_numpy(h5py.File(f\"{base_dir}/eta_{eta:03d}/eta_{eta:03d}_regular_binning/dataset_fine_cat.hdf5\", 'r')['showers'][:])\n",
    "    incident_energy_fine = torch.from_numpy(h5py.File(f\"{base_dir}/eta_{eta:03d}/eta_{eta:03d}_regular_binning/dataset_fine_cat.hdf5\", 'r')['incident_energy'][:])\n",
    "\n",
    "    overall_plots(incident_energy, incident_energy_pos, incident_energy_fine, showers, showers_pos, showers_fine)\n",
    "    check_negatives(showers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937a297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot counts of the number of negative events in showers\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31_cat\"\n",
    "n_negative = []\n",
    "n_negative_positive = []\n",
    "n_negative_fine = []\n",
    "for eta in range(0, 135, 5):\n",
    "    print(f\"Processing eta: {eta:03d}\")\n",
    "    file_names = [\"dataset_combined_cat.hdf5\", \"dataset_positive_cat.hdf5\", \"dataset_fine_cat.hdf5\"]\n",
    "    for file_name in file_names:\n",
    "        file = h5py.File(f\"{base_dir}/eta_{eta:03d}/eta_{eta:03d}_regular_binning/{file_name}\", 'r')\n",
    "        showers = torch.from_numpy(file['showers'][:])\n",
    "        incident_energy = torch.from_numpy(file['incident_energy'][:]).squeeze()\n",
    "\n",
    "        row_min = torch.amin(showers, dim=1)\n",
    "        bad_mask = row_min < 0\n",
    "        good_mask = ~bad_mask\n",
    "\n",
    "        n_bad = int(bad_mask.sum())\n",
    "        n_good = int(good_mask.sum())\n",
    "\n",
    "        if \"combined\" in file_name:\n",
    "            n_negative.append(n_bad)\n",
    "        elif \"positive\" in file_name:\n",
    "            n_negative_positive.append(n_bad)\n",
    "        elif \"fine\" in file_name:\n",
    "            n_negative_fine.append(n_bad)\n",
    "plt.figure(figsize=(10, 6))\n",
    "num_bins = len(n_negative)\n",
    "eta_centres = np.arange(num_bins) * 5\n",
    "bin_width = 5/3\n",
    "plt.bar(eta_centres - bin_width, n_negative, width=bin_width, label='Combined Dataset')\n",
    "plt.bar(eta_centres, n_negative_positive, width=bin_width, label='Positive Dataset')\n",
    "plt.bar(eta_centres + bin_width, n_negative_fine, width=bin_width, label='Fine Dataset')\n",
    "plt.xlabel('Eta')\n",
    "plt.ylabel('Number of Negative Events')\n",
    "plt.title('Number of Negative Events in Datasets by Eta')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a5895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_regular_cat/\"\n",
    "n_negative = []\n",
    "for eta in range(20, 100, 5):\n",
    "    print(f\"Processing eta: {eta:03d}\")\n",
    "    if eta==75 or eta==80:\n",
    "        continue\n",
    "    file_name = base_dir + f\"dataset_eta_{eta:03d}_positive_cat_smeared.hdf5\"\n",
    "    file = h5py.File(file_name, 'r')\n",
    "    showers = torch.from_numpy(file['showers'][:])\n",
    "\n",
    "    row_min = torch.amin(showers, dim=1)\n",
    "    bad_mask = row_min < 0\n",
    "    good_mask = ~bad_mask\n",
    "\n",
    "    n_bad = int(bad_mask.sum())\n",
    "    n_negative.append(n_bad)\n",
    "plt.figure(figsize=(10, 6))\n",
    "num_bins = len(n_negative)\n",
    "eta_centres = np.arange(num_bins) * 5\n",
    "bin_width = 4\n",
    "plt.bar(eta_centres, n_negative, width=bin_width, label='Negative Events')\n",
    "plt.xlabel('Eta')\n",
    "plt.ylabel('Number of Negative Events')\n",
    "plt.title('Number of Negative Events in Datasets by Eta')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6433f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_paths = {\n",
    "    \"combined\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/dataset_combined.hdf5\",\n",
    "    \"positive\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/dataset_combined_positive.hdf5\",\n",
    "}\n",
    "\n",
    "def process_file(path, label):\n",
    "    with h5py.File(path, 'r') as file:\n",
    "        f = {key: torch.tensor(np.array(file[key])) for key in file.keys()}\n",
    "\n",
    "        valid_layers = []\n",
    "        for layer in range(24):\n",
    "            key = f'energy_layer_{layer}'\n",
    "            if key in f and (f[key].sum(dim=1) != 0).any():\n",
    "                valid_layers.append(layer)\n",
    "\n",
    "        print(f\"\\n {label.upper()} — using layers: {valid_layers}\")\n",
    "\n",
    "        en_sizes = np.concatenate([[0], np.array([\n",
    "            torch.where(f[\"incident_energy\"].log2() == i, 1, 0).sum()\n",
    "            for i in range(8, 23)\n",
    "        ])])\n",
    "        idx = list(np.concatenate([\n",
    "            [j + en_sizes[:i].sum() for j in range(int(np.floor(0.8 * en_sizes[i])))]\n",
    "            for i in range(1, len(en_sizes))\n",
    "        ]))\n",
    "        idxTest = list(set(range(f['incident_energy'].shape[0])) - set(idx))\n",
    "        idxSort = idx + idxTest\n",
    "\n",
    "        combined = torch.cat([f[f'energy_layer_{l}'] for l in valid_layers], dim=1)\n",
    "        showers = (combined * f[\"incident_energy\"].unsqueeze(1))[idxSort, :]\n",
    "        incident_energies = f[\"incident_energy\"].unsqueeze(1)[idxSort, :]\n",
    "\n",
    "        return showers.numpy(), incident_energies.numpy()\n",
    "\n",
    "results = {}\n",
    "for label, path in data_paths.items():\n",
    "    showers, incident_energies = process_file(path, label)\n",
    "    results[label] = {\"showers\": showers, \"incident_energies\": incident_energies}\n",
    "\n",
    "# Plot total deposited energy (log)\n",
    "plt.figure(figsize=(8, 5))\n",
    "for label in results:\n",
    "    plt.hist(results[label][\"showers\"].sum(1),\n",
    "             bins=100, alpha=0.6, label=label, histtype='step')\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Total Deposited Energy (MeV)\")\n",
    "plt.ylabel(\"Events (log scale)\")\n",
    "plt.title(\"Total Energy Deposited per Event\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccbc71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=\"config\")\n",
    "cfg=compose(config_name=\"config.yaml\")\n",
    "wandb.init(tags = [cfg.data.dataset_name], project=cfg.wandb.project, entity=cfg.wandb.entity, config=OmegaConf.to_container(cfg, resolve=True), mode='disabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60d1cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.plots import vae_plots\n",
    "file_name_combined = '/fast_scratch_1/caloqvae/data/atlas_july31_cat/eta_000/eta_000_regular_binning/dataset_combined_cat.hdf5'\n",
    "with h5py.File(file_name, 'r') as f:\n",
    "    showers = torch.tensor(f['showers'][:])\n",
    "    incident_energy = torch.tensor(f['incident_energy'][:])\n",
    "file_name_positive = '/fast_scratch_1/caloqvae/data/atlas_july31_cat/eta_000/eta_000_regular_binning/dataset_positive_cat.hdf5'\n",
    "with h5py.File(file_name_positive, 'r') as f:\n",
    "    showers_positive = torch.tensor(f['showers'][:])\n",
    "    incident_energy_positive = torch.tensor(f['incident_energy'][:])\n",
    "file_name_fine = '/fast_scratch_1/caloqvae/data/atlas_july31_cat/eta_000/eta_000_regular_binning/dataset_fine_cat.hdf5'\n",
    "with h5py.File(file_name_fine, 'r') as f:\n",
    "    showers_fine = torch.tensor(f['showers'][:])\n",
    "    incident_energy_fine = torch.tensor(f['incident_energy'][:])\n",
    "vae_plots(cfg, incident_energy, showers, showers_positive, showers_fine)\n",
    "overall_plots(incident_energy, incident_energy_positive, incident_energy_fine, showers, showers_positive, showers_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0e782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot distribution of incident energy, looping through all eta and file types\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31_cat\"\n",
    "for eta in range(0, 135, 5):\n",
    "    file_names = [\"dataset_combined_cat_clean.hdf5\", \"dataset_positive_cat_clean.hdf5\", \"dataset_fine_cat_clean.hdf5\"]\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharey=True, constrained_layout=True)\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        file = h5py.File(f\"{base_dir}/eta_{eta:03d}/eta_{eta:03d}_regular_binning/{file_name}\", 'r')\n",
    "        incident_energy = torch.from_numpy(file['incident_energy'][:]).squeeze()\n",
    "        axs[i].hist(incident_energy.numpy(), bins=100, alpha=0.7, color='blue')\n",
    "        axs[i].set_title(f'{file_name}')\n",
    "        axs[i].set_xlabel('Incident Energy (MeV)')\n",
    "        axs[i].set_ylabel('Frequency')\n",
    "        axs[i].set_yscale('log')\n",
    "        axs[i].set_xscale('log')\n",
    "        axs[i].grid(True)\n",
    "    fig.suptitle(f'Incident Energy Distribution for Eta {eta/100.0}', fontsize=16)\n",
    "\n",
    "    fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5e7cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution of total deposited energy per layer for combined, positive, and fine datasets across all eta\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31_cat\"\n",
    "layer_cell_count = 14*24\n",
    "num_relevant_layers = 5\n",
    "for eta in range(0, 135, 5)[:1]:\n",
    "    print(f\"Processing eta {eta}\")\n",
    "    file_names = [\"dataset_combined_cat.hdf5\", \"dataset_positive_cat.hdf5\", \"dataset_fine_cat.hdf5\"]\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    layer_fig, layer_ax = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "    colors = ['blue', 'orange', 'green']\n",
    "    histep = [\"stepfilled\", \"step\", \"step\"]\n",
    "    histograms = []\n",
    "    for i, file_name in enumerate(file_names):\n",
    "        file = h5py.File(f\"{base_dir}/eta_{eta:03d}/eta_{eta:03d}_regular_binning/{file_name}\", 'r')\n",
    "        showers = torch.from_numpy(file['showers'][:])\n",
    "        total_deposited_energy = showers.sum(dim=1)\n",
    "        ax.hist(total_deposited_energy.numpy(), bins=30, alpha=0.7, color=colors[i], label=file_name.split('_')[1], histtype=histep[i])\n",
    "        histogram, bin_edges = np.histogram(total_deposited_energy.numpy(), bins=100)\n",
    "        histograms.append((histogram, bin_edges))\n",
    "\n",
    "        ax.legend()    \n",
    "        ax.set_xlabel('Total Deposited Energy (MeV)')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_title(f'Total Deposited Energy Distribution for Eta {eta/100.0}')\n",
    "        ax.grid(True)\n",
    "\n",
    "        for layer in range(num_relevant_layers):\n",
    "            idx = layer * layer_cell_count\n",
    "            layer_data = showers[:, idx:idx + layer_cell_count]\n",
    "            total_layer_energy = layer_data.sum(dim=1)\n",
    "            layer_ax[layer // 3, layer % 3].hist(total_layer_energy.numpy(), bins=30, alpha=0.7, color=colors[i], label=file_name.split('_')[1], histtype=histep[i])\n",
    "            layer_ax[layer // 3, layer % 3].set_xlabel('Total Layer Energy (MeV)')\n",
    "            layer_ax[layer // 3, layer % 3].set_ylabel('Frequency')\n",
    "            layer_ax[layer // 3, layer % 3].set_yscale('log')\n",
    "            layer_ax[layer // 3, layer % 3].grid(True)\n",
    "            layer_ax[layer // 3, layer % 3].legend()\n",
    "    layer_fig.tight_layout()\n",
    "    fig.tight_layout()\n",
    "    bin_edges = histograms[0][1]\n",
    "    bin_centers = 0.5 * (bin_edges[1:] + bin_edges[:-1])\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(bin_centers, histograms[0][0] - histograms[1][0], label='Combined - Positive', color='blue')\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(bin_centers, histograms[0][0] - histograms[2][0], label='Combined - Fine', color='orange')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb5b4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if incident energies are identical for each eta in original datasets\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31_cat\"\n",
    "for eta in range(0, 135, 5):\n",
    "    file_names = [\"dataset_combined_cat.hdf5\", \"dataset_positive_cat.hdf5\", \"dataset_fine_cat.hdf5\"]\n",
    "    incident_energies = []\n",
    "    showers_list = []\n",
    "    for file_name in file_names:\n",
    "        with h5py.File(f\"{base_dir}/eta_{eta:03d}/eta_{eta:03d}_regular_binning/{file_name}\", 'r') as f:\n",
    "            incident_energy = torch.from_numpy(f['incident_energy'][:])\n",
    "            incident_energies.append(incident_energy)\n",
    "            showers = torch.from_numpy(f['showers'][:]).sum(dim=1)\n",
    "            showers_list.append(showers)\n",
    "\n",
    "    \n",
    "    # Check if all incident energies are the same\n",
    "    if all(torch.equal(incident_energies[0], ie) for ie in incident_energies):\n",
    "        print(f\"Incident energies are identical for eta {eta}\")\n",
    "    else:\n",
    "        print(f\"Incident energies differ for eta {eta}\")\n",
    "    # Count differences in showers\n",
    "    combined_showers = showers_list[0]\n",
    "    positive_showers = showers_list[1]\n",
    "    #fine_showers = showers_list[2]\n",
    "    shower_differences_positive = (combined_showers != positive_showers).sum().nonzero().shape[0]\n",
    "    #shower_differences_fine = (combined_showers != fine_showers).sum(dim=1).nonzero().shape[0]\n",
    "    print(f\"Number of differences in showers between positive and combined for eta {eta}: {shower_differences_positive}\")\n",
    "    #print(f\"Number of differences in showers between fine and combined for eta {eta}: {shower_differences_fine}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61515a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if incident energies are identical for each eta in original datasets\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31_cat\"\n",
    "for eta in range(0, 135, 5):\n",
    "    file_names = [\"dataset_combined_cat.hdf5\", \"dataset_positive_cat.hdf5\", \"dataset_fine_cat.hdf5\"]\n",
    "    incident_energies = []\n",
    "    showers_list = []\n",
    "    for file_name in file_names:\n",
    "        with h5py.File(f\"{base_dir}/eta_{eta:03d}/eta_{eta:03d}_regular_binning/{file_name}\", 'r') as f:\n",
    "            incident_energy = torch.from_numpy(f['incident_energy'][:])\n",
    "            incident_energies.append(incident_energy)\n",
    "            showers = torch.from_numpy(f['showers'][:]).sum(dim=1)\n",
    "            showers_list.append(showers)\n",
    "\n",
    "    \n",
    "    # Check if all incident energies are the same\n",
    "    if all(torch.equal(incident_energies[0], ie) for ie in incident_energies):\n",
    "        print(f\"Incident energies are identical for eta {eta}\")\n",
    "    else:\n",
    "        print(f\"Incident energies differ for eta {eta}\")\n",
    "    # Count differences in showers\n",
    "    combined_showers = showers_list[0]\n",
    "    positive_showers = showers_list[1]\n",
    "    #fine_showers = showers_list[2]\n",
    "    shower_differences_positive = (combined_showers != positive_showers).sum().nonzero().shape[0]\n",
    "    #shower_differences_fine = (combined_showers != fine_showers).sum(dim=1).nonzero().shape[0]\n",
    "    print(f\"Number of differences in showers between positive and combined for eta {eta}: {shower_differences_positive}\")\n",
    "    #print(f\"Number of differences in showers between fine and combined for eta {eta}: {shower_differences_fine}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa4d5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.evaluate_vae(self.data_mgr.test_loader, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff19a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils.HighLevelFeatsAtlasReg\n",
    "importlib.reload(utils.HighLevelFeatsAtlasReg)\n",
    "from utils.HighLevelFeatsAtlasReg import HighLevelFeatures_ATLAS_regular as HLF2\n",
    "import scripts.jet_metrics_atlas\n",
    "importlib.reload(scripts.jet_metrics_atlas)\n",
    "from scripts.jet_metrics_atlas import HepMetricsAtlas\n",
    "importlib.reload(scripts.Jet_metrics)\n",
    "from scripts.Jet_metrics import get_fpd_kpd_metrics\n",
    "\n",
    "sample_sets = [\n",
    "    (self.showers[0], 'Ground Truth'),\n",
    "    (self.showers_recon[0], 'Reconstructions'),\n",
    "    (self.showers_prior[0], 'Generated')\n",
    "]\n",
    "recon_HLF = HLF2(\"electron\", filename=self._config.data.binning_path)\n",
    "recon_HLF.Einc = self.incident_energy\n",
    "gt_HLF = HLF2(\"electron\", filename=self._config.data.binning_path)\n",
    "\n",
    "\n",
    "ATLAS_hep = HepMetricsAtlas(self)\n",
    "get_fpd_kpd_metrics(self.showers, self.showers_recon, False, recon_HLF, gt_HLF, if_Atlas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d7e407",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_engine = setup_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1125c51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.evaluate_vae(self.data_mgr.test_loader, 0)\n",
    "test_engine.evaluate_ae(self.data_mgr.val_loader, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe7d63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.evaluate_vae(self.data_mgr.val_loader, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d759539",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.evaluate_vae(self.data_mgr.test_loader, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4426db",
   "metadata": {},
   "outputs": [],
   "source": [
    "HLF_020 = HLF2(\"electron\", filename=test_engine._config.data.binning_path)\n",
    "HLF_035 = HLF2(\"electron\", filename=self._config.data.binning_path)\n",
    "HLF_020.Einc = test_engine.incident_energy\n",
    "\n",
    "get_fpd_kpd_metrics(test_engine.showers, self.showers, False, HLF_020, HLF_035, if_Atlas=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed99efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(scripts.jet_metrics_atlas)\n",
    "importlib.reload(scripts.Jet_metrics)\n",
    "#test_metrics = get_fpd_kpd_metrics(self.showers, self.showers_recon, False, recon_HLF, gt_HLF, if_Atlas=True)\n",
    "ATLAS_hep = HepMetricsAtlas(self)\n",
    "ATLAS_hep.run(self, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d096b7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(scripts.jet_metrics_atlas)\n",
    "from scripts.jet_metrics_atlas import main_run\n",
    "main_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb353774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.evaluate(self.data_mgr.test_loader, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb559917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vae_plots(self.incident_energy, self.showers, self.showers_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92be879",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.model.train()\n",
    "# with torch.no_grad():\n",
    "x,x0 = next(iter(self.data_mgr.train_loader))\n",
    "#reduce batch size to 1 for testing\n",
    "x = x\n",
    "x0 = x0\n",
    "print(\"batch size:\", x.shape[0])\n",
    "print(\"total number of parameters:\", np.log(sum(p.numel() for p in self.model.parameters() if p.requires_grad))) #e^19 for large model\n",
    "print(\"memory before forward pass:\", torch.cuda.memory_allocated(self.device)/1e6, \"MB\")\n",
    "x = x.to(self.device).to(dtype=torch.float32)\n",
    "x0 = x0.to(self.device).to(dtype=torch.float32)\n",
    "x_reduce = self._reduce(x, x0)\n",
    "# Forward pass\n",
    "with torch.profiler.profile(profile_memory=True) as prof:\n",
    "    output = self.model((x_reduce, x0))\n",
    "    print(\"memory after forward pass:\", torch.cuda.memory_allocated(self.device)/1e6, \"MB\")\n",
    "print(prof.key_averages().table(sort_by=\"self_cuda_memory_usage\"))\n",
    "    # prior_samples = self.model.prior.block_gibbs_sampling_cond(p0 = output[2][0])\n",
    "    # _, shower_prior = self.model.decode(prior_samples, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6654fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack([loss_dict[key] * config.engine.loss_coeff[key]  for key in loss_dict.keys() if \"loss\" != key]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d721f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "[loss_dict[key] * config.engine.loss_coeff[key]  for key in loss_dict.keys() if \"loss\" != key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c7806",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dict[\"ae_loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0df1ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.rbm.zephyr import ZephyrRBM, ZephyrRBM_Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861bd8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.rbm.latent_nodes_per_p=512\n",
    "new = ZephyrRBM(config)\n",
    "old = ZephyrRBM_Old(config)\n",
    "\n",
    "binwidth = 1.0\n",
    "new_data, old_data = {}, {}\n",
    "for key in new.weight_dict.keys():\n",
    "    # data[key] = engine.model.prior.weight_dict[key].sign().abs().sum(dim=0).detach().cpu().numpy()\n",
    "    new_data[key] = new._weight_mask_dict[key].abs().sum(dim=0).cpu().numpy()\n",
    "for key in old.weight_dict.keys():\n",
    "    old_data[key] = old._weight_mask_dict[key].abs().sum(dim=0).cpu().numpy()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f2efe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(data, label):\n",
    "    # Create 2x2 grid of subplots\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(10, 7), sharey=True, sharex=True, tight_layout=True)  # 2x2 grid, figure size 10x10\n",
    "    fig.text(0.5, 0.0, 'Couplings/qubit', ha='center', fontsize=15)\n",
    "    fig.text(0.0, 0.4, 'Histogram', rotation=90, ha='center', fontsize=15)\n",
    "\n",
    "    # Plot data on each subplot\n",
    "    labels, counts = np.unique(data['01'], return_counts=True)\n",
    "    axs[0,0].bar(labels, counts, align='center', color=\"b\", alpha=0.8)\n",
    "    # axs[0, 0].hist(data['01'], bins=np.arange(min(data['01']), max(data['01']) + binwidth, binwidth), histtype='stepfilled', linewidth=2.5, color=\"b\", alpha=0.7, align='center')\n",
    "    axs[0,0].grid(\"True\")\n",
    "    axs[0,0].legend([\"v to h\"], fontsize=18)\n",
    "\n",
    "    labels, counts = np.unique(data['02'], return_counts=True)\n",
    "    axs[0,1].bar(labels, counts, align='center', color=\"b\", alpha=0.9)\n",
    "    axs[0,1].grid(\"True\")\n",
    "    axs[0,1].legend([\"v to s\"], fontsize=18)\n",
    "\n",
    "    labels, counts = np.unique(data['03'], return_counts=True)\n",
    "    axs[0,2].bar(labels, counts, align='center', color=\"b\", alpha=0.9)\n",
    "    axs[0,2].grid(\"True\")\n",
    "    axs[0,2].legend([\"v to t\"], fontsize=18)\n",
    "\n",
    "    labels, counts = np.unique(data['12'], return_counts=True)\n",
    "    axs[1,0].bar(labels, counts, align='center', color=\"b\", alpha=0.9)\n",
    "    axs[1,0].grid(\"True\")\n",
    "    axs[1,0].legend([\"h to s\"], fontsize=18)\n",
    "\n",
    "    labels, counts = np.unique(data['13'], return_counts=True)\n",
    "    axs[1,1].bar(labels, counts, align='center', color=\"b\", alpha=0.9)\n",
    "    axs[1,1].grid(\"True\")\n",
    "    axs[1,1].legend([\"h to t\"], fontsize=18)\n",
    "\n",
    "    labels, counts = np.unique(data['23'], return_counts=True)\n",
    "    axs[1,2].bar(labels, counts, align='center', color=\"b\", alpha=0.9)\n",
    "    axs[1,2].grid(\"True\")\n",
    "    axs[1,2].legend([\"s to t\"], fontsize=18)\n",
    "\n",
    "    # plt.savefig(f'/home/javier/Projects/CaloQVAE/figs/weights_plot_zephyr.png', bbox_inches=\"tight\")\n",
    "    plt.suptitle(f\"Weight mask histogram for {label}\", fontsize=20)\n",
    "\n",
    "    plt.show()\n",
    "plot_weights(new_data, \"new algorithm\")\n",
    "plot_weights(old_data, \"old algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3501076",
   "metadata": {},
   "outputs": [],
   "source": [
    "zph = new\n",
    "# plt.hist(torch.cat((zph._weight_mask_dict['01'],zph._weight_mask_dict['02'],zph._weight_mask_dict['03']),1).sum(dim=1).numpy(), bins=20)\n",
    "# plt.show()\n",
    "# plt.hist(torch.cat((zph._weight_mask_dict['01'].transpose(1,0),zph._weight_mask_dict['12'],zph._weight_mask_dict['13']),1).sum(dim=1), bins=20)\n",
    "# plt.show()\n",
    "# plt.hist(torch.cat((zph._weight_mask_dict['02'].transpose(1,0),zph._weight_mask_dict['12'].transpose(1,0),zph._weight_mask_dict['23']),1).sum(dim=1), bins=20)\n",
    "# plt.show()\n",
    "# plt.hist(torch.cat((zph._weight_mask_dict['03'].transpose(1,0),zph._weight_mask_dict['13'].transpose(1,0),zph._weight_mask_dict['23'].transpose(1,0)),1).sum(dim=1), bins=20)\n",
    "# plt.show()\n",
    "def plot_total_weights(zeph, label):\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10, 7), sharey=True, sharex=True, tight_layout=True)  # 2x2 grid, figure size 10x10\n",
    "    fig.text(0.5, 0.0, 'Couplings/qubit', ha='center', fontsize=15)\n",
    "    fig.text(0.0, 0.4, 'Histogram', rotation=90, ha='center', fontsize=15)\n",
    "    ax[0, 0].hist(torch.cat((zeph._weight_mask_dict['01'], zeph._weight_mask_dict['02'], zeph._weight_mask_dict['03']), 1).sum(dim=1).numpy(), bins=20)\n",
    "    ax[0, 0].set_title(\"v to h, s, t\")\n",
    "    ax[0, 1].hist(torch.cat((zeph._weight_mask_dict['01'].transpose(1,0), zeph._weight_mask_dict['12'], zeph._weight_mask_dict['13']), 1).sum(dim=1), bins=20)\n",
    "    ax[0, 1].set_title(\"h to v, s, t\")\n",
    "    ax[1, 0].hist(torch.cat((zeph._weight_mask_dict['02'].transpose(1,0), zeph._weight_mask_dict['12'].transpose(1,0), zeph._weight_mask_dict['23']), 1).sum(dim=1), bins=20)\n",
    "    ax[1, 0].set_title(\"s to v, h, t\")\n",
    "    ax[1, 1].hist(torch.cat((zeph._weight_mask_dict['03'].transpose(1,0), zeph._weight_mask_dict['13'].transpose(1,0), zeph._weight_mask_dict['23'].transpose(1,0)), 1).sum(dim=1), bins=20)\n",
    "    ax[1, 1].set_title(\"t to v, h, s\")\n",
    "    fig.suptitle(f\"Total weight mask histogram for {label}\", fontsize=20)\n",
    "\n",
    "plot_total_weights(new, \"new algorithm\")\n",
    "plot_total_weights(old, \"old algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b9e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights_overlay(new_data, old_data):\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(10, 7), sharey=True, sharex=True, tight_layout=True)\n",
    "    fig.text(0.5, 0.0, 'Couplings/qubit', ha='center', fontsize=15)\n",
    "    fig.text(0.0, 0.4, 'Histogram', rotation=90, ha='center', fontsize=15)\n",
    "\n",
    "    keys = ['01', '02', '03', '12', '13', '23']\n",
    "    titles = [\"v to h\", \"v to s\", \"v to t\", \"h to s\", \"h to t\", \"s to t\"]\n",
    "    \n",
    "    for i, key in enumerate(keys):\n",
    "        ax = axs[i // 3, i % 3]\n",
    "        data_new = new_data[key]\n",
    "        data_old = old_data[key]\n",
    "        # Determine bin edges to align histograms, centering on integers\n",
    "        min_val = min(data_new.min(), data_old.min())\n",
    "        max_val = max(data_new.max(), data_old.max())\n",
    "        bins = np.arange(min_val, max_val + 2) - 0.5  # Shift by 0.5 to center bins on integers\n",
    "        ax.hist(data_new, bins=bins, alpha=0.5, label='new', color='blue')\n",
    "        ax.hist(data_old, bins=bins, alpha=0.5, label='old', color='red')\n",
    "        ax.legend()\n",
    "        ax.set_title(titles[i])\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.suptitle(\"Weight mask histogram comparison\", fontsize=20)\n",
    "    plt.show()\n",
    "plot_weights_overlay(new_data, old_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b675bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_connections(zeph):\n",
    "    total_v = torch.cat((zeph._weight_mask_dict['01'], zeph._weight_mask_dict['02'], zeph._weight_mask_dict['03']), 1).sum(dim=1).cpu().numpy()\n",
    "    total_h = torch.cat((zeph._weight_mask_dict['01'].transpose(1,0), zeph._weight_mask_dict['12'], zeph._weight_mask_dict['13']), 1).sum(dim=1).cpu().numpy()\n",
    "    total_s = torch.cat((zeph._weight_mask_dict['02'].transpose(1,0), zeph._weight_mask_dict['12'].transpose(1,0), zeph._weight_mask_dict['23']), 1).sum(dim=1).cpu().numpy()\n",
    "    total_t = torch.cat((zeph._weight_mask_dict['03'].transpose(1,0), zeph._weight_mask_dict['13'].transpose(1,0), zeph._weight_mask_dict['23'].transpose(1,0)), 1).sum(dim=1).cpu().numpy()\n",
    "    return total_v, total_h, total_s, total_t\n",
    "\n",
    "def plot_total_weights_overlay(new, old):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 7), sharey=True, sharex=True, tight_layout=True)\n",
    "    fig.text(0.5, 0.0, 'Couplings/qubit', ha='center', fontsize=15)\n",
    "    fig.text(0.0, 0.4, 'Histogram', rotation=90, ha='center', fontsize=15)\n",
    "\n",
    "    total_new = compute_total_connections(new)\n",
    "    total_old = compute_total_connections(old)\n",
    "\n",
    "    all_data = np.concatenate((total_new[0], total_new[1], total_new[2], total_new[3],\n",
    "                               total_old[0], total_old[1], total_old[2], total_old[3]))\n",
    "    min_val = int(all_data.min())\n",
    "    max_val = int(all_data.max())\n",
    "    bins = np.arange(min_val, max_val + 1, 1)  #\n",
    "\n",
    "    titles = [\"v to h, s, t\", \"h to v, s, t\", \"s to v, h, t\", \"t to v, h, s\"]\n",
    "    \n",
    "    for i in range(len(titles)):\n",
    "        ax = axs[i // 2, i % 2]\n",
    "        data_new = total_new[i]\n",
    "        data_old = total_old[i]\n",
    "        ax.hist(data_new, bins=bins, alpha=0.5, label='new', color='blue', align='mid')\n",
    "        ax.hist(data_old, bins=bins, alpha=0.5, label='old', color='red', align='mid')\n",
    "        ax.set_xticks(bins)  # Integer ticks\n",
    "        ax.legend()\n",
    "        ax.set_title(titles[i])\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.suptitle(\"Total weight mask histogram comparison\", fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_total_weights_overlay(new, old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd36bb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assume 'run_path' is the path you passed to the flush method\n",
    "# and the file was saved as 'hep_metrics.npz' within it.\n",
    "file_path = \"/home/leozhu/CaloQuVAE/wandb-outputs/run-20250718_163200-em9a1ujl/files/JetData.npz\"\n",
    "\n",
    "# Load the .npz file\n",
    "loaded_data = np.load(file_path)\n",
    "\n",
    "# You can see the names of the arrays stored in the file\n",
    "print(\"Arrays in file:\", loaded_data.files)\n",
    "\n",
    "# Assign the data to variables based on the original order\n",
    "# (en_list, fpd_recon, fpd_recon_err, kpd_recon, kpd_recon_err, fpd_sample, ...)\n",
    "en_list = loaded_data['array1']\n",
    "fpd_recon = loaded_data['array2']\n",
    "fpd_recon_err = loaded_data['array3']\n",
    "kpd_recon = loaded_data['array4']\n",
    "kpd_recon_err = loaded_data['array5']\n",
    "fpd_sample = loaded_data['array6']\n",
    "fpd_sample_err = loaded_data['array7']\n",
    "kpd_sample = loaded_data['array8']\n",
    "kpd_sample_err = loaded_data['array9']\n",
    "\n",
    "print(\"\\nLoaded Epochs:\", en_list)\n",
    "print(\"Loaded FPD Sample values:\", fpd_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0601f4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make plots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 6))\n",
    "ax[0].errorbar(en_list, fpd_recon, yerr=fpd_recon_err, label='FPD Recon', color='c')\n",
    "ax[0].errorbar(en_list, fpd_sample, yerr=fpd_sample_err, label='FPD Sample', color='orange')\n",
    "ax[0].set_xlabel('Epochs')\n",
    "ax[0].set_ylabel('FPD Values')\n",
    "ax[0].set_title('FPD Reconstruction and Sample over Epochs')\n",
    "ax[0].legend()\n",
    "ax[1].errorbar(en_list, kpd_recon, yerr=kpd_recon_err, label='KPD Recon', color='c')\n",
    "ax[1].errorbar(en_list, kpd_sample, yerr=kpd_sample_err, label='KPD Sample', color='orange')\n",
    "ax[1].set_xlabel('Epochs')\n",
    "ax[1].set_ylabel('KPD Values')\n",
    "ax[1].set_title('KPD Reconstruction and Sample over Epochs')\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e355a415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
