{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba551643",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[22:14:10.794]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mCaloQuVAE                                         \u001b[0mLoading configuration.\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "import dwave_networkx as dnx\n",
    "import minorminer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import random\n",
    "from dwave.system import DWaveSampler\n",
    "import sys\n",
    "from minorminer import find_embedding\n",
    "from dwave.embedding.zephyr import find_biclique_embedding\n",
    "from model.rbm.rbm_two_partite import RBM_TwoPartite\n",
    "import torch\n",
    "import torch\n",
    "from hydra.utils import instantiate\n",
    "from hydra import initialize, compose\n",
    "import hydra\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "from scripts.run import setup_model, load_model_instance\n",
    "from utils.rbm_plots import rbm_to_networkx, plot_pruning_analysis\n",
    "import dwave_networkx as dnx\n",
    "import dwave.inspector\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "778ab2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[22:15:45.931]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.config.models                         \u001b[0mInvalid solver JSON, parsing as string identity: 'Advantage2_system1.9'\n",
      "\u001b[1m[22:15:46.211]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mFetching definition of a solver with name='Advantage2_system1.9'\n",
      "\u001b[1m[22:15:46.234]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mReceived solver data for 1 solver(s).\n",
      "\u001b[1m[22:15:46.235]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(name='Advantage2_system1.9', graph_id='014a2885f7')\n"
     ]
    }
   ],
   "source": [
    "solver_name = \"Advantage2_system1.9\"\n",
    "TARGET_SOLVER = solver_name\n",
    "try:\n",
    "    target_sampler = DWaveSampler(solver=TARGET_SOLVER)\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing sampler: {e}\")\n",
    "    print(\"Please ensure you have D-Wave credentials configured.\")\n",
    "\n",
    "# Use the sampler's graph directly\n",
    "working_graph = target_sampler.to_networkx_graph() \n",
    "\n",
    "try:\n",
    "    left_chains, right_chains = find_biclique_embedding(\n",
    "        75, 75, target_graph=working_graph\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error during find_biclique_embedding: {e}\")\n",
    "    print(\"This often means the graph is too large for the QPU.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5e761fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=\"config\")\n",
    "cfg=compose(config_name=\"config.yaml\")\n",
    "new_model = False\n",
    "if new_model:\n",
    "    self = setup_model(config)\n",
    "else:\n",
    "    config = OmegaConf.load(cfg.config_path)\n",
    "    config.gpu_list = cfg.gpu_list\n",
    "    config.load_state = cfg.load_state\n",
    "    self = setup_model(config)\n",
    "    self._model_creator.load_state(config.run_path, self.device)\n",
    "\n",
    "# wandb.init(tags = [cfg.data.dataset_name], project=cfg.wandb.project, entity=cfg.wandb.entity, config=OmegaConf.to_container(cfg, resolve=True), mode='disabled')\n",
    "dummy_data = torch.zeros(1, config.rbm.latent_nodes_per_p).to(self.device)\n",
    "CHECKPOINT_FILE = \"/home/leozhu/CaloQuVAE/wandb-outputs/run_2025-11-09_19-19-34_RBM_TwoPartite/training_checkpoint.h5\"\n",
    "rbm = RBM_TwoPartite(config, data=dummy_data)\n",
    "\n",
    "try:\n",
    "    # Load the latest epoch (epoch=None)\n",
    "    loaded_epoch = rbm.load_checkpoint(CHECKPOINT_FILE, epoch=None) \n",
    "    print(f\"Successfully loaded checkpoint from epoch {loaded_epoch}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading checkpoint: {e}\")\n",
    "\n",
    "weights = rbm.params[\"weight_matrix\"]\n",
    "vbias = rbm.params[\"vbias\"]\n",
    "print(f\"  Weight matrix shape: {weights.shape}\")\n",
    "print(f\"  Visible bias mean: {vbias.mean().item():.4f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "831d0247",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_target = dnx.zephyr_graph(12)   # choose m that gives enough qubits (increase m if needed)\n",
    "print(f\"Target zephyr_graph (m=12): {G_target.number_of_nodes()} nodes, {G_target.number_of_edges()} edges\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "39a19205",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Running Pruning Analysis Plot ---\")\n",
    "# Let's check tolerances up to 0.05\n",
    "plot_pruning_analysis(rbm, max_tolerance=0.2, num_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7779c877",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOLERANCE_CUTOFF = 0.20\n",
    "print(f\"\\n--- Converting to NetworkX Graph (tolerance={TOLERANCE_CUTOFF}) ---\")\n",
    "G = rbm_to_networkx(rbm, tolerance=TOLERANCE_CUTOFF)\n",
    "\n",
    "# You can now analyze this graph\n",
    "print(f\"Graph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges.\")\n",
    "# For example, find the visible node with the most connections\n",
    "vis_nodes = {n for n, d in G.nodes(data=True) if d['bipartite'] == 0}\n",
    "degrees = G.degree(vis_nodes)\n",
    "if degrees:\n",
    "    max_deg_node, max_deg = max(degrees, key=lambda item: item[1])\n",
    "    print(f\"Visible node {max_deg_node} has the highest degree: {max_deg}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6314ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_SOLVER = \"Advantage2_system1.7\" \n",
    "sampler = DWaveSampler(solver=TARGET_SOLVER)\n",
    "target_edgelist = sampler.structure.edgelist\n",
    "\n",
    "print(f\" -> Fetched working graph with {len(sampler.structure.nodelist)} active qubits.\")\n",
    "print(f\" -> Fetched working graph with {len(target_edgelist)} active couplers.\\n\")\n",
    "\n",
    "source_edgelist = list(G.edges())\n",
    "target_edgelist = sampler.structure.edgelist\n",
    "\n",
    "embedding = find_embedding(source_edgelist, target_edgelist)\n",
    "\n",
    "if not embedding:\n",
    "    print(\"Result: FAILED\")\n",
    "    print(f\"minorminer.find_embedding returned an empty dictionary.\")\n",
    "    print(f\"graph could NOT be embedded onto {TARGET_SOLVER}.\")\n",
    "else:\n",
    "    print(\"Result: SUCCESS\")\n",
    "    print(f\"graph WAS successfully embedded!\")\n",
    "    print(\"\\nEmbedding Details:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "af9856e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_target = sampler.to_networkx_graph()\n",
    "dwave.inspector.show(\n",
    "        G,      # Your problem graph\n",
    "        embedding,     # Your embedding dictionary\n",
    "        G_target       # The hardware graph\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f04f175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_embedding(rbm_size, solver_name):\n",
    "    \"\"\"\n",
    "    Performs the biclique embedding on the target QPU.\n",
    "    Returns the sampler, the working graph, and the chains.\n",
    "    \"\"\"\n",
    "    print(f\"--- 1. Running Embedding for K_{rbm_size},{rbm_size} on {solver_name} ---\")\n",
    "    TARGET_SOLVER = solver_name\n",
    "    try:\n",
    "        target_sampler = DWaveSampler(solver=TARGET_SOLVER)\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing sampler: {e}\")\n",
    "        print(\"Please ensure you have D-Wave credentials configured.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    # Use the sampler's graph directly\n",
    "    working_graph = target_sampler.to_networkx_graph() \n",
    "    \n",
    "    if not working_graph:\n",
    "        print(\"Could not fetch working graph from sampler.\")\n",
    "        return target_sampler, None, None, None, None\n",
    "\n",
    "    print(f\"Successfully fetched QPU graph with {len(working_graph.nodes)} nodes.\")\n",
    "\n",
    "    try:\n",
    "        left_chains, right_chains = find_biclique_embedding(\n",
    "            rbm_size, rbm_size, target_graph=working_graph\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error during find_biclique_embedding: {e}\")\n",
    "        print(\"This often means the graph is too large for the QPU.\")\n",
    "        return target_sampler, working_graph, None, None, None\n",
    "\n",
    "    all_chains = list(left_chains.values()) + list(right_chains.values())\n",
    "    max_len = max(len(chain) for chain in all_chains)\n",
    "\n",
    "    # This is the set of ALL qubits used in the 76x76 embedding\n",
    "    qubits_used = set()\n",
    "    for chain in all_chains:\n",
    "        qubits_used.update(chain)\n",
    "\n",
    "    print(f\" -> Max chain length: {max_len}\")\n",
    "    print(f\" -> Total qubits used: {len(qubits_used)}\")\n",
    "    return target_sampler, working_graph, qubits_used, left_chains, right_chains\n",
    "\n",
    "def build_neighbor_sets(target_sampler, target_logical_nodes, qubits_used):\n",
    "    \"\"\"\n",
    "    Builds the 76 \"available neighbor\" sets, V_i.\n",
    "    V_i = {all *available* qubits adjacent to logical node i}\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- 2. Building {len(target_logical_nodes)} Neighbor Sets ---\")\n",
    "    \n",
    "    # 1. Get all qubits available on the chip\n",
    "    all_physical_qubits = set(target_sampler.nodelist)\n",
    "    qubits_avail = all_physical_qubits - qubits_used\n",
    "    print(f\"Total available qubits: {len(qubits_avail)}\")\n",
    "\n",
    "    # 2. Get the adjacency property of the sampler\n",
    "    qpu_adjacency = target_sampler.adjacency\n",
    "    \n",
    "    neighbor_sets = []\n",
    "    min_adj_size = float('inf')\n",
    "    \n",
    "    # 3. Iterate over each of the 76 logical nodes in Side A\n",
    "    for i, chain in enumerate(target_logical_nodes):\n",
    "        \n",
    "        # This set will hold all *available* neighbors for this one logical node\n",
    "        chain_available_neighbors = set()\n",
    "        \n",
    "        # 4. Check neighbors for every physical qubit in the chain\n",
    "        for q_in_chain in chain:\n",
    "            for neighbor in qpu_adjacency[q_in_chain]:\n",
    "                # 5. If the neighbor is in the available set, add it\n",
    "                if neighbor in qubits_avail:\n",
    "                    chain_available_neighbors.add(neighbor)\n",
    "        \n",
    "        neighbor_sets.append(chain_available_neighbors)\n",
    "        \n",
    "        if len(chain_available_neighbors) < min_adj_size:\n",
    "            min_adj_size = len(chain_available_neighbors)\n",
    "            \n",
    "    print(f\"All 76 neighbor sets built.\")\n",
    "    print(f\"The 'bottleneck' (min neighbors) is: {min_adj_size}\")\n",
    "    print(f\"This is the *absolute upper bound* on the number of nodes.\")\n",
    "    \n",
    "    # This is V_all, the total pool of qubits we can *ever* use\n",
    "    all_available_neighbors = set().union(*neighbor_sets)\n",
    "    \n",
    "    return neighbor_sets, all_available_neighbors\n",
    "\n",
    "def find_max_disjoint_hitting_sets_heuristic(neighbor_sets, all_available_neighbors):\n",
    "    \"\"\"\n",
    "    Heuristic algorithm to find the maximum number of disjoint hitting sets.\n",
    "    \n",
    "    Inputs:\n",
    "    - neighbor_sets (list of sets): The 76 sets V_i.\n",
    "    - all_available_neighbors (set): The pool of all qubits we can use.\n",
    "    \n",
    "    Returns:\n",
    "    - N_nodes (int): The estimated max number of conditioning nodes.\n",
    "    - all_found_nodes (list of sets): The physical qubit sets for each node.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- 3. Running Greedy Heuristic for Max Disjoint Hitting Sets ---\")\n",
    "    \n",
    "    N_nodes_found = 0\n",
    "    all_found_nodes = []\n",
    "    \n",
    "    # Make a copy so we can safely modify it\n",
    "    qubit_pool = all_available_neighbors.copy()\n",
    "    num_logical_nodes = len(neighbor_sets)\n",
    "    \n",
    "    # --- Outer Greedy Loop ---\n",
    "    # Try to build nodes one by one\n",
    "    while True:\n",
    "        \n",
    "        # --- Inner Greedy Loop ---\n",
    "        # Try to build *one* valid conditioning node (a hitting set)\n",
    "        current_hitting_set = set()\n",
    "        \n",
    "        # Indices of the logical nodes (0 to 75) we still need to hit\n",
    "        sets_to_hit_indices = set(range(num_logical_nodes))\n",
    "        \n",
    "        # We need a pool of qubits *available for this node*\n",
    "        # This pool will shrink as we build the current_hitting_set\n",
    "        current_qubit_pool = qubit_pool.copy()\n",
    "        \n",
    "        while sets_to_hit_indices:\n",
    "            # 1. Find the \"best\" qubit to add\n",
    "            best_qubit = None\n",
    "            max_hits = -1\n",
    "            \n",
    "            # This is the greedy \"Set Cover\" part:\n",
    "            # Check every available qubit...\n",
    "            for q in current_qubit_pool:\n",
    "                current_hits = 0\n",
    "                # ...to see how many *un-hit* logical nodes it hits\n",
    "                for i in sets_to_hit_indices:\n",
    "                    if q in neighbor_sets[i]:\n",
    "                        current_hits += 1\n",
    "                \n",
    "                if current_hits > max_hits:\n",
    "                    max_hits = current_hits\n",
    "                    best_qubit = q\n",
    "            \n",
    "            # 2. Check if we failed\n",
    "            if max_hits == 0:\n",
    "                # We failed to build a complete hitting set\n",
    "                # The remaining qubits in current_qubit_pool\n",
    "                # cannot hit the remaining sets_to_hit_indices.\n",
    "                # Break the inner loop (this node fails)\n",
    "                break\n",
    "                \n",
    "            # 3. Add the best qubit to our node\n",
    "            current_hitting_set.add(best_qubit)\n",
    "            \n",
    "            # 4. Remove it from the pool for *this node*\n",
    "            current_qubit_pool.remove(best_qubit)\n",
    "            \n",
    "            # 5. Update the list of nodes we still need to hit\n",
    "            indices_hit = set()\n",
    "            for i in sets_to_hit_indices:\n",
    "                if best_qubit in neighbor_sets[i]:\n",
    "                    indices_hit.add(i)\n",
    "            \n",
    "            sets_to_hit_indices.difference_update(indices_hit)\n",
    "            \n",
    "        # --- End of Inner Loop ---\n",
    "        \n",
    "        if not sets_to_hit_indices:\n",
    "            # SUCCESS! We hit all 76 nodes.\n",
    "            N_nodes_found += 1\n",
    "            all_found_nodes.append(current_hitting_set)\n",
    "            \n",
    "            # Now, permanently remove these qubits from the *global* pool\n",
    "            qubit_pool.difference_update(current_hitting_set)\n",
    "            \n",
    "            # print(f\"  -> Found conditioning node {N_nodes_found} (size {len(current_hitting_set)})\")\n",
    "        else:\n",
    "            # FAILURE. We couldn't build a new node.\n",
    "            # The remaining qubits in qubit_pool are not sufficient.\n",
    "            # Break the outer loop\n",
    "            break\n",
    "            \n",
    "    # --- End of Outer Loop ---\n",
    "    \n",
    "    return N_nodes_found, all_found_nodes\n",
    "\n",
    "def analyze_target_side(side_name, sampler, target_nodes_chains, qubits_used):\n",
    "    \"\"\"\n",
    "    Runs the full analysis (build sets + run heuristic) for a given side.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"===\" * 15)\n",
    "    print(f\"--- Analyzing Target Side: {side_name} ---\")\n",
    "    print(\"===\" * 15)\n",
    "    \n",
    "    # Build the V_i sets\n",
    "    v_sets, v_all = build_neighbor_sets(sampler, target_nodes_chains, qubits_used)\n",
    "    \n",
    "    if not v_all:\n",
    "        print(\"\\nNo available neighbors found for any target node on this side.\")\n",
    "        print(f\"Estimated max conditioning nodes for {side_name}: 0\")\n",
    "        return 0\n",
    "\n",
    "    # Run the heuristic\n",
    "    num_nodes, node_qubit_sets = find_max_disjoint_hitting_sets_heuristic(v_sets, v_all)\n",
    "    \n",
    "    print(\"---\" * 10)\n",
    "    print(f\"Heuristic Result for {side_name}: {num_nodes}\")\n",
    "    print(f\"   (Estimated max number of conditioning nodes)\")\n",
    "    print(\"---\" * 10)\n",
    "    \n",
    "    if num_nodes > 0:\n",
    "        print(\"\\nPhysical qubit set sizes for each found node:\")\n",
    "        print(sum([len(s) for s in node_qubit_sets]))\n",
    "    \n",
    "    return num_nodes, node_qubit_sets\n",
    "\n",
    "\n",
    "\n",
    "def visualize_embedding(sampler, graph, left_chains_dict, right_chains_dict, conditioning_sets, colors):\n",
    "    \"\"\"\n",
    "    Draws the QPU graph with all embedded nodes colored using draw_zephyr_embedding.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- 4. Generating Visualization ---\")\n",
    "    \n",
    "    # 1. Build the 'emb' (embedding) dictionary\n",
    "    #    This maps a unique logical node ID to its physical chain (or set)\n",
    "    emb = {}\n",
    "    \n",
    "    # Add left chains: {'L_0': [q1, q2], 'L_1': [q3, q4], ...}\n",
    "    for logical_node, chain in left_chains_dict.items():\n",
    "        emb[f'L_{logical_node}'] = chain\n",
    "        \n",
    "    # Add right chains: {'R_0': [q5, q6], 'R_1': [q7, q8], ...}\n",
    "    for logical_node, chain in right_chains_dict.items():\n",
    "        emb[f'R_{logical_node}'] = chain\n",
    "        \n",
    "    # Add conditioning nodes: {'C_0': {q9, q10}, 'C_1': {q11, q12}, ...}\n",
    "    for i, q_set in enumerate(conditioning_sets):\n",
    "        emb[f'C_{i}'] = q_set # The function accepts iterables (sets are fine)\n",
    "\n",
    "    # 2. Build the 'chain_color' dictionary\n",
    "    #    This maps the same unique logical IDs to their colors\n",
    "    chain_color = {}\n",
    "    for logical_node in emb:\n",
    "        if logical_node.startswith('L_'):\n",
    "            chain_color[logical_node] = colors['left']\n",
    "        elif logical_node.startswith('R_'):\n",
    "            chain_color[logical_node] = colors['right']\n",
    "        elif logical_node.startswith('C_'):\n",
    "            chain_color[logical_node] = colors['cond']\n",
    "\n",
    "    print(\"Embedding and color map created. Drawing plot (this may take a moment)...\")\n",
    "\n",
    "    # 3. Draw the graph using the correct function\n",
    "    plt.figure(figsize=(18, 18))\n",
    "    \n",
    "    dnx.draw_zephyr_embedding(\n",
    "        graph, \n",
    "        # sampler=sampler,\n",
    "        emb=emb,\n",
    "        chain_color=chain_color,\n",
    "        unused_color=colors['unused_tuple'], # Must be an RGBA tuple\n",
    "        node_size=10,\n",
    "        show_labels=False, # This is the correct param instead of with_labels\n",
    "        width=0.5 # Edge line width\n",
    "        # REMOVED: edge_color=colors['edges'] -- This caused the TypeError\n",
    "    )\n",
    "    \n",
    "    # 4. Create a legend\n",
    "    # We still create the legend manually, but we get the counts\n",
    "    # for a more informative label.\n",
    "    left_qubits = set()\n",
    "    for chain in left_chains_dict.values():\n",
    "        left_qubits.update(chain)\n",
    "        \n",
    "    right_qubits = set()\n",
    "    for chain in right_chains_dict.values():\n",
    "        right_qubits.update(chain)\n",
    "        \n",
    "    conditioning_qubits = set()\n",
    "    for q_set in conditioning_sets:\n",
    "        conditioning_qubits.update(q_set)\n",
    "        \n",
    "    # We create dummy plots for the legend handles\n",
    "    l_patch = plt.Line2D([0], [0], marker='o', color='w', label=f'Left Chains ({len(left_qubits)} qubits)',\n",
    "                          markerfacecolor=colors['left'], markersize=10)\n",
    "    r_patch = plt.Line2D([0], [0], marker='o', color='w', label=f'Right Chains ({len(right_qubits)} qubits)',\n",
    "                          markerfacecolor=colors['right'], markersize=10)\n",
    "    c_patch = plt.Line2D([0], [0], marker='o', color='w', label=f'Conditioning Nodes ({len(conditioning_qubits)} qubits)',\n",
    "                          markerfacecolor=colors['cond'], markersize=10)\n",
    "    u_patch = plt.Line2D([0], [0], marker='o', color='w', label='Unused Qubits',\n",
    "                          markerfacecolor=colors['unused'], markersize=10) # Use hex for legend\n",
    "    \n",
    "    plt.legend(handles=[l_patch, r_patch, c_patch, u_patch], loc='upper right', fontsize=18)\n",
    "    plt.title(f\"Embedding on {sampler.solver.name}\", fontsize=20)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3c6dffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "RBM_SIZE = 76\n",
    "SOLVER = \"Advantage2_system1.7\"\n",
    "\n",
    "COLORS = {\n",
    "    'left': '#FF5733',  # Orange-Red\n",
    "    'right': '#33FF57', # Green\n",
    "    'cond': '#3357FF',  # Blue\n",
    "    'unused': '#E0E0E0', # Light Gray (for legend)\n",
    "    'unused_tuple': (0.878, 0.878, 0.878, 1.0), # RGBA tuple for the function\n",
    "    'edges': '#B0B0B0'  # Medium Gray\n",
    "}\n",
    "\n",
    "sampler, working_graph, q_used, l_chains, r_chains = run_embedding(RBM_SIZE, SOLVER)\n",
    "\n",
    "if l_chains is None or r_chains is None:\n",
    "    print(\"\\nEmbedding failed. Exiting.\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "# --- Analyze Left Chains ---\n",
    "target_nodes_left = list(l_chains.values())\n",
    "num_nodes_left, cond_sets_left = analyze_target_side(\n",
    "    \"Left Chains\", sampler, target_nodes_left, q_used\n",
    ")\n",
    "\n",
    "# --- Analyze Right Chains ---\n",
    "target_nodes_right = list(r_chains.values())\n",
    "num_nodes_right, cond_sets_right = analyze_target_side(\n",
    "    \"Right Chains\", sampler, target_nodes_right, q_used\n",
    ")\n",
    "\n",
    "# --- Final Summary ---\n",
    "print(\"\\n\" + \"===\" * 15)\n",
    "print(\"--- Overall Summary ---\")\n",
    "print(f\"Max nodes if connecting to Left Chains:  {num_nodes_left}\")\n",
    "print(f\"Max nodes if connecting to Right Chains: {num_nodes_right}\")\n",
    "\n",
    "best_cond_sets = []\n",
    "if num_nodes_left >= num_nodes_right:\n",
    "    print(\"\\n The 'Left Chains' side appears to be the better choice.\")\n",
    "    best_cond_sets = cond_sets_left\n",
    "else:\n",
    "    print(\"\\nThe 'Right Chains' side appears to be the better choice.\")\n",
    "    best_cond_sets = cond_sets_right\n",
    "print(\"===\" * 15)\n",
    "\n",
    "# --- Visualize the Result ---\n",
    "if working_graph:\n",
    "    visualize_embedding(\n",
    "        sampler,\n",
    "        working_graph,\n",
    "        l_chains,\n",
    "        r_chains,\n",
    "        best_cond_sets,\n",
    "        COLORS\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nCannot visualize: working_graph is not defined.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62ea13e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the embedding of a 76x76 bipartite RBM onto the D-Wave Advantage2_system1.7 QPU\n",
    "rbm_size = 76\n",
    "TARGET_SOLVER = \"Advantage2_system1.7\"\n",
    "target_sampler = DWaveSampler(solver=TARGET_SOLVER)\n",
    "working_graph = target_sampler.to_networkx_graph()\n",
    "\n",
    "left_chains, right_chains = find_biclique_embedding(\n",
    "                rbm_size, rbm_size, target_graph=working_graph\n",
    "            )\n",
    "all_chains = list(left_chains.values()) + list(right_chains.values())\n",
    "max_len = max(len(chain) for chain in all_chains)\n",
    "num_qubits_used = sum(len(chain) for chain in all_chains)\n",
    "\n",
    "# This is the set of ALL qubits used in the 76x76 embedding\n",
    "qubits_used = set()\n",
    "for chain in all_chains:\n",
    "    qubits_used.update(chain)\n",
    "\n",
    "print(f\" -> Max chain length: {max_len}\")\n",
    "print(f\" -> Total qubits used: {len(qubits_used)}\")\n",
    "max_practical_a = rbm_size\n",
    "# plot distribution of chain lengths\n",
    "chain_lengths = [len(chain) for chain in all_chains]\n",
    "plt.hist(chain_lengths, bins=range(1, max_len + 2), align='left', rwidth=0.8)\n",
    "plt.title(f\"Chain Length Distribution for K_{rbm_size},{rbm_size} on {TARGET_SOLVER}\")\n",
    "plt.xlabel(\"Chain Length\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Calculating Max Conditioning Nodes ---\")\n",
    "\n",
    "# 1. Get all qubits available on the chip\n",
    "all_physical_qubits = set(target_sampler.nodelist)\n",
    "qubits_avail = all_physical_qubits - qubits_used\n",
    "print(f\"Total available qubits: {len(qubits_avail)}\")\n",
    "\n",
    "# 2. Get the adjacency property of the sampler\n",
    "qpu_adj = target_sampler.adjacency\n",
    "\n",
    "# 3. This is the side (A) we want to connect to.\n",
    "#    We assume it's 'left_chains'.\n",
    "target_logical_nodes = left_chains.values() \n",
    "\n",
    "min_adj_size = float('inf')\n",
    "bottleneck_node_chain = None\n",
    "\n",
    "# 4. Iterate over each of the 76 logical nodes in Side A\n",
    "for i, chain in enumerate(target_logical_nodes):\n",
    "    \n",
    "    # This set will hold all *available* neighbors for this one logical node\n",
    "    chain_available_neighbors = set()\n",
    "    \n",
    "    # 5. Check neighbors for every physical qubit in the chain\n",
    "    for q_in_chain in chain:\n",
    "        for neighbor in qpu_adj[q_in_chain]:\n",
    "            # 6. If the neighbor is in the available set, add it\n",
    "            if neighbor in qubits_avail:\n",
    "                chain_available_neighbors.add(neighbor)\n",
    "    \n",
    "    current_adj_size = len(chain_available_neighbors)\n",
    "    # print(f\"Logical node {i} (chain len {len(chain)}) has {current_adj_size} available neighbors.\")\n",
    "\n",
    "    # 7. Check if this is our new bottleneck\n",
    "    if current_adj_size < min_adj_size:\n",
    "        min_adj_size = current_adj_size\n",
    "        bottleneck_node_chain = chain\n",
    "\n",
    "print(\"---\" * 10)\n",
    "print(f\"The bottleneck is a logical node with {min_adj_size} available neighbors.\")\n",
    "if bottleneck_node_chain:\n",
    "    print(f\"(Chain: {bottleneck_node_chain})\")\n",
    "\n",
    "print(f\"\\nâœ… Maximum number of conditioning nodes you can add: {min_adj_size}\")\n",
    "print(\"---\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e81781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "# The logical graph we want to embed\n",
    "BIPARTITE_N = 184\n",
    "# The specific D-Wave Advantage2 solver to target\n",
    "# As of May 2025, this is the GA Advantage2 solver \n",
    "TARGET_SOLVER = \"Advantage2_system1.7\" \n",
    "\n",
    "print(f\"--- STARTING EMBEDDING TEST ---\")\n",
    "print(f\"Attempting to embed a K_{BIPARTITE_N},{BIPARTITE_N} graph.\")\n",
    "print(f\"Targeting D-Wave solver: {TARGET_SOLVER}\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# STEP 1: FETCH THE TARGET HARDWARE GRAPH (THE 'WORKING GRAPH')\n",
    "# ------------------------------------------------------------------\n",
    "try:\n",
    "    # Instantiate the DWaveSampler for the specific solver \n",
    "    # This requires your API key to be configured.\n",
    "    sampler = DWaveSampler(solver=TARGET_SOLVER)\n",
    "    \n",
    "    # Get the target graph structure.\n",
    "    # The 'solver.structure' attribute contains the 'nodelist', 'edgelist', \n",
    "    # and 'adjacency' of the QPU's working graph.\n",
    "    # We only need the edgelist for minorminer.\n",
    "    print(f\"Successfully connected to {TARGET_SOLVER}.\")\n",
    "    print(f\"Fetching hardware 'working graph'...\")\n",
    "    \n",
    "    # This is the edgelist of the *actual* hardware\n",
    "    target_edgelist = sampler.structure.edgelist\n",
    "    \n",
    "    # For context, let's see how many qubits/couplers are active.\n",
    "    # This will NOT be the theoretical 4,800 qubits.\n",
    "    print(f\" -> Fetched working graph with {len(sampler.structure.nodelist)} active qubits.\")\n",
    "    print(f\" -> Fetched working graph with {len(target_edgelist)} active couplers.\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: Could not connect to D-Wave solver '{TARGET_SOLVER}'.\")\n",
    "    print(f\"Please check your API key and solver name.\")\n",
    "    print(f\"Full error: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# STEP 2: DEFINE THE LOGICAL SOURCE GRAPH\n",
    "# ------------------------------------------------------------------\n",
    "print(f\"Creating logical source graph: K_{BIPARTITE_N},{BIPARTITE_N}...\")\n",
    "\n",
    "# Create the complete bipartite graph K_184,184 using networkx\n",
    "# \n",
    "G_source = nx.complete_bipartite_graph(BIPARTITE_N, BIPARTITE_N)\n",
    "\n",
    "# The minorminer 'find_embedding' function takes an edgelist as input\n",
    "# for the source graph [14]\n",
    "source_edgelist = list(G_source.edges())\n",
    "\n",
    "print(f\" -> Source graph has {G_source.number_of_nodes()} nodes.\")\n",
    "print(f\" -> Source graph has {G_source.number_of_edges()} edges.\\n\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# STEP 3 & 4: RUN THE EMBEDDING HEURISTIC & ANALYZE RESULT\n",
    "# ------------------------------------------------------------------\n",
    "print(f\"Attempting to find embedding... (This may take some time)\")\n",
    "\n",
    "# Call the minorminer heuristic \n",
    "# S = source graph (edgelist)\n",
    "# T = target graph (edgelist)\n",
    "try:\n",
    "    embedding = find_embedding(source_edgelist, target_edgelist)\n",
    "    \n",
    "    print(\"\\n--- EMBEDDING TEST COMPLETE ---\")\n",
    "    \n",
    "    # CRITICAL: How to check for failure.\n",
    "    # The minorminer heuristic returns an EMPTY DICTIONARY\n",
    "    # if it fails to find an embedding.[14]\n",
    "    if not embedding:\n",
    "        print(\"Result: FAILED\")\n",
    "        print(f\"minorminer.find_embedding returned an empty dictionary.\")\n",
    "        print(f\"A K_{BIPARTITE_N},{BIPARTITE_N} graph could NOT be embedded onto {TARGET_SOLVER}.\")\n",
    "    else:\n",
    "        print(\"Result: SUCCESS\")\n",
    "        print(f\"A K_{BIPARTITE_N},{BIPARTITE_N} graph WAS successfully embedded!\")\n",
    "        print(\"\\nEmbedding Details:\")\n",
    "        \n",
    "        # Calculate and display max chain length\n",
    "        chain_lengths = [len(chain) for chain in embedding.values()]\n",
    "        max_chain_len = max(chain_lengths)\n",
    "        avg_chain_len = sum(chain_lengths) / len(chain_lengths)\n",
    "        \n",
    "        print(f\" -> Maximum chain length: {max_chain_len}\")\n",
    "        print(f\" -> Average chain length: {avg_chain_len:.2f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred during the embedding process: {e}\")\n",
    "\n",
    "print(\"--- END OF SCRIPT ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da831bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "STARTING_A = 76  # Start from the theoretical maximum (K_184,184)\n",
    "ENDING_A = 70    # Stop searching if we can't even embed this\n",
    "TARGET_SOLVER = \"Advantage2_system1.9\" \n",
    "\n",
    "print(\"--- STARTING ITERATIVE EMBEDDING SEARCH ---\")\n",
    "print(f\"Targeting D-Wave solver: {TARGET_SOLVER}\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# STEP 1: FETCH THE TARGET HARDWARE GRAPH (ONCE)\n",
    "# ------------------------------------------------------------------\n",
    "try:\n",
    "    sampler = DWaveSampler(solver=TARGET_SOLVER)\n",
    "    target_edgelist = sampler.structure.edgelist\n",
    "    print(f\"Successfully connected to {TARGET_SOLVER}.\")\n",
    "    print(f\" -> Fetched working graph with {len(sampler.structure.nodelist)} active qubits.\")\n",
    "    print(f\" -> Fetched working graph with {len(target_edgelist)} active couplers.\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error connecting to D-Wave solver: {e}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# STEP 2: ITERATIVELY SEARCH FOR MAX K_a,a\n",
    "# ------------------------------------------------------------------\n",
    "max_embedded_a = None\n",
    "\n",
    "for a in range(STARTING_A, ENDING_A - 1, -1):\n",
    "    print(f\"----- TESTING K_{a},{a} -----\")\n",
    "    \n",
    "    # 1. Create the source graph\n",
    "    G_source = nx.complete_bipartite_graph(a, a)\n",
    "    source_edgelist = list(G_source.edges())\n",
    "    print(f\" -> Source graph has {G_source.number_of_nodes()} nodes.\")\n",
    "    \n",
    "    # 2. Run the embedding heuristic\n",
    "    try:\n",
    "        embedding = find_embedding(source_edgelist, target_edgelist)\n",
    "        \n",
    "        # 3. Check for success\n",
    "        if not embedding:\n",
    "            print(f\" -> Result: FAILED (minorminer returned empty dict)\")\n",
    "        else:\n",
    "            print(f\" -> Result: SUCCESS!\")\n",
    "            print(f\"Found a valid embedding for K_{a},{a}.\")\n",
    "            \n",
    "            chain_lengths = [len(chain) for chain in embedding.values()]\n",
    "            print(f\" -> Max chain length: {max(chain_lengths)}\")\n",
    "            \n",
    "            max_embedded_a = a\n",
    "            break # We found the max, so we can stop searching\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" -> An error occurred during embedding for K_{a},{a}: {e}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# FINAL REPORT\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- ITERATIVE SEARCH COMPLETE ---\")\n",
    "if max_embedded_a:\n",
    "    print(f\"The largest complete bipartite graph K_a,a found was:\")\n",
    "    print(f\"    K_{max_embedded_a},{max_embedded_a}\")\n",
    "else:\n",
    "    print(f\"No successful embedding found for any K_a,a from\")\n",
    "    print(f\"a = {STARTING_A} down to a = {ENDING_A}.\")\n",
    "\n",
    "print(\"--- END OF SCRIPT ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f56e28c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# The K_a,a graph to embed\n",
    "A_B = 184\n",
    "# The Zephyr grid parameter for Advantage2\n",
    "M_ZEPHYR = 12\n",
    "# The live solver to target (as you specified)\n",
    "TARGET_SOLVER = \"Advantage2_system1.9\" \n",
    "\n",
    "print(\"--- STARTING ZEPHYR BICLIQUE EMBEDDING TEST ---\")\n",
    "print(f\"Targeting graph: K_{A_B},{A_B}\")\n",
    "print(f\"Zephyr grid parameter: m = {M_ZEPHYR}\\n\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# TEST 1: THEORETICAL (FULL-YIELD) EMBEDDING\n",
    "# ------------------------------------------------------------------\n",
    "print(\"--- TEST 1: THEORETICAL (FULL-YIELD) EMBEDDING ---\")\n",
    "print(f\"Attempting to embed K_{A_B},{A_B} on a *perfect* Z_{M_ZEPHYR} graph...\")\n",
    "\n",
    "try:\n",
    "    # We call the function using the 'm' parameter.\n",
    "    # This *generates* a perfect Z_12 graph in memory.\n",
    "    # We do not provide a 'target_graph'.\n",
    "    left_chains_theory, right_chains_theory = find_biclique_embedding(\n",
    "        A_B, A_B, m=M_ZEPHYR\n",
    "    )\n",
    "    \n",
    "    print(\"\\nResult: SUCCESS (As Expected)\")\n",
    "    print(f\"Successfully found an embedding for K_{A_B},{A_B} on a perfect Z_{M_ZEPHYR} graph.\")\n",
    "    \n",
    "    # Verify chain lengths\n",
    "    all_chains_theory = list(left_chains_theory.values()) + \\\n",
    "                        list(right_chains_theory.values())\n",
    "    max_chain_len = max(len(chain) for chain in all_chains_theory)\n",
    "    \n",
    "    print(f\" -> Total logical variables: {len(all_chains_theory)}\")\n",
    "    print(f\" -> Max chain length: {max_chain_len}\")\n",
    "    \n",
    "    if max_chain_len == M_ZEPHYR:\n",
    "        print(f\" -> Chain length of {max_chain_len} matches the theoretical paper.[1]\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nResult: FAILED (Unexpected)\")\n",
    "    print(f\"An error occurred during theoretical embedding: {e}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# TEST 2: PRACTICAL (WORKING-GRAPH) EMBEDDING @ K_184,184\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- TEST 2: PRACTICAL (WORKING-GRAPH) EMBEDDING ---\")\n",
    "print(f\"Attempting to embed K_{A_B},{A_B} on the *live* {TARGET_SOLVER}...\")\n",
    "\n",
    "# This variable will hold the fetched graph for Test 3\n",
    "working_graph = None \n",
    "\n",
    "try:\n",
    "    # 1. Connect to the sampler\n",
    "    print(f\"Connecting to {TARGET_SOLVER}...\")\n",
    "    sampler = DWaveSampler(solver=TARGET_SOLVER)\n",
    "    \n",
    "    # 2. Fetch the *actual* working graph as a networkx object\n",
    "    print(\"Fetching QPU 'working graph'...\")\n",
    "    working_graph = sampler.to_networkx_graph()\n",
    "    \n",
    "    # This will show ~4400+, NOT the theoretical 4,800 [2, 3, 4]\n",
    "    print(f\" -> Working graph has {working_graph.number_of_nodes()} active qubits.\")\n",
    "    print(f\" -> Working graph has {working_graph.number_of_edges()} active couplers.\")\n",
    "\n",
    "    # 3. Attempt embedding on the 'working_graph'\n",
    "    # NOTE: We pass 'target_graph=working_graph' and 'm=None' (default)\n",
    "    print(f\"Attempting embedding on {working_graph.number_of_nodes()}-qubit graph...\")\n",
    "    \n",
    "    left_chains_practical, right_chains_practical = find_biclique_embedding(\n",
    "        A_B, A_B, target_graph=working_graph\n",
    "    )\n",
    "    \n",
    "    print(\"\\nResult: SUCCESS (Highly Unlikely)\")\n",
    "    print(\"A valid embedding was found on the practical working graph!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\nResult: FAILED (As Expected)\")\n",
    "    print(\"The embedding function raised an error, indicating a valid embedding\")\n",
    "    print(\"for this 'tight-packing' problem could not be found on the defective graph.\")\n",
    "    print(f\"\\nFull Error Message:\\n{e}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# TEST 3: ITERATIVE SEARCH FOR *ACTUAL* MAX BICLIQUE\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- TEST 3: ITERATIVE PRACTICAL EMBEDDING SEARCH ---\")\n",
    "\n",
    "# We can only run this test if we successfully fetched the graph in Test 2\n",
    "if working_graph is not None:\n",
    "    \n",
    "    # We start from one less than the theoretical max, since we know 184 fails.\n",
    "    START_A = 77 \n",
    "    # Let's define a reasonable stopping point\n",
    "    END_A = 50 \n",
    "    \n",
    "    print(f\"Searching for largest K_a,a on {TARGET_SOLVER} working graph...\")\n",
    "    print(f\"Range: a = {START_A} down to {END_A}\\n\")\n",
    "    \n",
    "    max_practical_a = None\n",
    "    \n",
    "    for a in range(START_A, END_A - 1, -1):\n",
    "        print(f\"--- Attempting K_{a},{a} ---\")\n",
    "        try:\n",
    "            # Attempt the embedding with the current 'a'\n",
    "            left_chains, right_chains = find_biclique_embedding(\n",
    "                a, a, target_graph=working_graph\n",
    "            )\n",
    "            \n",
    "            # If the line above does NOT throw an error, we succeeded!\n",
    "            print(f\"\\n>>> Result: SUCCESS!\")\n",
    "            print(f\">>> Found embedding for K_{a},{a} on {TARGET_SOLVER}\")\n",
    "            \n",
    "            all_chains = list(left_chains.values()) + list(right_chains.values())\n",
    "            max_len = max(len(chain) for chain in all_chains)\n",
    "            num_qubits_used = sum(len(chain) for chain in all_chains)\n",
    "            \n",
    "            print(f\" -> Max chain length: {max_len}\")\n",
    "            print(f\" -> Total qubits used: {num_qubits_used}\")\n",
    "            \n",
    "            max_practical_a = a\n",
    "            # plot distribution of chain lengths\n",
    "            chain_lengths = [len(chain) for chain in all_chains]\n",
    "            plt.hist(chain_lengths, bins=range(1, max_len + 2), align='left', rwidth=0.8)\n",
    "            plt.title(f\"Chain Length Distribution for K_{a},{a} on {TARGET_SOLVER}\")\n",
    "            plt.xlabel(\"Chain Length\")\n",
    "            plt.ylabel(\"Frequency\")\n",
    "            plt.show()\n",
    "            break # Exit the loop, we found the largest 'a'\n",
    "            \n",
    "        except Exception as e:\n",
    "            # This is the expected path for 'a' values that are too large\n",
    "            print(f\" -> Result: FAILED\")\n",
    "            print(f\" -> Error: {e}\\n\")\n",
    "            \n",
    "    # Final report\n",
    "    print(\"\\n--- Iterative Search Complete ---\")\n",
    "    if max_practical_a:\n",
    "        print(f\"The largest embeddable K_a,a found was: K_{max_practical_a},{max_practical_a}\")\n",
    "    else:\n",
    "        print(f\"No successful embedding found in the range a = {START_A} to {END_A}.\")\n",
    "        \n",
    "else:\n",
    "    print(\"Skipping Test 3 because the working graph was not fetched in Test 2.\")\n",
    "\n",
    "print(\"\\n--- END OF SCRIPT ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# The K_a,a graph to embed\n",
    "A_B = 184\n",
    "# The Zephyr grid parameter for Advantage2\n",
    "M_ZEPHYR = 12\n",
    "# The live solver to target (as you specified)\n",
    "TARGET_SOLVER = \"Advantage2_system1.7\" \n",
    "\n",
    "print(\"--- STARTING ZEPHYR BICLIQUE EMBEDDING TEST ---\")\n",
    "print(f\"Targeting graph: K_{A_B},{A_B}\")\n",
    "print(f\"Zephyr grid parameter: m = {M_ZEPHYR}\\n\")\n",
    "\n",
    "print(f\"Attempting to embed K_{A_B},{A_B} on the *live* {TARGET_SOLVER}...\")\n",
    "\n",
    "# This variable will hold the fetched graph for Test 3\n",
    "working_graph = None \n",
    "\n",
    "try:\n",
    "    # 1. Connect to the sampler\n",
    "    print(f\"Connecting to {TARGET_SOLVER}...\")\n",
    "    sampler = DWaveSampler(solver=TARGET_SOLVER)\n",
    "    \n",
    "    # 2. Fetch the *actual* working graph as a networkx object\n",
    "    print(\"Fetching QPU 'working graph'...\")\n",
    "    working_graph = sampler.to_networkx_graph()\n",
    "    \n",
    "    # This will show ~4400+, NOT the theoretical 4,800 [2, 3, 4]\n",
    "    print(f\" -> Working graph has {working_graph.number_of_nodes()} active qubits.\")\n",
    "    print(f\" -> Working graph has {working_graph.number_of_edges()} active couplers.\")\n",
    "\n",
    "    # 3. Attempt embedding on the 'working_graph'\n",
    "    # NOTE: We pass 'target_graph=working_graph' and 'm=None' (default)\n",
    "    print(f\"Attempting embedding on {working_graph.number_of_nodes()}-qubit graph...\")\n",
    "    \n",
    "    left_chains_practical, right_chains_practical = find_biclique_embedding(\n",
    "        A_B, A_B, target_graph=working_graph\n",
    "    )\n",
    "    \n",
    "    print(\"\\nResult: SUCCESS (Highly Unlikely)\")\n",
    "    print(\"A valid embedding was found on the practical working graph!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"\\nResult: FAILED (As Expected)\")\n",
    "    print(\"The embedding function raised an error, indicating a valid embedding\")\n",
    "    print(\"for this 'tight-packing' problem could not be found on the defective graph.\")\n",
    "    print(f\"\\nFull Error Message:\\n{e}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# TEST 3: ITERATIVE SEARCH FOR *ACTUAL* MAX BICLIQUE\n",
    "# ------------------------------------------------------------------\n",
    "print(\"\\n--- TEST 3: ITERATIVE PRACTICAL EMBEDDING SEARCH ---\")\n",
    "\n",
    "# We can only run this test if we successfully fetched the graph in Test 2\n",
    "if working_graph is not None:\n",
    "    \n",
    "    # We start from one less than the theoretical max, since we know 184 fails.\n",
    "    START_A = 100\n",
    "    # Let's define a reasonable stopping point\n",
    "    END_A = 50\n",
    "    \n",
    "    print(f\"Searching for largest K_a,a on {TARGET_SOLVER} working graph...\")\n",
    "    print(f\"Range: a = {START_A} down to {END_A}\\n\")\n",
    "    \n",
    "    max_practical_a = None\n",
    "\n",
    "    for a in range(START_A, END_A - 1, -1):\n",
    "        print(f\"--- Attempting K_{a},{a} ---\")\n",
    "        try:\n",
    "            # Attempt the embedding with the current 'a'\n",
    "            left_chains, right_chains = find_biclique_embedding(\n",
    "                a, a, target_graph=working_graph\n",
    "            )\n",
    "            \n",
    "            # If the line above does NOT throw an error, we succeeded!\n",
    "            print(f\"\\n>>> Result: SUCCESS!\")\n",
    "            print(f\">>> Found embedding for K_{a},{a} on {TARGET_SOLVER}\")\n",
    "            \n",
    "            all_chains = list(left_chains.values()) + list(right_chains.values())\n",
    "            max_len = max(len(chain) for chain in all_chains)\n",
    "            \n",
    "            print(f\" -> Max chain length: {max_len}\")\n",
    "            \n",
    "            max_practical_a = a\n",
    "            break # Exit the loop, we found the largest 'a'\n",
    "            \n",
    "        except Exception as e:\n",
    "            # This is the expected path for 'a' values that are too large\n",
    "            print(f\" -> Result: FAILED\")\n",
    "            print(f\" -> Error: {e}\\n\")\n",
    "            \n",
    "    # Final report\n",
    "    print(\"\\n--- Iterative Search Complete ---\")\n",
    "    if max_practical_a:\n",
    "        print(f\"The largest embeddable K_a,a found was: K_{max_practical_a},{max_practical_a}\")\n",
    "    else:\n",
    "        print(f\"No successful embedding found in the range a = {START_A} to {END_A}.\")\n",
    "        \n",
    "else:\n",
    "    print(\"Skipping Test 3 because the working graph was not fetched in Test 2.\")\n",
    "\n",
    "print(\"\\n--- END OF SCRIPT ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a9847e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- set the size you actually want ---\n",
    "N = 200   # number of nodes PER layer (total logical nodes = 2*N)\n",
    "\n",
    "# create node labels (you can choose any hashable labels)\n",
    "L1_nodes = [f\"L1_{i}\" for i in range(N)]\n",
    "L2_nodes = [f\"L2_{i}\" for i in range(N)]\n",
    "\n",
    "G_logical = nx.Graph()\n",
    "G_logical.add_nodes_from(L1_nodes, bipartite=0)\n",
    "G_logical.add_nodes_from(L2_nodes, bipartite=1)\n",
    "\n",
    "# Alternating connectivity (interpretation of your bullets):\n",
    "# - even-indexed L1 nodes (i%2==0) connect to odd-indexed L2 nodes (j%2==1)\n",
    "# - odd-indexed L2 nodes (i%2==1) connect to even-indexed L1 nodes (j%2==0)\n",
    "for i in range(N):\n",
    "    if i % 2 == 0:\n",
    "        l1 = L1_nodes[i]\n",
    "        for j in range(N):\n",
    "            if j % 2 == 1:\n",
    "                l2 = L2_nodes[j]\n",
    "                G_logical.add_edge(l1, l2)\n",
    "    if i % 2 == 1:\n",
    "        l2 = L2_nodes[i]\n",
    "        for j in range(N):\n",
    "            if j % 2 == 0:\n",
    "                l1 = L1_nodes[j]\n",
    "                G_logical.add_edge(l2, l1)\n",
    "\n",
    "print(f\"Logical graph: {G_logical.number_of_nodes()} nodes, {G_logical.number_of_edges()} edges\")\n",
    "\n",
    "# --- get target (hardware) graph ---\n",
    "# Option A: prototype locally with a perfect Zephyr of size m\n",
    "G_target = dnx.zephyr_graph(16)   # choose m that gives enough qubits (increase m if needed)\n",
    "print(f\"Target zephyr_graph (m=16): {G_target.number_of_nodes()} nodes, {G_target.number_of_edges()} edges\")\n",
    "\n",
    "# Option B: (preferred if you will run on a real sampler)\n",
    "# sampler = DWaveSampler()                 # uncomment if you have credentials\n",
    "# G_target = sampler.to_networkx_graph()   # real hardware topology (with defects accounted for)\n",
    "\n",
    "# --- try minorminer ---\n",
    "params = dict(tries=30, timeout=60, threads=4, chainlength_patience=20, verbose=2)\n",
    "embedding = minorminer.find_embedding(G_logical, G_target, **params)\n",
    "\n",
    "if embedding:\n",
    "    chain_lengths = [len(chain) for chain in embedding.values()]\n",
    "    print(\"Embedding found\")\n",
    "    print(f\"  max chain length: {max(chain_lengths)}\")\n",
    "    print(f\"  avg chain length: {sum(chain_lengths)/len(chain_lengths):.2f}\")\n",
    "else:\n",
    "    print(\"No embedding found (embedding is empty). Try reducing density or increasing tries/target size.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b842588a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import dwave_networkx as dnx\n",
    "import minorminer\n",
    "# from dwave.system import DWaveSampler # Uncomment for real hardware\n",
    "\n",
    "# --- set the size you actually want ---\n",
    "N = 100     # number of nodes PER layer (total logical nodes = 2*N)\n",
    "K_NEIGHBORS = 4  # The number of neighbors for each L1 node.\n",
    "               # Must be >= 2 to ensure a connected graph.\n",
    "\n",
    "def create_logical_graph(n_nodes, k_neighbors):\n",
    "    \"\"\"\n",
    "    Creates a bipartite graph with n_nodes in each layer (L1, L2).\n",
    "    \n",
    "    Connects each node L1[i] to k_neighbors nodes in L2, starting\n",
    "    from L2[i] and wrapping around the layer using modulo.\n",
    "    \n",
    "    This creates a single, connected graph (for k_neighbors >= 2) \n",
    "    with a total of (n_nodes * k_neighbors) edges.\n",
    "    \"\"\"\n",
    "    L1_nodes = [f\"L1_{i}\" for i in range(n_nodes)]\n",
    "    L2_nodes = [f\"L2_{i}\" for i in range(n_nodes)]\n",
    "\n",
    "    G_logical = nx.Graph()\n",
    "    G_logical.add_nodes_from(L1_nodes, bipartite=0)\n",
    "    G_logical.add_nodes_from(L2_nodes, bipartite=1)\n",
    "\n",
    "    print(f\"Building logical graph with N={n_nodes} and K_NEIGHBORS={k_neighbors}...\")\n",
    "    \n",
    "    if k_neighbors > n_nodes:\n",
    "        print(f\"Warning: k_neighbors ({k_neighbors}) is > N ({n_nodes}). Clamping to N.\")\n",
    "        k_neighbors = n_nodes\n",
    "    elif k_neighbors < 2:\n",
    "        print(f\"Warning: k_neighbors ({k_neighbors}) < 2. The resulting graph will not be connected.\")\n",
    "\n",
    "    # This is the generalized \"k-neighbor\" logic:\n",
    "    for i in range(n_nodes):\n",
    "        l1 = L1_nodes[i]\n",
    "        for offset in range(k_neighbors):\n",
    "            # Connect L1[i] to L2[j], L2[j+1], ...\n",
    "            j = (i + offset) % n_nodes\n",
    "            l2 = L2_nodes[j]\n",
    "            G_logical.add_edge(l1, l2)\n",
    "                \n",
    "    return G_logical\n",
    "\n",
    "# --- create the logical graph ---\n",
    "G_logical = create_logical_graph(N, K_NEIGHBORS)\n",
    "print(f\"Logical graph (N={N}, K={K_NEIGHBORS}): {G_logical.number_of_nodes()} nodes, {G_logical.number_of_edges()} edges\")\n",
    "print(f\"Graph is connected: {nx.is_connected(G_logical)}\")\n",
    "\n",
    "# --- create the logical graph for visualization ---\n",
    "G_logical_viz = create_logical_graph(N, K_NEIGHBORS)\n",
    "print(f\"Logical graph for visualization (N={N}, K={K_NEIGHBORS}): {G_logical_viz.number_of_nodes()} nodes, {G_logical_viz.number_of_edges()} edges\")\n",
    "print(f\"Graph is connected: {nx.is_connected(G_logical_viz)}\")\n",
    "\n",
    "\n",
    "# --- Visualize the graph ---\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Define node sets for bipartite layout\n",
    "l1_nodes_viz = [f\"L1_{i}\" for i in range(N)]\n",
    "l2_nodes_viz = [f\"L2_{i}\" for i in range(N)]\n",
    "\n",
    "# Create bipartite layout\n",
    "pos = nx.bipartite_layout(G_logical_viz, l1_nodes_viz)\n",
    "\n",
    "# Draw nodes\n",
    "nx.draw_networkx_nodes(G_logical_viz, pos, nodelist=l1_nodes_viz, node_color='skyblue', label='Layer 1')\n",
    "nx.draw_networkx_nodes(G_logical_viz, pos, nodelist=l2_nodes_viz, node_color='lightcoral', label='Layer 2')\n",
    "\n",
    "# Draw edges\n",
    "nx.draw_networkx_edges(G_logical_viz, pos, edge_color='gray', alpha=0.7)\n",
    "\n",
    "# Draw labels\n",
    "nx.draw_networkx_labels(G_logical_viz, pos, font_size=8)\n",
    "\n",
    "plt.title(f\"Custom Bipartite Graph (N={N} per layer, K_NEIGHBORS={K_NEIGHBORS})\")\n",
    "plt.legend()\n",
    "plt.axis('off') # Hide axes\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2453726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- get target (hardware) graph ---\n",
    "# Option A: prototype locally with a perfect Zephyr of size m\n",
    "G_target = dnx.zephyr_graph(16)   # choose m that gives enough qubits (increase m if needed)\n",
    "print(f\"Target zephyr_graph (m=16): {G_target.number_of_nodes()} nodes, {G_target.number_of_edges()} edges\")\n",
    "\n",
    "# Option B: (preferred if you will run on a real sampler)\n",
    "# sampler = DWaveSampler()                 # uncomment if you have credentials\n",
    "# G_target = sampler.to_networkx_graph()   # real hardware topology (with defects accounted for)\n",
    "# print(f\"Target hardware graph: {G_target.number_of_nodes()} nodes, {G_target.number_of_edges()} edges\")\n",
    "\n",
    "\n",
    "# --- try minorminer ---\n",
    "print(\"\\nStarting minorminer.find_embedding...\")\n",
    "params = dict(tries=30, timeout=60, threads=4, chainlength_patience=20, verbose=2)\n",
    "embedding = minorminer.find_embedding(G_logical, G_target, **params)\n",
    "\n",
    "if embedding:\n",
    "    chain_lengths = [len(chain) for chain in embedding.values()]\n",
    "    print(\"\\nEmbedding found!\")\n",
    "    print(f\"  Max chain length: {max(chain_lengths)}\")\n",
    "    print(f\"  Avg chain length: {sum(chain_lengths)/len(chain_lengths):.2f}\")\n",
    "else:\n",
    "    print(\"\\nNo embedding found (embedding is empty).\")\n",
    "    print(\"Try increasing K (to make the graph sparser), increasing 'tries'/'timeout', or using a larger target graph (increase 'm').\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ef5d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sparse_connected_bipartite(n: int, target_edges: int):\n",
    "    \"\"\"\n",
    "    Generates a sparse, connected, random bipartite graph of size n x n.\n",
    "\n",
    "    This function implements a \"build-up\" method:\n",
    "    1. It starts with two partitions of n nodes each (2n nodes total).\n",
    "    2. It adds random bipartite edges until the graph is connected.\n",
    "    3. It continues adding random bipartite edges until target_edges is reached.\n",
    "\n",
    "    Args:\n",
    "        n: The number of nodes in each of the two partitions.\n",
    "        target_edges: The total number of edges the final graph should have.\n",
    "\n",
    "    Returns:\n",
    "        A networkx Graph object.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the target_edges is not possible.\n",
    "            - If target_edges is less than the minimum (2n - 1) required\n",
    "              for a 2n-node graph to be connected.\n",
    "            - If n is 0 or negative.\n",
    "    \"\"\"\n",
    "    if n <= 0:\n",
    "        raise ValueError(\"n must be a positive integer.\")\n",
    "\n",
    "    # --- 1. Validation ---\n",
    "    min_edges = 2 * n - 1  # Minimum edges for a connected graph (spanning tree)\n",
    "    max_edges = n * n      # Maximum edges in a complete K_n,n graph\n",
    "\n",
    "    if target_edges < min_edges:\n",
    "        raise ValueError(\n",
    "            f\"Target edges ({target_edges}) is less than the minimum ({min_edges}) \"\n",
    "            f\"required for a connected graph of {2*n} nodes.\"\n",
    "        )\n",
    "\n",
    "    if target_edges > max_edges:\n",
    "        print(\n",
    "            f\"Warning: Target edges ({target_edges}) exceeds the maximum possible \"\n",
    "            f\"({max_edges}). Returning a complete bipartite K_{n},{n} graph.\"\n",
    "        )\n",
    "        target_edges = max_edges\n",
    "\n",
    "    # --- 2. Initialization ---\n",
    "    G = nx.Graph()\n",
    "    partition_U = list(range(n))\n",
    "    partition_V = list(range(n, 2 * n))\n",
    "    \n",
    "    # Add nodes with bipartite attribute\n",
    "    G.add_nodes_from(partition_U, bipartite=0)\n",
    "    G.add_nodes_from(partition_V, bipartite=1)\n",
    "\n",
    "    # Generate all possible bipartite edges\n",
    "    possible_edges = [\n",
    "        (u, v) for u in partition_U for v in partition_V\n",
    "    ]\n",
    "    random.shuffle(possible_edges)\n",
    "    \n",
    "    edge_iterator = iter(possible_edges)\n",
    "\n",
    "    # --- 3. Phase 1: Ensure Connectivity (Build Spanning Tree) ---\n",
    "    # We add edges until the graph has only one connected component\n",
    "    while not nx.is_connected(G):\n",
    "        try:\n",
    "            u, v = next(edge_iterator)\n",
    "            G.add_edge(u, v)\n",
    "        except StopIteration:\n",
    "            # This should only happen if n=1 and target_edges=1\n",
    "            break \n",
    "            \n",
    "    # --- 4. Phase 2: Add Remaining Edges to Reach Target ---\n",
    "    num_edges_to_add = target_edges - G.number_of_edges()\n",
    "\n",
    "    for _ in range(num_edges_to_add):\n",
    "        try:\n",
    "            u, v = next(edge_iterator)\n",
    "            G.add_edge(u, v)\n",
    "        except StopIteration:\n",
    "            # This means we've added all possible n*n edges\n",
    "            break\n",
    "            \n",
    "    return G, partition_U, partition_V\n",
    "\n",
    "N_NODES = 200 \n",
    "TARGET_EDGES = 50 * N_NODES\n",
    "\n",
    "print(f\"Generating a {N_NODES}x{N_NODES} sparse bipartite graph...\")\n",
    "print(f\"Target edges: {TARGET_EDGES}\\n\")\n",
    "\n",
    "try:\n",
    "    G, U, V = create_sparse_connected_bipartite(N_NODES, TARGET_EDGES)\n",
    "\n",
    "    print(\"--- Graph Generation Successful ---\")\n",
    "    print(f\"Partition U nodes: {U}\")\n",
    "    print(f\"Partition V nodes: {V}\")\n",
    "    print(f\"Total nodes: {G.number_of_nodes()}\")\n",
    "    print(f\"Final edges: {G.number_of_edges()}\")\n",
    "    print(f\"Is connected: {nx.is_connected(G)}\")\n",
    "    print(f\"Is bipartite: {nx.is_bipartite(G)}\")\n",
    "\n",
    "    # Example of low target edge count\n",
    "    print(\"\\n--- Testing minimum connectivity ---\")\n",
    "    min_edges_required = 2 * N_NODES - 1\n",
    "    G_min, _, _ = create_sparse_connected_bipartite(N_NODES, min_edges_required)\n",
    "    print(f\"Targeting minimum edges: {min_edges_required}\")\n",
    "    print(f\"Resulting edges: {G_min.number_of_edges()}\")\n",
    "    print(f\"Is connected: {nx.is_connected(G_min)}\")\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"Error generating graph: {e}\")\n",
    "\n",
    "#plot generated graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "pos = nx.bipartite_layout(G, U)\n",
    "nx.draw(G, pos, with_labels=True, node_color=['lightblue' if n in U else 'lightgreen' for n in G.nodes()])\n",
    "plt.title(f\"Sparse Bipartite Graph with {G.number_of_edges()} Edges\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f59940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- get target (hardware) graph ---\n",
    "# Option A: prototype locally with a perfect Zephyr of size m\n",
    "G_target = dnx.zephyr_graph(16)   # choose m that gives enough qubits (increase m if needed)\n",
    "print(f\"Target zephyr_graph (m=16): {G_target.number_of_nodes()} nodes, {G_target.number_of_edges()} edges\")\n",
    "\n",
    "# Option B: (preferred if you will run on a real sampler)\n",
    "# sampler = DWaveSampler()                 # uncomment if you have credentials\n",
    "# G_target = sampler.to_networkx_graph()   # real hardware topology (with defects accounted for)\n",
    "# print(f\"Target hardware graph: {G_target.number_of_nodes()} nodes, {G_target.number_of_edges()} edges\")\n",
    "\n",
    "\n",
    "# --- try minorminer ---\n",
    "print(\"\\nStarting minorminer.find_embedding...\")\n",
    "params = dict(tries=30, timeout=60, threads=4, chainlength_patience=20, verbose=2)\n",
    "embedding = minorminer.find_embedding(G, G_target, **params)\n",
    "\n",
    "if embedding:\n",
    "    chain_lengths = [len(chain) for chain in embedding.values()]\n",
    "    print(\"\\nEmbedding found!\")\n",
    "    print(f\"  Max chain length: {max(chain_lengths)}\")\n",
    "    print(f\"  Avg chain length: {sum(chain_lengths)/len(chain_lengths):.2f}\")\n",
    "else:\n",
    "    print(\"\\nNo embedding found (embedding is empty).\")\n",
    "    print(\"Try increasing K (to make the graph sparser), increasing 'tries'/'timeout', or using a larger target graph (increase 'm').\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e0648a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
