{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c1f6d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import h5py # these are HDF5 files\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina' \n",
    "\n",
    "cwd   = os.getcwd()\n",
    "parent_cwd = os.path.dirname(cwd)\n",
    "\n",
    "sys.path.insert(0, cwd)\n",
    "sys.path.insert(0, parent_cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "898daeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "\n",
    "# hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "# initialize(version_base=None, config_path=\"../config\")\n",
    "# config=compose(config_name=\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "599f3561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first check the keys:\n",
    "# with h5py.File(f'/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_default_binning/dataset_combined_fine.hdf5', 'r') as file: \n",
    "#     print(\"Keys: %s\" % list(file.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "822124ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with h5py.File(f'/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_default_binning/dataset_combined_positive.hdf5', 'r') as file: \n",
    "#     print(\"Keys: %s\" % list(file.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40c4bf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with h5py.File(f'/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_default_binning/dataset_combined.hdf5', 'r') as file: \n",
    "#     print(\"Keys: %s\" % list(file.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af055b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " File: default (/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/0/dataset_combined.hdf5)\n",
      "\n",
      " File: positive (/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/0/dataset_combined_positive.hdf5)\n",
      "\n",
      " File: fine (/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/0/dataset_combined_fine.hdf5)\n"
     ]
    }
   ],
   "source": [
    "file_paths = {\n",
    "    \"default\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/0/dataset_combined.hdf5\",\n",
    "    \"positive\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/0/dataset_combined_positive.hdf5\",\n",
    "    \"fine\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/0/dataset_combined_fine.hdf5\",\n",
    "}\n",
    "\n",
    "for label, path in file_paths.items():\n",
    "    print(f\"\\n File: {label} ({path})\")\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        for key in f.keys():\n",
    "            dataset = f[key]\n",
    "            # uncomment to print outputs:\n",
    "            #print(f\"{key}: shape={dataset.shape}, dtype={dataset.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8645a6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy_layer_0: shape=(150000, 336), dtype=float32\n"
     ]
    }
   ],
   "source": [
    "path = \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_130/eta_130_regular_binning/dataset_combined.hdf5\"\n",
    "with h5py.File(path, 'r') as f:\n",
    "    # pick a layer\n",
    "    layer_key = \"energy_layer_0\" # can change this to check diff layers\n",
    "    if layer_key in f:\n",
    "        data = f[layer_key]  # shape (N_events, N_voxels)\n",
    "        print(f\"{layer_key}: shape={data.shape}, dtype={data.dtype}\")\n",
    "        # print first few events, first few voxels\n",
    "        #print(\"Sample values:\")\n",
    "        #print(np.array(data[:5, :10]))  # first 5 events, first 10 voxels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df464297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "energy_layer_0: shape=(7500, 336), dtype=float32\n"
     ]
    }
   ],
   "source": [
    "# looking at a split file:\n",
    "\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_130/eta_130_regular_binning\"\n",
    "split = \"0\"\n",
    "file_name = \"dataset_combined.hdf5\"  # default file\n",
    "\n",
    "path = os.path.join(base_dir, split, file_name)\n",
    "\n",
    "with h5py.File(path, 'r') as f:\n",
    "    # pick a layer\n",
    "    layer_key = \"energy_layer_0\" # can change this to check diff layers\n",
    "    if layer_key in f:\n",
    "        data = f[layer_key]  # shape (N_events, N_voxels)\n",
    "        print(f\"{layer_key}: shape={data.shape}, dtype={data.dtype}\")\n",
    "        # print first few events, first few voxels\n",
    "        # uncomment to print outputs:\n",
    "        #print(\"Sample values:\")\n",
    "        #print(np.array(data[:5, :10]))  # first 5 events, first 10 voxels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30a7b0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths:\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning\"\n",
    "file_names = {\n",
    "    \"default\":  \"dataset_combined.hdf5\",\n",
    "    \"positive\": \"dataset_combined_positive.hdf5\",\n",
    "}\n",
    "\n",
    "splits = list(range(20))         # 0..19\n",
    "voxel_indices = [0, 5, 10, 20, 35, 50]   # choosing a few random voxels\n",
    "density = True  \n",
    "bins_count = 60\n",
    "colors = {\"default\": \"C0\", \"positive\": \"C1\"}\n",
    "robust_pct = (0.1, 99.9)  # percentiles for x axis\n",
    "\n",
    "probe_path = os.path.join(base_dir, \"0\", file_names[\"default\"])\n",
    "with h5py.File(probe_path, 'r') as f0:\n",
    "    # get energies and probabilities\n",
    "    energy_keys = sorted([k for k in f0.keys() if k.startswith(\"energy_layer_\")],\n",
    "                         key=lambda s: int(s.split(\"_\")[-1]))\n",
    "    prob_keys   = {int(k.split(\"_\")[-1]): k\n",
    "                   for k in f0.keys() if k.startswith(\"probabilities_layer_\")}\n",
    "\n",
    "def gather_energy_for_voxel(energy_key, voxel_idx):\n",
    "    \"\"\"\n",
    "    return dict with label for concatenated energies (all splits) for one voxel index\n",
    "    \"\"\"\n",
    "    vals = {lbl: [] for lbl in file_names}\n",
    "    for label, fname in file_names.items():\n",
    "        for split in splits:\n",
    "            path = os.path.join(base_dir, str(split), fname)\n",
    "            if not os.path.exists(path): \n",
    "                continue\n",
    "            with h5py.File(path, 'r') as f:\n",
    "                if energy_key not in f:\n",
    "                    continue\n",
    "                arr = f[energy_key]  # (N_events, voxels)\n",
    "                if voxel_idx >= arr.shape[1]:\n",
    "                    continue\n",
    "                vals[label].append(arr[:, voxel_idx])\n",
    "    return {lbl: (np.concatenate(vs) if vs else np.array([], dtype=np.float32))\n",
    "            for lbl, vs in vals.items()}\n",
    "\n",
    "def get_prob_triplet(layer_idx, voxel_idx):\n",
    "    \"\"\"\n",
    "    return dict with label: triplet or None (from split 0)\n",
    "    \"\"\"\n",
    "    key = prob_keys.get(layer_idx, None)\n",
    "    out = {}\n",
    "    for label, fname in file_names.items():\n",
    "        path = os.path.join(base_dir, \"0\", fname)\n",
    "        tri = None\n",
    "        if key and os.path.exists(path):\n",
    "            with h5py.File(path, 'r') as f:\n",
    "                if key in f:\n",
    "                    parr = f[key]  # (V,3)\n",
    "                    if voxel_idx < parr.shape[0] and parr.shape[1] >= 3:\n",
    "                        tri = np.array(parr[voxel_idx, :3], dtype=np.float64)\n",
    "        out[label] = tri\n",
    "    return out\n",
    "#############################################\n",
    "# uncomment this to show output:\n",
    "\n",
    "# now loop\n",
    "# for ekey in energy_keys:\n",
    "#     layer_idx = int(ekey.split(\"_\")[-1])\n",
    "#     # figure out how many voxels this layer has to filter invalid indices\n",
    "#     with h5py.File(os.path.join(base_dir, \"0\", file_names[\"default\"]), 'r') as f:\n",
    "#         V = f[ekey].shape[1]\n",
    "#     sel = [v for v in voxel_indices if v < V]\n",
    "#     if not sel:\n",
    "#         print(f\"[skip] {ekey}: none of {voxel_indices} are valid (V={V})\")\n",
    "#         continue\n",
    "\n",
    "#     # plotting layout\n",
    "#     nplots = len(sel)\n",
    "#     ncols = min(3, nplots)\n",
    "#     nrows = math.ceil(nplots / ncols)\n",
    "#     fig, axes = plt.subplots(nrows, ncols, figsize=(5*ncols, 3.8*nrows), squeeze=False)\n",
    "#     fig.suptitle(f\"{ekey} — energy distributions per selected voxel (splits 0–19)\", fontsize=12)\n",
    "\n",
    "#     for idx, v in enumerate(sel):\n",
    "#         r, c = divmod(idx, ncols)\n",
    "#         ax = axes[r][c]\n",
    "\n",
    "#         # get energies across splits per label for this voxel\n",
    "#         energies_by_label = gather_energy_for_voxel(ekey, v)\n",
    "#         # combine all values to pick x-limits & common bins\n",
    "#         all_vals = np.concatenate([vals for vals in energies_by_label.values() if vals.size],\n",
    "#                                   axis=0) if any(x.size for x in energies_by_label.values()) else np.array([])\n",
    "#         if all_vals.size:\n",
    "#             lo = np.nanpercentile(all_vals, robust_pct[0])\n",
    "#             hi = np.nanpercentile(all_vals, robust_pct[1])\n",
    "#             if not np.isfinite(lo): lo = all_vals.min()\n",
    "#             if not np.isfinite(hi): hi = all_vals.max()\n",
    "#             if hi <= lo:\n",
    "#                 lo, hi = float(np.min(all_vals)), float(np.max(all_vals))\n",
    "#             bins = np.linspace(lo, hi, bins_count+1)\n",
    "\n",
    "#             # plot overlays\n",
    "#             for label, vals in energies_by_label.items():\n",
    "#                 if vals.size == 0: \n",
    "#                     continue\n",
    "#                 ax.hist(vals, bins=bins, density=density, histtype=\"step\",\n",
    "#                         linewidth=1.3, label=label, color=colors.get(label, None))\n",
    "#             ax.set_xlim(lo, hi)\n",
    "#         else:\n",
    "#             ax.text(0.5, 0.5, \"No data\", ha=\"center\", va=\"center\")\n",
    "        \n",
    "#         # prob triplets (from split 0) printed\n",
    "#         ax.set_title(f\"voxel {v}\")\n",
    "#         ax.set_xlabel(\"Energy\")\n",
    "#         if r == 0 and c == 0:\n",
    "#             ax.set_ylabel(\"Density\" if density else \"Count\")\n",
    "#         ax.grid(alpha=0.2)\n",
    "\n",
    "#     # remove empty axes\n",
    "#     for k in range(nplots, nrows*ncols):\n",
    "#         r, c = divmod(k, ncols)\n",
    "#         axes[r][c].axis(\"off\")\n",
    "\n",
    "#     # legend\n",
    "#     handles, labels_ = axes[0][0].get_legend_handles_labels()\n",
    "#     if handles:\n",
    "#         fig.legend(handles, labels_, loc=\"upper right\")\n",
    "#     plt.tight_layout(rect=[0, 0, 0.98, 0.95])\n",
    "#     plt.show()\n",
    "\n",
    "#     # print probability triplets for these voxels (split 0)\n",
    "#     print(f\"\\nProbability triplets (split 0) : {ekey}\")\n",
    "#     for v in sel:\n",
    "#         probs = get_prob_triplet(layer_idx, v)\n",
    "#         row = [f\"{lbl}: \" + (f\"[{probs[lbl][0]:.6g}, {probs[lbl][1]:.6g}, {probs[lbl][2]:.6g}]\" if probs[lbl] is not None else \"None\")\n",
    "#                for lbl in (\"default\", \"positive\")]\n",
    "#         print(f\"voxel {v:4d}  ->  \" + \" | \".join(row))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffb86737",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "# configs\n",
    "root = \"/fast_scratch_1/caloqvae/data/atlas_july31\"\n",
    "eta_tags = [\n",
    "    \"eta_000\",\"eta_005\",\"eta_010\",\"eta_015\",\"eta_020\",\"eta_025\",\"eta_030\",\"eta_035\",\"eta_040\",\"eta_045\",\n",
    "    \"eta_050\",\"eta_055\",\"eta_060\",\"eta_065\",\"eta_070\",\"eta_075\",\"eta_080\",\"eta_085\",\"eta_090\",\"eta_095\",\n",
    "    \"eta_100\",\"eta_105\",\"eta_110\",\"eta_115\",\"eta_120\",\"eta_125\",\"eta_130\"\n",
    "]\n",
    "file_names = {\n",
    "    \"default\":  \"dataset_combined.hdf5\",\n",
    "    \"positive\": \"dataset_combined_positive.hdf5\",\n",
    "}\n",
    "splits = list(range(20))                 # 0..19\n",
    "voxel_indices = [0, 5, 10, 20, 30, 40, 50]   # choosing some voxels\n",
    "density = False\n",
    "bins_count = 80\n",
    "robust_pct = (0.1, 99.9)  # to tighten x range\n",
    "ENERGY_EPS = 1e-12  # treating energies less than this as no hit\n",
    "\n",
    "# coloring:\n",
    "def color_for_idx(i):\n",
    "    return f\"C{i % 10}\"\n",
    "linestyles = {\"default\": \"-\", \"positive\": \"--\"}\n",
    "\n",
    "def base_dir_for_eta(eta):\n",
    "    return os.path.join(root, eta, f\"{eta}_regular_binning\")\n",
    "\n",
    "def discover_energy_layers(example_file):\n",
    "    with h5py.File(example_file, \"r\") as f:\n",
    "        keys = [k for k in f.keys() if k.startswith(\"energy_layer_\")]\n",
    "    return sorted(keys, key=lambda s: int(s.split(\"_\")[-1]))\n",
    "\n",
    "def gather_energies_one_voxel(base_dir, label, energy_key, voxel_idx):\n",
    "    \"\"\"Concatenate energies across splits for one layer/voxel/file label\"\"\"\n",
    "    vals = []\n",
    "    for split in splits:\n",
    "        path = os.path.join(base_dir, str(split), file_names[label])\n",
    "        if not os.path.exists(path): \n",
    "            continue\n",
    "        with h5py.File(path, \"r\") as f:\n",
    "            if energy_key not in f: \n",
    "                continue\n",
    "            arr = f[energy_key]  # (N_events, V)\n",
    "            if voxel_idx >= arr.shape[1]: \n",
    "                continue\n",
    "            x = arr[:, voxel_idx]\n",
    "            x = x[np.isfinite(x)]\n",
    "            vals.append(x)\n",
    "    return np.concatenate(vals) if vals else np.array([], dtype=np.float32)\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "# uncomment this to show outputs:\n",
    "\n",
    "\n",
    "# # loop through eta\n",
    "# for eta in eta_tags:\n",
    "#     base_dir = base_dir_for_eta(eta)\n",
    "#     example_path = os.path.join(base_dir, \"0\", file_names[\"default\"])\n",
    "#     if not os.path.exists(example_path):\n",
    "#         print(f\"[skip eta] {eta}: missing {example_path}\")\n",
    "#         continue\n",
    "\n",
    "#     energy_keys = discover_energy_layers(example_path)\n",
    "#     if not energy_keys:\n",
    "#         print(f\"[skip eta] {eta}: no energy_layer_*\")\n",
    "#         continue\n",
    "\n",
    "#     # find active layers and get bins\n",
    "#     active_layers = []\n",
    "#     layer_bins = {}       # ekey -> (lo, hi)\n",
    "#     layer_valid_vox = {}  # ekey -> [valid voxel indices]\n",
    "#     with h5py.File(example_path, \"r\") as f0:\n",
    "#         for ekey in energy_keys:\n",
    "#             if ekey not in f0: \n",
    "#                 continue\n",
    "#             V = f0[ekey].shape[1]\n",
    "#             valid_vox = [v for v in voxel_indices if v < V]\n",
    "#             if not valid_vox:\n",
    "#                 continue\n",
    "\n",
    "#             # get values across labels and voxels to decide if there's a hit and to set bins\n",
    "#             pooled = []\n",
    "#             has_hits = False\n",
    "#             for v in valid_vox:\n",
    "#                 for label in file_names:\n",
    "#                     vals = gather_energies_one_voxel(base_dir, label, ekey, v)\n",
    "#                     if vals.size:\n",
    "#                         pooled.append(vals)\n",
    "#                         if np.any(np.abs(vals) > ENERGY_EPS):\n",
    "#                             has_hits = True\n",
    "#             if not has_hits or not pooled:\n",
    "#                 # no non-zero energies in selected voxels across both labels so skip this layer when plotting\n",
    "#                 continue\n",
    "\n",
    "#             pooled = np.concatenate(pooled)\n",
    "#             lo = np.nanpercentile(pooled, robust_pct[0])\n",
    "#             hi = np.nanpercentile(pooled, robust_pct[1])\n",
    "#             if not np.isfinite(lo): lo = pooled.min()\n",
    "#             if not np.isfinite(hi): hi = pooled.max()\n",
    "#             if hi <= lo: lo, hi = float(pooled.min()), float(pooled.max())\n",
    "\n",
    "#             active_layers.append(ekey)\n",
    "#             layer_bins[ekey] = (lo, hi)\n",
    "#             layer_valid_vox[ekey] = valid_vox\n",
    "\n",
    "#     if not active_layers:\n",
    "#         print(f\"[info] {eta}: no active layers for selected voxels {voxel_indices}\")\n",
    "#         continue\n",
    "\n",
    "#     # gigure grid\n",
    "#     n_layers = len(active_layers)\n",
    "#     ncols = 6 if n_layers >= 18 else min(5, n_layers)\n",
    "#     nrows = math.ceil(n_layers / ncols)\n",
    "\n",
    "#     fig, axes = plt.subplots(nrows, ncols, figsize=(4.0*ncols, 3.2*nrows), squeeze=False)\n",
    "#     fig.suptitle(f\"{eta} — voxel energy distributions by layer (splits 0–19)\", fontsize=12)\n",
    "\n",
    "#     # plotting the active layers\n",
    "#     for i, ekey in enumerate(active_layers):\n",
    "#         r, c = divmod(i, ncols)\n",
    "#         ax = axes[r][c]\n",
    "#         lo, hi = layer_bins[ekey]\n",
    "#         bins = np.linspace(lo, hi, bins_count + 1)\n",
    "\n",
    "#         for j, v in enumerate(layer_valid_vox[ekey]):\n",
    "#             # overlay both labels for this voxel\n",
    "#             for label in (\"default\", \"positive\"):\n",
    "#                 vals = gather_energies_one_voxel(base_dir, label, ekey, v)\n",
    "#                 if vals.size == 0: \n",
    "#                     continue\n",
    "#                 ax.hist(\n",
    "#                     vals, bins=bins, density=density, histtype=\"step\",\n",
    "#                     linewidth=1.2, color=color_for_idx(j), linestyle=linestyles[label],\n",
    "#                 )\n",
    "\n",
    "#         ax.set_xlim(lo, hi)\n",
    "#         ax.tick_params(axis='x', labelrotation=25) # rotate a bit for visual reasons\n",
    "#         for lab in ax.get_xticklabels():\n",
    "#             lab.set_horizontalalignment('right')  \n",
    "#         ax.set_yscale(\"log\")\n",
    "#         ax.set_title(ekey.replace(\"energy_layer_\", \"Layer \"))\n",
    "#         ax.set_xlabel(\"Energy [MeV]\")\n",
    "#         if c == 0:\n",
    "#             ax.set_ylabel(\"Density\" if density else \"Count\")\n",
    "#         ax.grid(alpha=0.2)\n",
    "\n",
    "#     # hide any unused subplots\n",
    "#     for k in range(n_layers, nrows*ncols):\n",
    "#         r, c = divmod(k, ncols)\n",
    "#         axes[r][c].set_visible(False)\n",
    "\n",
    "#     # legends\n",
    "#     voxel_labels = [f\"voxel {v}\" for v in voxel_indices]\n",
    "#     voxel_proxies = [Line2D([0],[0], color=color_for_idx(i), lw=1.5) for i,_ in enumerate(voxel_labels)]\n",
    "#     label_proxies = [Line2D([0],[0], color=\"k\", lw=1.5, linestyle=linestyles[k]) for k in (\"default\",\"positive\")]\n",
    "#     label_names   = [\"default\",\"positive\"]\n",
    "\n",
    "#     # adding in the legends\n",
    "#     fig.legend(voxel_proxies, voxel_labels, loc=\"upper right\", bbox_to_anchor=(0.98, 0.98), title=\"Selected voxels\")\n",
    "#     fig.legend(label_proxies, label_names, loc=\"lower right\", bbox_to_anchor=(0.98, 0.02), title=\"File label\")\n",
    "\n",
    "#     plt.tight_layout(rect=[0.02, 0.04, 0.96, 0.94])\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a13931c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "eta_000\n",
      "\n",
      "eta_005\n",
      "\n",
      "eta_010\n",
      "\n",
      "eta_015\n",
      "\n",
      "eta_020\n",
      "\n",
      "eta_025\n",
      "\n",
      "eta_030\n",
      "\n",
      "eta_035\n",
      "\n",
      "eta_040\n",
      "\n",
      "eta_045\n",
      "\n",
      "eta_050\n",
      "\n",
      "eta_055\n",
      "\n",
      "eta_060\n",
      "\n",
      "eta_065\n",
      "\n",
      "eta_070\n",
      "\n",
      "eta_075\n",
      "\n",
      "eta_080\n",
      "\n",
      "eta_085\n",
      "\n",
      "eta_090\n",
      "\n",
      "eta_095\n",
      "\n",
      "eta_100\n",
      "\n",
      "eta_105\n",
      "\n",
      "eta_110\n",
      "\n",
      "eta_115\n",
      "\n",
      "eta_120\n",
      "\n",
      "eta_125\n",
      "\n",
      "eta_130\n"
     ]
    }
   ],
   "source": [
    "root = \"/fast_scratch_1/caloqvae/data/atlas_july31\"\n",
    "\n",
    "# eta vals\n",
    "eta_tags = [\n",
    "    \"eta_000\",\"eta_005\",\"eta_010\",\"eta_015\",\"eta_020\",\"eta_025\",\"eta_030\",\"eta_035\",\"eta_040\",\"eta_045\",\n",
    "    \"eta_050\",\"eta_055\",\"eta_060\",\"eta_065\",\"eta_070\",\"eta_075\",\"eta_080\",\"eta_085\",\"eta_090\",\"eta_095\",\n",
    "    \"eta_100\",\"eta_105\",\"eta_110\",\"eta_115\",\"eta_120\",\"eta_125\",\"eta_130\"\n",
    "]\n",
    "\n",
    "file_names = {\n",
    "    \"default\":  \"dataset_combined.hdf5\",\n",
    "    \"positive\": \"dataset_combined_positive.hdf5\",\n",
    "    # \"fine\":     \"dataset_combined_fine.hdf5\", \n",
    "}\n",
    "\n",
    "splits = range(20)  # 0..19\n",
    "\n",
    "# numerical tolerance\n",
    "RTOL = 1e-6\n",
    "ATOL = 1e-8\n",
    "EQUAL_NAN = True \n",
    "\n",
    "def base_dir_for_eta(eta):\n",
    "    return os.path.join(root, eta, f\"{eta}_regular_binning\")\n",
    "\n",
    "def discover_prob_keys_from_probe(probe_file):\n",
    "    \"\"\"get probabilities_layer_* keys from a HDF5 file\"\"\"\n",
    "    with h5py.File(probe_file, \"r\") as f:\n",
    "        keys = [k for k in f.keys() if k.startswith(\"probabilities_layer_\")]\n",
    "    keys.sort(key=lambda s: int(s.split(\"_\")[-1]))\n",
    "    return keys\n",
    "\n",
    "def first_available_array(base_dir, label, key):\n",
    "    \"\"\"return (split_idx, array) for the first split containing this key for a given label\"\"\"\n",
    "    for s in splits:\n",
    "        path = os.path.join(base_dir, str(s), file_names[label])\n",
    "        if not os.path.exists(path):\n",
    "            continue\n",
    "        try:\n",
    "            with h5py.File(path, \"r\") as f:\n",
    "                if key in f:\n",
    "                    return s, f[key][:]  # (V, 3)\n",
    "        except OSError:\n",
    "            continue\n",
    "    return None, None\n",
    "\n",
    "# loop\n",
    "for eta in eta_tags:\n",
    "    base_dir = base_dir_for_eta(eta)\n",
    "    probe = os.path.join(base_dir, \"0\", file_names.get(\"default\", \"\"))\n",
    "\n",
    "    if not os.path.exists(probe):\n",
    "        print(f\"\\n={eta}\")\n",
    "        print(f\"[skip] missing file: {probe}\")\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        prob_keys = discover_prob_keys_from_probe(probe)\n",
    "    except OSError as e:\n",
    "        print(f\"\\n{eta}\")\n",
    "        print(f\"[skip] can't open file ({e})\")\n",
    "        continue\n",
    "\n",
    "    if not prob_keys:\n",
    "        print(f\"\\n{eta}\")\n",
    "        print(\"[info] no probabilities_layer_* keys found in file\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n{eta}\")\n",
    "    for label in file_names:\n",
    "        #print(f\"\\n label: {label}\")\n",
    "\n",
    "        for key in prob_keys:\n",
    "            ref_split, ref = first_available_array(base_dir, label, key)\n",
    "            if ref_split is None:\n",
    "                print(f\"{key}: (missing in all splits for '{label}')\")\n",
    "                continue\n",
    "\n",
    "            ok_all = True\n",
    "            diffs = []             # list of (split, message)\n",
    "            shape_mismatches = []  # list of (split, shape)\n",
    "\n",
    "            for s in splits:\n",
    "                path = os.path.join(base_dir, str(s), file_names[label])\n",
    "                if not os.path.exists(path):\n",
    "                    diffs.append((s, \"missing file\"))\n",
    "                    ok_all = False\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    with h5py.File(path, \"r\") as f:\n",
    "                        if key not in f:\n",
    "                            diffs.append((s, \"missing key\"))\n",
    "                            ok_all = False\n",
    "                            continue\n",
    "                        arr = f[key][:]\n",
    "                except OSError as e:\n",
    "                    diffs.append((s, f\"error opening file: {e}\"))\n",
    "                    ok_all = False\n",
    "                    continue\n",
    "\n",
    "                if arr.shape != ref.shape:\n",
    "                    shape_mismatches.append((s, arr.shape))\n",
    "                    ok_all = False\n",
    "                    continue\n",
    "\n",
    "                # elementwise compare with tolerance\n",
    "                mask = ~np.isclose(arr, ref, rtol=RTOL, atol=ATOL, equal_nan=EQUAL_NAN)\n",
    "                n_diff = int(mask.sum())\n",
    "                if n_diff > 0:\n",
    "                    # compute a robust max absolute difference\n",
    "                    max_abs = float(np.nanmax(np.abs(arr - ref)))\n",
    "                    frac = n_diff / arr.size\n",
    "                    diffs.append((s, f\"{n_diff} diffs ({frac:.3%}), max|delta|={max_abs:.3e}\"))\n",
    "                    ok_all = False\n",
    "                    \n",
    "# uncomment to print outputs:\n",
    "\n",
    "#             if ok_all:\n",
    "#                 print(f\"{key}: identical across splits (within rtol={RTOL}, atol={ATOL}) [ref split {ref_split}]\")\n",
    "#             else:\n",
    "#                 print(f\"{key}:  differences vs ref split {ref_split} (shape {ref.shape})\")\n",
    "#                 if shape_mismatches:\n",
    "#                     shapes = \", \".join([f\"s{s}:{sh}\" for s, sh in shape_mismatches])\n",
    "#                     print(f\"  shape mismatches: {shapes}\")\n",
    "#                 for s, msg in diffs:\n",
    "#                     print(f\"  split {s:2d}: {msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0dbd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking probabilities:\n",
    "\n",
    "def read_prob_triplet_for_voxel(base_dir, label, layer_idx, voxel_idx, prefer_splits=None):\n",
    "    \"\"\"\n",
    "    return probability triplet for a given layer/voxel\n",
    "    will try preferred splits first (default: [0] then 1..19)\n",
    "    \"\"\"\n",
    "    if prefer_splits is None:\n",
    "        prefer_splits = [0] + [s for s in range(1, 20)]\n",
    "    key = f\"probabilities_layer_{layer_idx}\"\n",
    "    for split in prefer_splits:\n",
    "        path = os.path.join(base_dir, str(split), file_names[label])\n",
    "        if not os.path.exists(path):\n",
    "            continue\n",
    "        try:\n",
    "            with h5py.File(path, \"r\") as f:\n",
    "                if key not in f:\n",
    "                    continue\n",
    "                arr = f[key]  # expected (V, 3)\n",
    "                if arr.ndim != 2 or arr.shape[1] < 3 or voxel_idx >= arr.shape[0]:\n",
    "                    continue\n",
    "                return np.array(arr[voxel_idx, :3], dtype=np.float64)\n",
    "        except OSError:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def base_dir_for_eta(eta):\n",
    "    return os.path.join(root, eta, f\"{eta}_regular_binning\")\n",
    "\n",
    "######################\n",
    "# uncomment to see outputs:\n",
    "\n",
    "\n",
    "# # print probability triplets for voxels/layers\n",
    "# for eta in eta_tags:\n",
    "#     base_dir = base_dir_for_eta(eta)\n",
    "#     example_path = os.path.join(base_dir, \"0\", file_names[\"default\"])\n",
    "#     if not os.path.exists(example_path):\n",
    "#         print(f\"[skip probs] {eta}: missing {example_path}\")\n",
    "#         continue\n",
    "\n",
    "#     # get all energy layers and their voxel counts from split 0 (default)\n",
    "#     with h5py.File(example_path, \"r\") as f0:\n",
    "#         energy_keys = sorted([k for k in f0.keys() if k.startswith(\"energy_layer_\")],\n",
    "#                              key=lambda s: int(s.split(\"_\")[-1]))\n",
    "\n",
    "#     print(f\"\\n {eta} — probability triplets for selected voxels \")\n",
    "#     for ekey in energy_keys:\n",
    "#         layer_idx = int(ekey.split(\"_\")[-1])\n",
    "#         # get voxel count for this layer\n",
    "#         with h5py.File(example_path, \"r\") as f0:\n",
    "#             if ekey not in f0:\n",
    "#                 continue\n",
    "#             V = f0[ekey].shape[1]\n",
    "#         valid_vox = [v for v in voxel_indices if v < V]\n",
    "#         if not valid_vox:\n",
    "#             continue\n",
    "\n",
    "#         print(f\"{ekey}:\")\n",
    "#         for v in valid_vox:\n",
    "#             trip_default  = read_prob_triplet_for_voxel(base_dir, \"default\",  layer_idx, v)\n",
    "#             trip_positive = read_prob_triplet_for_voxel(base_dir, \"positive\", layer_idx, v)\n",
    "\n",
    "#             d_str = (f\"[{trip_default[0]:.6g}, {trip_default[1]:.6g}, {trip_default[2]:.6g}]\"\n",
    "#                      if trip_default is not None else \"None\")\n",
    "#             p_str = (f\"[{trip_positive[0]:.6g}, {trip_positive[1]:.6g}, {trip_positive[2]:.6g}]\"\n",
    "#                      if trip_positive is not None else \"None\")\n",
    "\n",
    "#             print(f\"voxel {v:4d}-default: {d_str} | positive: {p_str}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea38824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_130/eta_130_regular_binning\"\n",
    "file_names = {\n",
    "    \"default\":  \"dataset_combined.hdf5\",\n",
    "    \"positive\": \"dataset_combined_positive.hdf5\",\n",
    "}\n",
    "splits = list(range(20))   # 0..19\n",
    "voxel_indices = [0, 5, 10, 20, 30, 40, 50]   # select a few voxel indices\n",
    "density = False\n",
    "bins_count = 100\n",
    "robust_pct = (0.1, 99.9)  # auto-tight x-axis\n",
    "ENERGY_EPS = 1e-12        # treat |energy| <= EPS as \"no hit\"\n",
    "colors = None             # None is matplotlib default\n",
    "\n",
    "# get layers\n",
    "probe_path = os.path.join(base_dir, \"0\", file_names[\"positive\"])\n",
    "with h5py.File(probe_path, 'r') as f0:\n",
    "    energy_keys = sorted([k for k in f0.keys() if k.startswith(\"energy_layer_\")],\n",
    "                         key=lambda s: int(s.split(\"_\")[-1]))\n",
    "\n",
    "def gather_energies_for_voxel(energy_key, voxel_idx, label):\n",
    "    \"\"\"concatenate energies across splits for a given layer/voxel/file label\"\"\"\n",
    "    vals = []\n",
    "    for split in splits:\n",
    "        path = os.path.join(base_dir, str(split), file_names[label])\n",
    "        if not os.path.exists(path):\n",
    "            continue\n",
    "        with h5py.File(path, 'r') as f:\n",
    "            if energy_key not in f:\n",
    "                continue\n",
    "            arr = f[energy_key]  # (N_events, V)\n",
    "            if voxel_idx >= arr.shape[1]:\n",
    "                continue\n",
    "            x = arr[:, voxel_idx]\n",
    "            x = x[np.isfinite(x)] # drop NaN/inf if any here\n",
    "            vals.append(x)\n",
    "    return np.concatenate(vals) if vals else np.array([], dtype=np.float32)\n",
    "\n",
    "# uncomment this to show output:\n",
    "\n",
    "# # plotting loop\n",
    "# for ekey in energy_keys:\n",
    "#     # determine voxel count for this layer\n",
    "#     V = None\n",
    "#     for lbl, fname in file_names.items():\n",
    "#         p = os.path.join(base_dir, \"0\", fname)\n",
    "#         if os.path.exists(p):\n",
    "#             with h5py.File(p, 'r') as f:\n",
    "#                 if ekey in f:\n",
    "#                     V = f[ekey].shape[1]\n",
    "#                     break\n",
    "#     if V is None:\n",
    "#         print(f\"[skip] {ekey}: not found\")\n",
    "#         continue\n",
    "\n",
    "#     valid_voxels = [v for v in voxel_indices if v < V]\n",
    "#     invalid = [v for v in voxel_indices if v >= V]\n",
    "#     if invalid: # some layers have less voxels:\n",
    "#         print(f\"[info] {ekey}: skipping out-of-range voxels {invalid} (V={V})\")\n",
    "#     if not valid_voxels:\n",
    "#         print(f\"[skip] {ekey}: no valid voxel indices from {voxel_indices} (V={V})\")\n",
    "#         continue\n",
    "\n",
    "#     for label in (\"default\", \"positive\"):\n",
    "#         # collect all selected-voxel values\n",
    "#         all_vals = []\n",
    "#         per_voxel_vals = {}\n",
    "#         for v in valid_voxels:\n",
    "#             vals = gather_energies_for_voxel(ekey, v, label)\n",
    "#             per_voxel_vals[v] = vals\n",
    "#             if vals.size:\n",
    "#                 all_vals.append(vals)\n",
    "\n",
    "#         # skip if no data at all for selected voxels\n",
    "#         if not any(per_voxel_vals[v].size for v in valid_voxels):\n",
    "#             print(f\"[skip] {ekey} :: {label}: no data for selected voxels\")\n",
    "#             continue\n",
    "\n",
    "#         # skip if ALL values are (near) zero across selected voxels (no hits)\n",
    "#         has_hits = any(np.any(np.abs(per_voxel_vals[v]) > ENERGY_EPS) for v in valid_voxels if per_voxel_vals[v].size)\n",
    "#         if not has_hits:\n",
    "#             print(f\"[skip empty] {ekey} :: {label}: all selected voxels have zero energy (no hits)\")\n",
    "#             continue\n",
    "\n",
    "#         # making the bins\n",
    "#         all_vals = np.concatenate(all_vals)\n",
    "#         lo = np.nanpercentile(all_vals, robust_pct[0])\n",
    "#         hi = np.nanpercentile(all_vals, robust_pct[1])\n",
    "#         if not np.isfinite(lo): lo = all_vals.min()\n",
    "#         if not np.isfinite(hi): hi = all_vals.max()\n",
    "#         if hi <= lo:\n",
    "#             lo, hi = float(all_vals.min()), float(all_vals.max())\n",
    "#         bins = np.linspace(lo, hi, bins_count+1)\n",
    "\n",
    "#         plt.figure(figsize=(8, 5))\n",
    "#         for i, v in enumerate(valid_voxels):\n",
    "#             vals = per_voxel_vals[v]\n",
    "#             if vals.size == 0:\n",
    "#                 continue\n",
    "#             plt.hist(vals, bins=bins, density=density, histtype=\"step\",\n",
    "#                      linewidth=1.5, label=f\"voxel {v}\",\n",
    "#                      color=None if colors is None else colors[i % len(colors)])\n",
    "#         plt.title(f\"{ekey} — {label} — splits 0–19\")\n",
    "#         plt.xlabel(\"Energy [MeV]\")\n",
    "#         plt.ylabel(\"Density\" if density else \"Count\")\n",
    "#         plt.xlim(lo, hi)\n",
    "#         plt.yscale('log')\n",
    "#         plt.legend(title=\"Selected voxels\")\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bede7eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_130/eta_130_regular_binning\"\n",
    "file_names = {\n",
    "    \"default\":  \"dataset_combined.hdf5\",\n",
    "    \"positive\": \"dataset_combined_positive.hdf5\",\n",
    "    \"fine\":     \"dataset_combined_fine.hdf5\",\n",
    "}\n",
    "splits = list(range(20))  # 0..19\n",
    "file_colors = {\"default\": \"C0\", \"positive\": \"C1\", \"fine\": \"C2\"}\n",
    "line_alpha = 0.25\n",
    "line_width = 0.6\n",
    "\n",
    "# get probabilities_* keys from split 0 of default\n",
    "example_path = os.path.join(base_dir, \"0\", file_names[\"default\"])\n",
    "with h5py.File(example_path, 'r') as f0:\n",
    "    prob_keys = sorted([k for k in f0.keys() if k.startswith(\"probabilities_\")],\n",
    "                       key=lambda s: int(s.split(\"_\")[-1]))\n",
    "\n",
    "##########################################33   \n",
    "# uncomment this to show output:\n",
    "\n",
    "# for prob_key in prob_keys:\n",
    "#     # get voxel count V\n",
    "#     V = None\n",
    "#     for label, fname in file_names.items():\n",
    "#         p = os.path.join(base_dir, \"0\", fname)\n",
    "#         if os.path.exists(p):\n",
    "#             with h5py.File(p, 'r') as f:\n",
    "#                 if prob_key in f:\n",
    "#                     V = f[prob_key].shape[0]\n",
    "#                     break\n",
    "#     if V is None:\n",
    "#         continue\n",
    "\n",
    "#     x = np.arange(V)\n",
    "#     fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n",
    "#     fig.suptitle(f\"{prob_key} — all splits overlaid (by class)\", fontsize=12)\n",
    "\n",
    "#     for cls in range(3):\n",
    "#         ax = axes[cls]\n",
    "#         for label, fname in file_names.items():\n",
    "#             color = file_colors[label]\n",
    "#             labeled = False\n",
    "#             for split in splits:\n",
    "#                 path = os.path.join(base_dir, str(split), fname)\n",
    "#                 if not os.path.exists(path):\n",
    "#                     continue\n",
    "#                 with h5py.File(path, 'r') as f:\n",
    "#                     if prob_key not in f:\n",
    "#                         continue\n",
    "#                     arr = f[prob_key][:]  # (V, 3)\n",
    "#                 if arr.shape[1] <= cls:\n",
    "#                     continue\n",
    "#                 # label only once per file here\n",
    "#                 ax.plot(\n",
    "#                     x, arr[:, cls],\n",
    "#                     color=color,\n",
    "#                     alpha=line_alpha,\n",
    "#                     linewidth=line_width,\n",
    "#                     label=(label if not labeled else None)\n",
    "#                 )\n",
    "#                 labeled = True\n",
    "\n",
    "#         ax.set_title(f\"Class {cls}\")\n",
    "#         ax.set_xlabel(\"Voxel index\")\n",
    "#         if cls == 0:\n",
    "#             ax.set_ylabel(\"Value\")\n",
    "#         ax.grid(alpha=0.2)\n",
    "#         ax.legend()\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad04874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path:\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_130/eta_130_regular_binning\"\n",
    "file_names = {\n",
    "    \"default\":  \"dataset_combined.hdf5\",\n",
    "    \"positive\": \"dataset_combined_positive.hdf5\",\n",
    "}\n",
    "splits = list(range(20))  # 0..19\n",
    "linestyles = {\"default\": \"-\", \"positive\": \"--\"}\n",
    "class_colors = {0: \"C0\", 1: \"C1\", 2: \"C2\"}  # Class 0/1/2 colors\n",
    "\n",
    "# get probabilities_* keys from split 0 of default\n",
    "example_path = os.path.join(base_dir, \"0\", file_names[\"default\"])\n",
    "with h5py.File(example_path, 'r') as f0:\n",
    "    prob_keys = sorted([k for k in f0.keys() if k.startswith(\"probabilities_\")],\n",
    "                       key=lambda s: int(s.split(\"_\")[-1]))\n",
    "\n",
    "    ############################################################\n",
    "# uncomment this to show output:   \n",
    "# # plotting loop\n",
    "# for prob_key in prob_keys:\n",
    "#     plt.figure(figsize=(8, 5))\n",
    "#     for label, fname in file_names.items():\n",
    "#         for split in splits:\n",
    "#             path = os.path.join(base_dir, str(split), fname)\n",
    "#             if not os.path.exists(path):\n",
    "#                 continue\n",
    "#             with h5py.File(path, 'r') as f:\n",
    "#                 if prob_key not in f:\n",
    "#                     continue\n",
    "#                 arr = f[prob_key][:]  # (N_voxels, 3)\n",
    "#                 for cls in range(3):\n",
    "#                     plt.plot(\n",
    "#                         np.arange(arr.shape[0]),\n",
    "#                         arr[:, cls],\n",
    "#                         linestyle=linestyles[label],\n",
    "#                         color=class_colors[cls],\n",
    "#                         alpha=0.4\n",
    "#                     )\n",
    "\n",
    "#     plt.title(f\"{prob_key} — all splits overlaid\")\n",
    "#     plt.xlabel(\"Voxel index\")\n",
    "#     plt.ylabel(\"Probability value\")\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7f6ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path:\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_130/eta_130_regular_binning\"\n",
    "split = \"10\"  # probabilities_* are per-voxel, choose a split\n",
    "file_names = {\n",
    "    \"default\":  \"dataset_combined.hdf5\",\n",
    "    \"positive\": \"dataset_combined_positive.hdf5\",\n",
    "}\n",
    "colors = {\"default\": \"C0\", \"positive\": \"C1\", \"fine\": \"C2\"}\n",
    "\n",
    "# get probabilities_* keys from default file\n",
    "example_path = os.path.join(base_dir, split, file_names[\"default\"])\n",
    "with h5py.File(example_path, 'r') as f0:\n",
    "    prob_keys = [k for k in f0.keys() if k.startswith(\"probabilities_\")]\n",
    "prob_keys = sorted(prob_keys, key=lambda s: int(s.split('_')[-1]))  # order by layer idx\n",
    "\n",
    "#######################################################\n",
    "# uncomment this to show output:\n",
    "# loop for plotting:\n",
    "# for prob_key in prob_keys:\n",
    "#     # voxel count\n",
    "#     with h5py.File(example_path, 'r') as f0:\n",
    "#         V = f0[prob_key].shape[0]\n",
    "#     x = np.arange(V)\n",
    "\n",
    "#     # one row: class 0, class 1, class 2\n",
    "#     fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n",
    "#     fig.suptitle(f\"{prob_key} — value vs. voxel index (split {split})\", fontsize=12)\n",
    "\n",
    "#     for class_idx, ax in enumerate(axes):\n",
    "#         for label, fname in file_names.items():\n",
    "#             path = os.path.join(base_dir, split, fname)\n",
    "#             if not os.path.exists(path):\n",
    "#                 continue\n",
    "#             with h5py.File(path, 'r') as f:\n",
    "#                 if prob_key not in f:\n",
    "#                     continue\n",
    "#                 arr = f[prob_key][:]  # shape (V, 3)\n",
    "#             ax.plot(x, arr[:, class_idx], label=label, color=colors[label])\n",
    "#         ax.set_title(f\"Class {class_idx}\")\n",
    "#         ax.set_xlabel(\"Voxel index\")\n",
    "#         if class_idx == 0:\n",
    "#             ax.set_ylabel(\"Value\")\n",
    "#         ax.grid(alpha=0.2)\n",
    "#         ax.legend()\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52892ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path:\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_130/eta_130_regular_binning\"\n",
    "file_names = {\n",
    "    \"default\": \"dataset_combined.hdf5\",\n",
    "    \"positive\": \"dataset_combined_positive.hdf5\",\n",
    "    \"fine\": \"dataset_combined_fine.hdf5\",\n",
    "}\n",
    "splits_to_compare = [0, 3, 5, 9, 13, 19]   # choosing random splits\n",
    "use_density = True \n",
    "nbins = 50\n",
    "\n",
    "# color per split\n",
    "split_colors = {s: f\"C{i}\" for i, s in enumerate(splits_to_compare)}\n",
    "\n",
    "# get probabilities_* keys from an example file\n",
    "example_path = os.path.join(base_dir, str(splits_to_compare[0]), file_names[\"default\"])\n",
    "with h5py.File(example_path, \"r\") as f0:\n",
    "    prob_keys = sorted([k for k in f0.keys() if k.startswith(\"probabilities_\")],\n",
    "                       key=lambda s: int(s.split(\"_\")[-1]))\n",
    "\n",
    "    \n",
    "########################################################################\n",
    "# uncomment this to show output:   \n",
    "# for each probabilities_* dataset loop\n",
    "# for prob_key in prob_keys:\n",
    "#     for label, fname in file_names.items():\n",
    "#         # load arrays for the chosen splits\n",
    "#         per_split = {}\n",
    "#         for s in splits_to_compare:\n",
    "#             path = os.path.join(base_dir, str(s), fname)\n",
    "#             if not os.path.exists(path):\n",
    "#                 continue\n",
    "#             with h5py.File(path, \"r\") as f:\n",
    "#                 if prob_key not in f:\n",
    "#                     continue\n",
    "#                 arr = f[prob_key][:]  # expected shape (N, 3)\n",
    "#                 if arr.ndim == 2 and arr.shape[1] >= 3:\n",
    "#                     per_split[s] = arr\n",
    "\n",
    "#         if not per_split:\n",
    "#             print(f\"no data for {label} :: {prob_key} in splits {splits_to_compare}\")\n",
    "#             continue\n",
    "\n",
    "#         # make 3 subplots: class 0/1/2\n",
    "#         fig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n",
    "#         fig.suptitle(f\"{label} — {prob_key} — splits {splits_to_compare}\", fontsize=12)\n",
    "\n",
    "#         for cls in range(3):\n",
    "#             ax = axes[cls]\n",
    "\n",
    "#             # bins\n",
    "#             pooled = np.concatenate([a[:, cls] for a in per_split.values()], axis=0)\n",
    "#             lo = np.nanpercentile(pooled, 0.1)\n",
    "#             hi = np.nanpercentile(pooled, 99.9)\n",
    "#             if not np.isfinite(lo): lo = np.nanmin(pooled)\n",
    "#             if not np.isfinite(hi): hi = np.nanmax(pooled)\n",
    "#             if hi <= lo: lo, hi = float(np.nanmin(pooled)), float(np.nanmax(pooled))\n",
    "#             bins = np.linspace(lo, hi, nbins)\n",
    "\n",
    "#             # now overlay each split in different colors\n",
    "#             for s, arr in per_split.items():\n",
    "#                 ax.hist(\n",
    "#                     arr[:, cls],\n",
    "#                     bins=bins,\n",
    "#                     density=use_density,\n",
    "#                     histtype=\"step\",\n",
    "#                     linewidth=1.5,\n",
    "#                     color=split_colors[s],\n",
    "#                     label=f\"split {s}\",\n",
    "#                 )\n",
    "\n",
    "#             ax.set_title(f\"Class {cls}\")\n",
    "#             ax.set_xlabel(\"Value\")\n",
    "#             if cls == 0:\n",
    "#                 ax.set_ylabel(\"Density\" if use_density else \"Count\")\n",
    "#             ax.grid(alpha=0.2)\n",
    "#             ax.legend()\n",
    "\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ca45696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path:\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_130/eta_130_regular_binning\"\n",
    "file_names = {\n",
    "    \"default\": \"dataset_combined.hdf5\",\n",
    "    \"positive\": \"dataset_combined_positive.hdf5\",\n",
    "    \"fine\": \"dataset_combined_fine.hdf5\",\n",
    "}\n",
    "splits = list(range(20))  # 0..19\n",
    "class_colors = ['skyblue', 'salmon', 'lightgreen']  # class 0, 1, 2\n",
    "use_density = True \n",
    "\n",
    "# get probabilities_* keys from file\n",
    "example_path = os.path.join(base_dir, \"0\", file_names[\"default\"])\n",
    "with h5py.File(example_path, 'r') as f0:\n",
    "    prob_keys = [k for k in f0.keys() if k.startswith(\"probabilities_\")]\n",
    "\n",
    "    \n",
    "###############################################\n",
    "# uncomment this to show the output:    \n",
    "# loop\n",
    "# for prob_key in prob_keys:\n",
    "#     # for each file type (default/positive/fine), combine all splits then plot 3 classes\n",
    "#     for label, fname in file_names.items():\n",
    "#         combined = []\n",
    "\n",
    "#         for split in splits:\n",
    "#             path = os.path.join(base_dir, str(split), fname)\n",
    "#             if not os.path.exists(path):\n",
    "#                 continue\n",
    "#             with h5py.File(path, 'r') as f:\n",
    "#                 if prob_key not in f:\n",
    "#                     continue\n",
    "#                 arr = f[prob_key][:]  # shape (N, 3)\n",
    "#                 if arr.ndim != 2 or arr.shape[1] < 3:\n",
    "#                     continue\n",
    "#                 combined.append(arr)\n",
    "\n",
    "#         if not combined:\n",
    "#             print(f\"no data found for {label} : {prob_key}\")\n",
    "#             continue\n",
    "\n",
    "#         all_data = np.vstack(combined)  # shape (total_rows, 3)\n",
    "\n",
    "#         # plot the three columns separately on one figure\n",
    "#         plt.figure(figsize=(7, 4))\n",
    "#         for cls in range(3):\n",
    "#             plt.hist(\n",
    "#                 all_data[:, cls],\n",
    "#                 bins=50,\n",
    "#                 alpha=0.6,\n",
    "#                 density=use_density,\n",
    "#                 label=f\"Class {cls}\",\n",
    "#                 color=class_colors[cls],\n",
    "#             )\n",
    "\n",
    "#         plt.title(f\"{label} — {prob_key} (splits 0–19 combined)\")\n",
    "#         plt.xlabel(\"Value\")\n",
    "#         plt.ylabel(\"Density\" if use_density else \"Count\")\n",
    "#         plt.legend()\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2120729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths:\n",
    "base_paths = {\n",
    "    \"default\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning\",\n",
    "    \"positive\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning\",\n",
    "    \"fine\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning\",\n",
    "}\n",
    "\n",
    "file_names = {\n",
    "    \"default\": \"dataset_combined.hdf5\",\n",
    "    \"positive\": \"dataset_combined_positive.hdf5\",\n",
    "    \"fine\": \"dataset_combined_fine.hdf5\",\n",
    "}\n",
    "\n",
    "colors = ['skyblue', 'salmon', 'lightgreen']\n",
    "\n",
    "# get the probabilities_* keys from one file\n",
    "example_path = os.path.join(base_paths[\"default\"], \"0\", file_names[\"default\"])\n",
    "with h5py.File(example_path, 'r') as f:\n",
    "    prob_keys = [k for k in f.keys() if k.startswith(\"probabilities_\")]\n",
    "\n",
    "######################################################################    \n",
    "# uncommment this to show output:\n",
    "# loop over each probabilities_* dataset\n",
    "# for prob_key in prob_keys:\n",
    "#     plt.figure(figsize=(6, 4))\n",
    "\n",
    "#     for idx, label in enumerate([\"default\", \"positive\", \"fine\"]):\n",
    "#         combined_data = []\n",
    "\n",
    "#         #loop over splits 0–19\n",
    "#         for split in range(20):\n",
    "#             split_path = os.path.join(base_paths[label], str(split), file_names[label])\n",
    "#             if not os.path.exists(split_path):\n",
    "#                 continue\n",
    "#             with h5py.File(split_path, 'r') as f:\n",
    "#                 if prob_key in f:\n",
    "#                     combined_data.append(f[prob_key][:])\n",
    "\n",
    "#         if not combined_data:\n",
    "#             continue \n",
    "\n",
    "#         # concatenate all splits into one array\n",
    "#         all_data = np.vstack(combined_data)  # shape (total_rows, 3)\n",
    "\n",
    "#         # flatten classes together\n",
    "#         plt.hist(\n",
    "#             all_data.flatten(),\n",
    "#             bins=50,\n",
    "#             alpha=0.5,\n",
    "#             label=label,\n",
    "#             color=colors[idx]\n",
    "#         )\n",
    "\n",
    "#     plt.title(f\"{prob_key} (all splits combined)\")\n",
    "#     plt.xlabel(\"Value\")\n",
    "#     plt.ylabel(\"Count\")\n",
    "#     plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4a31615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "File: default (/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/0/dataset_combined.hdf5)\n",
      "\n",
      "File: positive (/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/0/dataset_combined_positive.hdf5)\n",
      "\n",
      "File: fine (/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/0/dataset_combined_fine.hdf5)\n"
     ]
    }
   ],
   "source": [
    "file_paths = {\n",
    "    \"default\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/0/dataset_combined.hdf5\",\n",
    "    \"positive\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/0/dataset_combined_positive.hdf5\",\n",
    "    \"fine\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/0/dataset_combined_fine.hdf5\",\n",
    "}\n",
    "\n",
    "for label, path in file_paths.items():\n",
    "    print(f\"\\nFile: {label} ({path})\")\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        for key in f.keys():\n",
    "            dataset = f[key]\n",
    "            #print(f\"  {key}: shape={dataset.shape}, dtype={dataset.dtype}\")\n",
    "            # show a small sample of the values\n",
    "            arr = dataset[()]\n",
    "            if arr.ndim == 1:\n",
    "                preview = arr[:2]  # first 2 entries\n",
    "            else:\n",
    "                preview = arr[:2, :]  # first 2 rows\n",
    "            # uncomment this to show output:\n",
    "            #print(f\"sample values:\\n{preview}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32e55d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist(f, fn):\n",
    "    \"\"\"\n",
    "    function to plot histograms from the files for eta_mod, phi_mod, center_eta, and log_2(E)\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 3), sharey=False, sharex=False, tight_layout=False)\n",
    "    fig.text(0.5, 1.01, f'{fn}', ha='center', fontsize=15)\n",
    "\n",
    "    axes[0].hist(f['eta_mod'], bins=50, color='skyblue')\n",
    "    axes[0].set_xlabel('eta_mod')\n",
    "    axes[0].set_ylabel('Histogram')\n",
    "\n",
    "    axes[1].hist(f['phi_mod'], bins=50, color='magenta')\n",
    "    axes[1].set_xlabel('phi_mod')\n",
    "    axes[1].set_ylabel('Histogram')\n",
    "\n",
    "    axes[2].hist(f['center_eta'], bins=50, color='orange')\n",
    "    axes[2].set_xlabel('center_eta')\n",
    "    axes[2].set_ylabel('Histogram')\n",
    "\n",
    "    axes[3].hist(np.log2(f['incident_energy']), density=False, log=True, bins=np.arange(8, 24, 0.5))\n",
    "    axes[3].set_xlabel('log_2(E)')\n",
    "    axes[3].set_ylabel('Histogram')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# loop over the files to plot\n",
    "file_paths = {\n",
    "    \"default\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_default_binning/dataset_combined.hdf5\",\n",
    "    \"positive\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_default_binning/dataset_combined_positive.hdf5\",\n",
    "    \"fine\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_default_binning/dataset_combined_fine.hdf5\",\n",
    "}\n",
    "\n",
    "# uncomment to show outputs:\n",
    "# for label, path in file_paths.items():\n",
    "#     with h5py.File(path, 'r') as f_h5:\n",
    "#         data_dict = {\n",
    "#             'eta_mod': f_h5['eta_mod'][:],\n",
    "#             'phi_mod': f_h5['phi_mod'][:],\n",
    "#             'center_eta': f_h5['center_eta'][:],\n",
    "#             'incident_energy': f_h5['incident_energy'][:]\n",
    "#         }\n",
    "#         plot_hist(data_dict, f\"{label} binning\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94f4292f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting inferred bin counts in active layers:\n",
      "/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/dataset_combined_fine.hdf5\n",
      "\n",
      "Layer  0: radial bins = 48, angular bins = 28\n",
      "Layer  1: radial bins = 48, angular bins = 28\n",
      "Layer  2: radial bins = 48, angular bins = 28\n",
      "Layer  3: radial bins = 48, angular bins = 28\n",
      "Layer 12: radial bins = 48, angular bins = 28\n"
     ]
    }
   ],
   "source": [
    "def infer_r_phi_bins(f, layer):\n",
    "    \"\"\"\n",
    "    function to get r and phi bins\n",
    "    \"\"\"\n",
    "    r_key = f\"binstart_radius_layer_{layer}\"\n",
    "    phi_key = f\"binstart_alpha_layer_{layer}\"\n",
    "    if r_key in f and phi_key in f:\n",
    "        r_edges = f[r_key][:]\n",
    "        phi_edges = f[phi_key][:]\n",
    "        total_voxels = len(r_edges)\n",
    "        for r_bins in range(1, total_voxels + 1):\n",
    "            if total_voxels % r_bins != 0:\n",
    "                continue\n",
    "            phi_bins = total_voxels // r_bins\n",
    "            # check uniqueness of phi in a slice\n",
    "            sample_phi = phi_edges[:phi_bins]\n",
    "            if len(set(sample_phi)) == phi_bins:\n",
    "                return r_bins, phi_bins\n",
    "        return None, None\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# file path:\n",
    "path = \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/dataset_combined_fine.hdf5\"\n",
    "\n",
    "with h5py.File(path, 'r') as f:\n",
    "    print(f\"bin counts in active layers:\\n{path}\\n\")\n",
    "\n",
    "    for layer in range(24):\n",
    "        energy_key = f\"energy_layer_{layer}\"\n",
    "        if energy_key in f:\n",
    "            energy = f[energy_key][:]\n",
    "            if np.any(energy):  # check if energy is non-zero\n",
    "                r_bins, phi_bins = infer_r_phi_bins(f, layer)\n",
    "                if r_bins and phi_bins:\n",
    "                    print(f\"Layer {layer:2d}: radial bins = {r_bins:2d}, angular bins = {phi_bins:2d}\")\n",
    "                else:\n",
    "                    print(f\"Layer {layer:2d}: issue\")\n",
    "        else:\n",
    "            print(f\"Layer {layer:2d}: skipped (no energy_layer key found)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1931e085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active layers (for chosen event): [0, 1, 2, 3, 12]\n"
     ]
    }
   ],
   "source": [
    "# can check a single event or all events\n",
    "def active_layers_for_event(f, event_n, threshold=0.0):\n",
    "    active = []\n",
    "    for L in range(24):\n",
    "        ek = f'energy_layer_{L}'\n",
    "        if ek in f:\n",
    "            e = f[ek][event_n] \n",
    "            if np.any(e > threshold): # choose 0 (ie greater than 0 energy in a voxel means is an active layer)\n",
    "                active.append(L)\n",
    "    return active\n",
    "\n",
    "# check active layers for a given event:\n",
    "with h5py.File(path, 'r') as f:\n",
    "    print(\"active layers (for chosen event):\", active_layers_for_event(f, event_n=600, threshold=0.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
