{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "561a09dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina' \n",
    "\n",
    "cwd   = os.getcwd()\n",
    "parent_cwd = os.path.dirname(cwd)\n",
    "\n",
    "sys.path.insert(0, cwd)\n",
    "sys.path.insert(0, parent_cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d2d0576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hydra\n",
    "from hydra import initialize, compose\n",
    "\n",
    "# hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "# initialize(version_base=None, config_path=\"../config\")\n",
    "# config=compose(config_name=\"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9938f712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " File: default (/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_default_binning/dataset_combined.hdf5)\n",
      "\n",
      " File: positive (/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_default_binning/dataset_combined_positive.hdf5)\n",
      "\n",
      " File: fine (/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_default_binning/dataset_combined_fine.hdf5)\n"
     ]
    }
   ],
   "source": [
    "# default binning shapes:\n",
    "file_paths = {\n",
    "    \"default\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_default_binning/dataset_combined.hdf5\",\n",
    "    \"positive\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_default_binning/dataset_combined_positive.hdf5\",\n",
    "    \"fine\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_default_binning/dataset_combined_fine.hdf5\",\n",
    "}\n",
    "\n",
    "for label, path in file_paths.items():\n",
    "    print(f\"\\n File: {label} ({path})\")\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        for key in f.keys():\n",
    "            dataset = f[key]\n",
    "            # uncomment to see the output\n",
    "            #print(f\" {key}: shape={dataset.shape}, dtype={dataset.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acbfe7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_r_phi_bins(f, layer):\n",
    "    \"\"\"\n",
    "    function to get the r and phi bins\n",
    "    \"\"\"\n",
    "    r_key = f\"binstart_radius_layer_{layer}\"\n",
    "    phi_key = f\"binstart_alpha_layer_{layer}\"\n",
    "    if r_key in f and phi_key in f:\n",
    "        r_edges = f[r_key][:]\n",
    "        phi_edges = f[phi_key][:]\n",
    "        total_voxels = len(r_edges)\n",
    "        for r_bins in range(1, total_voxels + 1):\n",
    "            if total_voxels % r_bins != 0:\n",
    "                continue\n",
    "            phi_bins = total_voxels // r_bins\n",
    "            # check uniqueness of phi within inner loop\n",
    "            sample_phi = phi_edges[:phi_bins]\n",
    "            if len(set(sample_phi)) == phi_bins:\n",
    "                return r_bins, phi_bins\n",
    "        return None, None\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "# old file path:\n",
    "#path = \"/fast_scratch_1/caloqvae/data/atlas_regular/dataset_eta_020_positive.hdf5\"\n",
    "# new file path:\n",
    "path = \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_100/eta_100_regular_binning/dataset_combined_fine.hdf5\"\n",
    "\n",
    "# uncomment to see output:\n",
    "# with h5py.File(path, 'r') as f:\n",
    "#     print(f\"bin counts in: {path}\\n\")\n",
    "#     for layer in range(24):\n",
    "#         r_bins, phi_bins = infer_r_phi_bins(f, layer)\n",
    "#         if r_bins and phi_bins:\n",
    "#             print(f\"Layer {layer:2d}: radial bins = {r_bins:2d}, angular bins = {phi_bins:2d}\")\n",
    "#         else:\n",
    "#             print(f\"Layer {layer:2d}: error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e59a654f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now only for active layers:\n",
    "\n",
    "# file path:\n",
    "path = \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/dataset_combined_fine.hdf5\"\n",
    "\n",
    "\n",
    "# uncomment to see output:\n",
    "# with h5py.File(path, 'r') as f:\n",
    "#     print(f\"bin counts in active layers:\\n{path}\\n\")\n",
    "\n",
    "#     for layer in range(24):\n",
    "#         energy_key = f\"energy_layer_{layer}\"\n",
    "#         if energy_key in f:\n",
    "#             energy = f[energy_key][:]\n",
    "#             if np.any(energy):  # check if energy is non-zero\n",
    "#                 r_bins, phi_bins = infer_r_phi_bins(f, layer)\n",
    "#                 if r_bins and phi_bins:\n",
    "#                     print(f\"Layer {layer:2d}: radial bins = {r_bins:2d}, angular bins = {phi_bins:2d}\")\n",
    "#                 else:\n",
    "#                     print(f\"Layer {layer:2d}: error\")\n",
    "#         else:\n",
    "#             print(f\"Layer {layer:2d}: skipped (no energy_layer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2211b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eta tags for files\n",
    "eta_tags = [\n",
    "    \"eta_000\", \"eta_005\", \"eta_010\", \"eta_015\", \"eta_020\", \"eta_025\", \"eta_030\", \"eta_035\", \"eta_040\",\n",
    "    \"eta_045\", \"eta_050\", \"eta_055\", \"eta_060\", \"eta_065\", \"eta_070\", \"eta_075\", \"eta_080\", \"eta_085\",\n",
    "    \"eta_090\", \"eta_095\", \"eta_100\", \"eta_105\", \"eta_110\", \"eta_115\", \"eta_120\", \"eta_125\", \"eta_130\",\n",
    "]\n",
    "\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31\"\n",
    "\n",
    "# here choose fine, positive, or combined\n",
    "file_type = \"dataset_combined_fine.hdf5\"\n",
    "# file_type = \"dataset_combined.hdf5\"\n",
    "\n",
    "\n",
    "# uncomment to see output:\n",
    "\n",
    "# for eta in eta_tags:\n",
    "#     file_path = os.path.join(base_dir, eta, f\"{eta}_regular_binning\", file_type)\n",
    "    \n",
    "#     if not os.path.exists(file_path):\n",
    "#         print(f\"\\n{eta}: file not found {file_path}\")\n",
    "#         continue\n",
    "\n",
    "#     with h5py.File(file_path, 'r') as f:\n",
    "#         print(f\"\\nbin counts in active layers: {eta}\\n{file_path}\\n\")\n",
    "#         for layer in range(24):\n",
    "#             energy_key = f\"energy_layer_{layer}\"\n",
    "#             if energy_key in f:\n",
    "#                 energy = f[energy_key][:]\n",
    "#                 if np.any(energy):\n",
    "#                     r_bins, phi_bins = infer_r_phi_bins(f, layer)\n",
    "#                     if r_bins and phi_bins:\n",
    "#                         print(f\"Layer {layer:2d}: radial bins = {r_bins:3d}, angular bins = {phi_bins:3d}\")\n",
    "#                     else:\n",
    "#                         print(f\"Layer {layer:2d}: error\")\n",
    "#             else:\n",
    "#                 print(f\"Layer {layer:2d}: skipped (no energy_layer)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0a4f188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell looks at each split:\n",
    "\n",
    "eta_tags = [\n",
    "    \"eta_000\", \"eta_005\", \"eta_010\", \"eta_015\", \"eta_020\", \"eta_025\", \"eta_030\", \"eta_035\", \"eta_040\",\n",
    "    \"eta_045\", \"eta_050\", \"eta_055\", \"eta_060\", \"eta_065\", \"eta_070\", \"eta_075\", \"eta_080\", \"eta_085\",\n",
    "    \"eta_090\", \"eta_095\", \"eta_100\", \"eta_105\", \"eta_110\", \"eta_115\", \"eta_120\", \"eta_125\", \"eta_130\",\n",
    "]\n",
    "\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31\"\n",
    "file_name = \"dataset_combined.hdf5\"\n",
    "\n",
    "##############################################################\n",
    "# uncomment this to print outputs:\n",
    "# for eta in eta_tags:\n",
    "#     for i in range(20):\n",
    "#         file_path = os.path.join(base_dir, eta, f\"{eta}_regular_binning\", str(i), file_name)\n",
    "        \n",
    "#         if not os.path.exists(file_path):\n",
    "#             print(f\"{eta} | split {i:2d}: file not found: {file_path}\")\n",
    "#             continue\n",
    "\n",
    "#         with h5py.File(file_path, 'r') as f:\n",
    "#             print(f\"\\n {eta} | split {i:2d}\\n{file_path}\\n\")\n",
    "\n",
    "#             for layer in range(24):\n",
    "#                 energy_key = f\"energy_layer_{layer}\"\n",
    "#                 if energy_key in f:\n",
    "#                     energy = f[energy_key][:]\n",
    "#                     if np.any(energy):\n",
    "#                         r_bins, phi_bins = infer_r_phi_bins(f, layer)\n",
    "#                         if r_bins and phi_bins:\n",
    "#                             print(f\"Layer {layer:2d}: radial bins = {r_bins:3d}, angular bins = {phi_bins:3d}\")\n",
    "#                         else:\n",
    "#                             print(f\"Layer {layer:2d}: error\")\n",
    "#                 else:\n",
    "#                     print(f\"Layer {layer:2d}: skipped (no energy_layer)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4169394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eta values to check\n",
    "eta_tags = [\n",
    "    \"eta_000\", \"eta_005\", \"eta_010\", \"eta_015\", \"eta_020\", \"eta_025\", \"eta_030\", \"eta_035\", \"eta_040\",\n",
    "    \"eta_045\", \"eta_050\", \"eta_055\", \"eta_060\", \"eta_065\", \"eta_070\", \"eta_075\", \"eta_080\", \"eta_085\",\n",
    "    \"eta_090\", \"eta_095\", \"eta_100\", \"eta_105\", \"eta_110\", \"eta_115\", \"eta_120\", \"eta_125\", \"eta_130\",\n",
    "]\n",
    "\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31\"\n",
    "# checking positive, normal, and fine\n",
    "main_file_name = \"dataset_combined_positive.hdf5\"\n",
    "split_file_name = \"dataset_combined_positive.hdf5\"\n",
    "\n",
    "\n",
    "# uncomment this to see the output:\n",
    "\n",
    "\n",
    "# for eta in eta_tags:\n",
    "#     # loading the main combined file\n",
    "#     main_path = os.path.join(base_dir, eta, f\"{eta}_regular_binning\", main_file_name)\n",
    "#     if not os.path.exists(main_path):\n",
    "#         print(f\"\\n{eta}: MAIN file not found: {main_path}\")\n",
    "#         continue\n",
    "\n",
    "#     with h5py.File(main_path, 'r') as f_main:\n",
    "#         print(f\"\\n Comparing binning for: {eta}\")\n",
    "#         print(f\"Main file: {main_path}\")\n",
    "\n",
    "#         # determine active layers in the main file and binning\n",
    "#         main_active_layers = {}\n",
    "#         for layer in range(24):\n",
    "#             energy_key = f\"energy_layer_{layer}\"\n",
    "#             if energy_key in f_main and np.any(f_main[energy_key][:]):\n",
    "#                 r_bins, phi_bins = infer_r_phi_bins(f_main, layer)\n",
    "#                 main_active_layers[layer] = (r_bins, phi_bins)\n",
    "\n",
    "#         # binning info from split files\n",
    "#         split_layer_bins = {}  # layer -> set of (r, phi) binning\n",
    "\n",
    "#         for i in range(20):\n",
    "#             split_path = os.path.join(base_dir, eta, f\"{eta}_regular_binning\", str(i), split_file_name)\n",
    "#             if not os.path.exists(split_path):\n",
    "#                 print(f\"{eta} | split {i:2d}: file not found: {split_path}\")\n",
    "#                 continue\n",
    "\n",
    "#             with h5py.File(split_path, 'r') as f_split:\n",
    "#                 for layer in range(24):\n",
    "#                     energy_key = f\"energy_layer_{layer}\"\n",
    "#                     if energy_key in f_split and np.any(f_split[energy_key][:]):\n",
    "#                         r_phi = infer_r_phi_bins(f_split, layer)\n",
    "#                         if layer not in split_layer_bins:\n",
    "#                             split_layer_bins[layer] = set()\n",
    "#                         split_layer_bins[layer].add(r_phi)\n",
    "\n",
    "#         # now compare\n",
    "#         for layer in sorted(split_layer_bins.keys()):\n",
    "#             split_binnings = split_layer_bins[layer]\n",
    "#             in_main = layer in main_active_layers\n",
    "\n",
    "#             if in_main:\n",
    "#                 main_rphi = main_active_layers[layer]\n",
    "#                 if split_binnings == {main_rphi}:\n",
    "#                     print(f\" {eta} | Layer {layer:2d}: Match | Binning = {main_rphi}\")\n",
    "#                 else:\n",
    "#                     print(f\" {eta} | Layer {layer:2d}: Mismatch | Main = {main_rphi} | Split = {split_binnings}\")\n",
    "#             else:\n",
    "#                 print(f\" {eta} | Layer {layer:2d}: Extra in split (inactive in main) | Binning = {split_binnings}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a12806ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_voxels_per_layer(hdf5_path):\n",
    "    \"\"\"\n",
    "    function to look at the voxels per layer\n",
    "    \"\"\"\n",
    "    with h5py.File(hdf5_path, 'r') as f:\n",
    "        print(f\"\\n File: {hdf5_path}\")\n",
    "        for layer in range(24):\n",
    "            energy_key = f'energy_layer_{layer}'\n",
    "            if energy_key in f:\n",
    "                energy_shape = f[energy_key].shape\n",
    "                n_voxels = energy_shape[1]\n",
    "                print(f\"  Layer {layer:2d}: {n_voxels} voxels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93c6ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking files:\n",
    "#inspect_voxels_per_layer(\"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/dataset_combined.hdf5\")\n",
    "#inspect_voxels_per_layer(\"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/dataset_combined_positive.hdf5\")\n",
    "#inspect_voxels_per_layer(\"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/dataset_combined_fine.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f56b24e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigating voxel hits and events:\n",
    "\n",
    "# file path:\n",
    "base_path = \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning\"\n",
    "\n",
    "# for eta_idx in range(20):\n",
    "#     file_path = os.path.join(base_path, str(eta_idx), \"dataset_combined.hdf5\")\n",
    "#     if not os.path.exists(file_path):\n",
    "#         print(f\"file missing: {file_path}\")\n",
    "#         continue\n",
    "\n",
    "#     print(f\"\\nFile: {file_path}\")\n",
    "#     with h5py.File(file_path, 'r') as f:\n",
    "#         total_events = f[\"incident_energy\"].shape[0]\n",
    "#         print(f\"Total events: {total_events}\")\n",
    "\n",
    "#         for layer in range(24):\n",
    "#             key = f\"energy_layer_{layer}\"\n",
    "#             if key not in f:\n",
    "#                 continue\n",
    "\n",
    "#             energy = f[key][:]\n",
    "#             if energy.ndim != 2:\n",
    "#                 continue\n",
    "\n",
    "#             hit_mask = energy > 0\n",
    "#             n_hits = np.count_nonzero(hit_mask)\n",
    "#             n_voxels_hit = np.any(hit_mask, axis=0).sum()\n",
    "#             n_events_hit = np.any(hit_mask, axis=1).sum()\n",
    "\n",
    "#             if n_hits > 0:\n",
    "#                 print(f\"  Layer {layer:2d}: {n_hits:9d} total hits, \"\n",
    "#                       f\"{n_voxels_hit:4d} voxels hit, \"\n",
    "#                       f\"{n_events_hit:6d} events with energy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79243ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out how many events are in the split files:\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning\"\n",
    "combined_path = os.path.join(base_dir, \"dataset_combined.hdf5\")\n",
    "\n",
    "# # count total events in combined file\n",
    "# with h5py.File(combined_path, 'r') as f:\n",
    "#     combined_events = f['incident_energy'].shape[0]\n",
    "#     print(f\"\\n combined file: {combined_events} total events\")\n",
    "\n",
    "# # count total events across splits\n",
    "# split_total = 0\n",
    "# for i in range(20):\n",
    "#     file_path = os.path.join(base_dir, str(i), \"dataset_combined.hdf5\")\n",
    "#     if os.path.exists(file_path):\n",
    "#         with h5py.File(file_path, 'r') as f:\n",
    "#             n = f['incident_energy'].shape[0]\n",
    "#             split_total += n\n",
    "#             print(f\"  file: {i:2d}: {n} events\")\n",
    "#     else:\n",
    "#         print(f\"missing: split {i:2d}\")\n",
    "\n",
    "# print(f\"\\n Total in 0–19 split files: {split_total}\")\n",
    "# print(f\" Matches combined: {split_total == combined_events}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "829880f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now checking every eta:\n",
    "\n",
    "eta_dirs = [\n",
    "    f\"eta_{i:03d}\" for i in range(0, 135+1, 5)\n",
    "]  # generates eta_000, eta_005, ..., eta_130\n",
    "\n",
    "base_path = \"/fast_scratch_1/caloqvae/data/atlas_july31\"\n",
    "\n",
    "# for eta_tag in eta_dirs:\n",
    "#     regular_dir = os.path.join(base_path, eta_tag, f\"{eta_tag}_regular_binning\")\n",
    "#     combined_file = os.path.join(regular_dir, \"dataset_combined_fine.hdf5\") # add positive or fine to look at others too\n",
    "\n",
    "#     if not os.path.exists(combined_file):\n",
    "#         print(f\"\\n missing combined file: {combined_file}\")\n",
    "#         continue\n",
    "\n",
    "#     with h5py.File(combined_file, \"r\") as f:\n",
    "#         n_combined = f[\"incident_energy\"].shape[0]\n",
    "\n",
    "#     total_split = 0\n",
    "#     missing_bins = []\n",
    "#     for i in range(20):\n",
    "#         split_path = os.path.join(regular_dir, str(i), \"dataset_combined_fine.hdf5\") # positive or fine\n",
    "#         if os.path.exists(split_path):\n",
    "#             with h5py.File(split_path, \"r\") as f:\n",
    "#                 n = f[\"incident_energy\"].shape[0]\n",
    "#                 total_split += n\n",
    "#         else:\n",
    "#             missing_bins.append(i)\n",
    "\n",
    "#     status = \" MATCH\" if total_split == n_combined else \"MISMATCH\"\n",
    "#     print(f\"\\n{eta_tag}:\")\n",
    "#     print(f\"Combined file: {n_combined} events\")\n",
    "#     print(f\"Split total  : {total_split} events\")\n",
    "#     print(f\"{status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a73841a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm # to track time\n",
    "\n",
    "def load_showers_and_incident_energy(path):\n",
    "    with h5py.File(path, 'r') as file:\n",
    "        data = {key: torch.tensor(file[key][:]) for key in file.keys()}\n",
    "        incident_energy = data[\"incident_energy\"]\n",
    "\n",
    "        valid_layers = []\n",
    "        for l in range(24):\n",
    "            key = f\"energy_layer_{l}\"\n",
    "            if key in data and (data[key].sum(dim=1) != 0).any():\n",
    "                valid_layers.append(l)\n",
    "\n",
    "        if not valid_layers:\n",
    "            return None, None\n",
    "\n",
    "        combined = torch.cat([data[f\"energy_layer_{l}\"] for l in valid_layers], dim=1)\n",
    "        showers = combined * incident_energy.unsqueeze(1)\n",
    "        return showers.numpy(), incident_energy.numpy()\n",
    "\n",
    "eta_dirs = [f\"eta_{i:03d}\" for i in range(0, 135 + 1, 5)]\n",
    "base_path = \"/fast_scratch_1/caloqvae/data/atlas_july31\"\n",
    "file_name = \"dataset_combined.hdf5\"  # or combined_fine\n",
    "\n",
    "#####################################################\n",
    "# uncomment this to see output:\n",
    "\n",
    "\n",
    "# for eta_tag in tqdm(eta_dirs):\n",
    "#     regular_dir = os.path.join(base_path, eta_tag, f\"{eta_tag}_regular_binning\")\n",
    "#     combined_path = os.path.join(regular_dir, file_name)\n",
    "\n",
    "#     if not os.path.exists(combined_path):\n",
    "#         print(f\" Missing combined file: {combined_path}\")\n",
    "#         continue\n",
    "\n",
    "#     # loading combined\n",
    "#     combined_showers, combined_incident_energy = load_showers_and_incident_energy(combined_path)\n",
    "#     if combined_showers is None:\n",
    "#         print(f\"skipping {eta_tag} due to no valid layers in combined.\")\n",
    "#         continue\n",
    "\n",
    "#     # load split and concat\n",
    "#     split_showers_list, split_incident_energy_list = [], []\n",
    "#     for i in range(20):\n",
    "#         split_path = os.path.join(regular_dir, str(i), file_name)\n",
    "#         if os.path.exists(split_path):\n",
    "#             showers, incident_energy = load_showers_and_incident_energy(split_path)\n",
    "#             if showers is not None:\n",
    "#                 split_showers_list.append(showers)\n",
    "#                 split_incident_energy_list.append(incident_energy)\n",
    "#         else:\n",
    "#             print(f\"missing split {i} for {eta_tag}\")\n",
    "\n",
    "#     if not split_showers_list:\n",
    "#         print(f\"no split files with valid data for {eta_tag}\")\n",
    "#         continue\n",
    "\n",
    "#     split_showers = np.concatenate(split_showers_list)\n",
    "#     split_incident_energy = np.concatenate(split_incident_energy_list)\n",
    "\n",
    "#     # check sizes\n",
    "#     status = \" MATCH\" if len(split_incident_energy) == len(combined_incident_energy) else \" MISMATCH\"\n",
    "#     print(f\"\\n{eta_tag}:\")\n",
    "#     print(f\"Combined file: {len(combined_incident_energy)} events\")\n",
    "#     print(f\"Split total  : {len(split_incident_energy)} events\")\n",
    "#     print(f\"{status}\")\n",
    "\n",
    "#     # plotting here:\n",
    "#     # incident energy distribution \n",
    "#     plt.figure(figsize=(8, 4))\n",
    "#     plt.hist(split_incident_energy, bins=100, label=\"Split\", alpha=0.6, histtype='step')\n",
    "#     plt.hist(combined_incident_energy, bins=100, label=\"Combined\", alpha=0.6, histtype='step')\n",
    "#     plt.xlabel(\"Incident Energy (MeV)\")\n",
    "#     plt.ylabel(\"Event Count\")\n",
    "#     plt.title(f\"Incident Energy — {eta_tag}\")\n",
    "#     plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.xscale('log')\n",
    "#     plt.yscale('log')\n",
    "#     plt.show()\n",
    "    \n",
    "#     bins = np.logspace(np.log10(10), np.log10(40000), 100)\n",
    "#     #bins=100\n",
    "#     split_counts, _ = np.histogram(split_incident_energy, bins=bins)\n",
    "#     combined_counts, _ = np.histogram(combined_incident_energy, bins=bins)\n",
    "#     diff = combined_counts - split_counts\n",
    "\n",
    "#     plt.figure(figsize=(8, 4))\n",
    "#     plt.step(bins[:-1], diff, where='mid', color='red')\n",
    "#     plt.xlabel(\"Incident Energy (MeV)\")\n",
    "#     plt.ylabel(\"Delta Count (Combined − Split)\")\n",
    "#     plt.title(f\"Delta Histogram — Incident Energy — {eta_tag}\")\n",
    "#     plt.xscale('log')\n",
    "#     #plt.yscale('log')\n",
    "#     plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # deposited energy per event (shower sum)\n",
    "#     plt.figure(figsize=(8, 4))\n",
    "#     plt.hist(split_showers.sum(1), bins=100, label=\"Split\", alpha=0.6, histtype='step')\n",
    "#     plt.hist(combined_showers.sum(1), bins=100, label=\"Combined\", alpha=0.6, histtype='step')\n",
    "#     plt.xlabel(\"Total Deposited Energy (MeV)\")\n",
    "#     plt.ylabel(\"Event Count\")\n",
    "#     plt.title(f\"Total Shower Energy — {eta_tag}\")\n",
    "#     plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.yscale('log')\n",
    "#     plt.xscale('log')\n",
    "#     plt.show()\n",
    "    \n",
    "#     deposited_split = split_showers.sum(1)\n",
    "#     deposited_combined = combined_showers.sum(1)\n",
    "#     bins = np.logspace(np.log10(1), np.log10(50000), 100)\n",
    "#     #bins=100\n",
    "#     split_counts, _ = np.histogram(deposited_split, bins=bins)\n",
    "#     combined_counts, _ = np.histogram(deposited_combined, bins=bins)\n",
    "#     diff = combined_counts - split_counts\n",
    "\n",
    "#     plt.figure(figsize=(8, 4))\n",
    "#     plt.step(bins[:-1], diff, where='mid', color='blue')\n",
    "#     plt.xlabel(\"Deposited Energy (MeV)\")\n",
    "#     plt.ylabel(\"Delta Count (Combined − Split)\")\n",
    "#     plt.title(f\"Delta Histogram — Shower Energy — {eta_tag}\")\n",
    "#     plt.xscale('log')\n",
    "#     plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # ratio of incident to deposited energy\n",
    "#     split_ratios = split_incident_energy.flatten() / split_showers.sum(1)\n",
    "#     combined_ratios = combined_incident_energy.flatten() / combined_showers.sum(1)\n",
    "\n",
    "#     # filter inf/nan from zero-division\n",
    "#     split_ratios = split_ratios[np.isfinite(split_ratios)]\n",
    "#     combined_ratios = combined_ratios[np.isfinite(combined_ratios)]\n",
    "\n",
    "#     plt.figure(figsize=(8, 4))\n",
    "#     plt.hist(split_ratios, bins=100, label=\"Split\", alpha=0.6, histtype='step')\n",
    "#     plt.hist(combined_ratios, bins=100, label=\"Combined\", alpha=0.6, histtype='step')\n",
    "#     plt.xlabel(\"Incident / Deposited Energy Ratio\")\n",
    "#     plt.ylabel(\"Event Count\")\n",
    "#     plt.title(f\"Energy Ratio — {eta_tag}\")\n",
    "#     plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.yscale('log')\n",
    "#     plt.show()\n",
    "    \n",
    "#     bins = np.linspace(0, 3, 100)\n",
    "#     #bins=100\n",
    "#     split_counts, _ = np.histogram(split_ratios, bins=bins)\n",
    "#     combined_counts, _ = np.histogram(combined_ratios, bins=bins)\n",
    "#     diff = combined_counts - split_counts\n",
    "\n",
    "#     plt.figure(figsize=(8, 4))\n",
    "#     plt.step(bins[:-1], diff, where='mid', color='green')\n",
    "#     plt.xlabel(\"Incident / Deposited Energy Ratio\")\n",
    "#     plt.ylabel(\"Delta Count (Combined − Split)\")\n",
    "#     plt.title(f\"Delta Histogram — Energy Ratio — {eta_tag}\")\n",
    "#     plt.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "617d3df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting\n",
    "def overall_plots_v2(incident_combined, incident_rebuilt, showers_combined, showers_rebuilt, \n",
    "                     label_combined=\"Old Combined\", label_rebuilt=\"Rebuilt from 0–19\", \n",
    "                     incident_extra=None, showers_extra=None, label_extra=\"Extra\"):\n",
    "    \"\"\"\n",
    "    used to plot the deposited energy, sparsity, and ratio of deposited to incident energy\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(3, 2, figsize=(10, 15))\n",
    "    n_bins = 1000\n",
    "\n",
    "    def hist_and_diff(ax_row, values1, values2, title, xlabel, logy=True, range_=None):\n",
    "        hist1, bin_edges = np.histogram(values1, bins=n_bins, range=range_)\n",
    "        hist2, _ = np.histogram(values2, bins=bin_edges)\n",
    "        ax_row[0].stairs(hist1, bin_edges, label=label_combined, color='blue', fill=True, alpha=0.5)\n",
    "        ax_row[0].stairs(hist2, bin_edges, label=label_rebuilt, color='orange', fill=False, alpha=0.8)\n",
    "        if showers_extra is not None:\n",
    "            values3 = values_extra_func()\n",
    "            hist3, _ = np.histogram(values3, bins=bin_edges)\n",
    "            ax_row[0].stairs(hist3, bin_edges, label=label_extra, color='green', fill=False, alpha=0.8)\n",
    "            ax_row[1].scatter(bin_edges[:-1], hist1 - hist3, label=f\"{label_combined} - {label_extra}\", color='green')\n",
    "\n",
    "        ax_row[1].scatter(bin_edges[:-1], hist1 - hist2, label=f\"{label_combined} - {label_rebuilt}\", color='red')\n",
    "        ax_row[0].set_title(title)\n",
    "        ax_row[0].set_xlabel(xlabel)\n",
    "        ax_row[0].set_ylabel(\"Counts\")\n",
    "        if logy:\n",
    "            ax_row[0].set_yscale('log')\n",
    "        ax_row[0].legend()\n",
    "        ax_row[1].set_title(f\"Difference in {title}\")\n",
    "        ax_row[1].set_xlabel(xlabel)\n",
    "        ax_row[1].set_ylabel(\"Difference in Counts\")\n",
    "\n",
    "    # Total deposited energy\n",
    "    sum_combined = torch.sum(showers_combined, dim=1).numpy()\n",
    "    sum_rebuilt = torch.sum(showers_rebuilt, dim=1).numpy()\n",
    "    hist_and_diff(ax[0], sum_combined, sum_rebuilt, \"Total Deposited Energy\", \"Energy (MeV)\", range_=(0, np.max(sum_combined)))\n",
    "\n",
    "    # sparsity\n",
    "    spars_combined = (torch.sum(showers_combined == 0, dim=1) / showers_combined.shape[1]).numpy()\n",
    "    spars_rebuilt = (torch.sum(showers_rebuilt == 0, dim=1) / showers_rebuilt.shape[1]).numpy()\n",
    "    hist_and_diff(ax[1], spars_combined, spars_rebuilt, \"Sparsity of Showers\", \"Sparsity\", range_=(0, 1))\n",
    "\n",
    "    # ratio of deposited to incident energy\n",
    "    ratio_combined = sum_combined / incident_combined.view(-1).numpy()\n",
    "    ratio_rebuilt = sum_rebuilt / incident_rebuilt.view(-1).numpy()\n",
    "    hist_and_diff(ax[2], ratio_combined, ratio_rebuilt, \"Deposited / Incident Energy Ratio\", \"Ratio\", range_=(0, 3))\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def load_showers_and_incident_energy(path):\n",
    "    \"\"\"\n",
    "    loads a file and gets the incident energy and layers\n",
    "    \"\"\"\n",
    "    with h5py.File(path, 'r') as file:\n",
    "        data = {key: torch.tensor(file[key][:]) for key in file.keys()}\n",
    "        incident_energy = data[\"incident_energy\"]\n",
    "        valid_layers = []\n",
    "        for l in range(24):\n",
    "            key = f\"energy_layer_{l}\"\n",
    "            if key in data and (data[key].sum(dim=1) != 0).any():\n",
    "                valid_layers.append(l)\n",
    "        if not valid_layers:\n",
    "            return None, None\n",
    "        combined = torch.cat([data[f\"energy_layer_{l}\"] for l in valid_layers], dim=1)\n",
    "        showers = combined * incident_energy.unsqueeze(1)\n",
    "        return showers, incident_energy\n",
    "\n",
    "def load_direct_showers_and_incident_energy(path):\n",
    "    \"\"\"\n",
    "    loads file with different format\n",
    "    \"\"\"\n",
    "    with h5py.File(path, 'r') as f:\n",
    "        showers = torch.tensor(f[\"showers\"][:])\n",
    "        incident_energy = torch.tensor(f[\"incident_energies\"][:])\n",
    "    return showers, incident_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f0a90f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing one eta (035) to check\n",
    "eta_tag = \"eta_035\"\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31\"\n",
    "regular_dir = os.path.join(base_dir, eta_tag, f\"{eta_tag}_regular_binning\")\n",
    "\n",
    "file_name = \"dataset_combined.hdf5\"  #  OR dataset_combined_positive.hdf5, fine\n",
    "\n",
    "showers_list = []\n",
    "incident_list = []\n",
    "\n",
    "for i in range(20):\n",
    "    path = os.path.join(regular_dir, str(i), file_name)\n",
    "    if os.path.exists(path):\n",
    "        showers, incident = load_showers_and_incident_energy(path)\n",
    "        if showers is not None:\n",
    "            showers_list.append(showers)\n",
    "            incident_list.append(incident)\n",
    "    else:\n",
    "        print(f\"missing: {path}\")\n",
    "\n",
    "# grab showers and incident energies\n",
    "showers_rebuilt = torch.cat(showers_list, dim=0)\n",
    "incident_rebuilt = torch.cat(incident_list, dim=0)\n",
    "\n",
    "path_to_old_combined = \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_035/eta_035_regular_binning/dataset_combined.hdf5\"\n",
    "#showers_combined, incident_combined = load_direct_showers_and_incident_energy(path_to_old_combined)\n",
    "showers_combined, incident_combined = load_showers_and_incident_energy(path_to_old_combined)\n",
    "\n",
    "# overall_plots_v2(\n",
    "#     incident_combined=incident_combined,\n",
    "#     incident_rebuilt=incident_rebuilt,\n",
    "#     showers_combined=showers_combined,\n",
    "#     showers_rebuilt=showers_rebuilt,\n",
    "#     label_combined=\"New Combined\",\n",
    "#     label_rebuilt=\"Rebuilt from 0–19\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e8958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing combined vs 0-19 for new datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80f8e1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eta files to go through\n",
    "eta_tags = [\n",
    "    \"eta_000\", \"eta_005\", \"eta_010\", \"eta_015\", \"eta_020\", \"eta_025\", \"eta_030\", \"eta_035\", \"eta_040\",\n",
    "    \"eta_045\", \"eta_050\", \"eta_055\", \"eta_060\", \"eta_065\", \"eta_070\", \"eta_075\", \"eta_080\", \"eta_085\",\n",
    "    \"eta_090\", \"eta_095\", \"eta_100\", \"eta_105\", \"eta_110\", \"eta_115\", \"eta_120\", \"eta_125\", \"eta_130\",\n",
    "]\n",
    "\n",
    "base_dir = \"/fast_scratch_1/caloqvae/data/atlas_july31\"\n",
    "file_name = \"dataset_combined.hdf5\"  # or dataset_combined_positive.hdf5\n",
    "\n",
    "##################################################\n",
    "# uncomment this to see output:\n",
    "\n",
    "\n",
    "# for eta_tag in eta_tags:\n",
    "#     print(f\"\\nprocessing {eta_tag}\")\n",
    "#     regular_dir = os.path.join(base_dir, eta_tag, f\"{eta_tag}_regular_binning\")\n",
    "    \n",
    "#     showers_list, incident_list = [], []\n",
    "\n",
    "#     for i in range(20):\n",
    "#         path = os.path.join(regular_dir, str(i), file_name)\n",
    "#         if os.path.exists(path):\n",
    "#             showers, incident = load_showers_and_incident_energy(path)\n",
    "#             if showers is not None:\n",
    "#                 showers_list.append(showers)\n",
    "#                 incident_list.append(incident)\n",
    "#         else:\n",
    "#             print(f\"missing: {path}\")\n",
    "\n",
    "#     if not showers_list:\n",
    "#         print(f\"skipping {eta_tag}\")\n",
    "#         continue\n",
    "\n",
    "#     showers_rebuilt = torch.cat(showers_list, dim=0)\n",
    "#     incident_rebuilt = torch.cat(incident_list, dim=0)\n",
    "\n",
    "#     path_to_combined = os.path.join(regular_dir, file_name)\n",
    "#     if not os.path.exists(path_to_combined):\n",
    "#         print(f\"combined file missing: {path_to_combined}\")\n",
    "#         continue\n",
    "\n",
    "#     showers_combined, incident_combined = load_showers_and_incident_energy(path_to_combined)\n",
    "\n",
    "#     overall_plots_v2(\n",
    "#         incident_combined=incident_combined,\n",
    "#         incident_rebuilt=incident_rebuilt,\n",
    "#         showers_combined=showers_combined,\n",
    "#         showers_rebuilt=showers_rebuilt,\n",
    "#         label_combined=f\"{eta_tag} Combined\",\n",
    "#         label_rebuilt=f\"{eta_tag} Rebuilt\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "383f1ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_active_layers_and_shapes(path):\n",
    "    \"\"\"\n",
    "    function to get the active layers in a file and look at key shapes\n",
    "    \"\"\"\n",
    "    with h5py.File(path, 'r') as file:\n",
    "        active_layers = []\n",
    "        layer_shapes = {}\n",
    "        total_voxels = 0\n",
    "        for l in range(24):\n",
    "            key = f\"energy_layer_{l}\"\n",
    "            if key in file:\n",
    "                data = torch.tensor(file[key][:])\n",
    "                if (data.sum(dim=1) != 0).any():\n",
    "                    active_layers.append(l)\n",
    "                    layer_shapes[l] = data.shape\n",
    "                    total_voxels += data.shape[1]\n",
    "        return active_layers, layer_shapes, total_voxels\n",
    "\n",
    "# eta to inspect (here choosing 010)\n",
    "eta_tag = \"eta_010\"\n",
    "base_path = \"/fast_scratch_1/caloqvae/data/atlas_july31\"\n",
    "file_name = \"dataset_combined.hdf5\"\n",
    "regular_dir = os.path.join(base_path, eta_tag, f\"{eta_tag}_regular_binning\")\n",
    "combined_path = os.path.join(regular_dir, file_name)\n",
    "\n",
    "############################################\n",
    "# uncomment this to see output:\n",
    "\n",
    "# print(f\"checking combined file: {combined_path}\")\n",
    "# combined_layers, combined_shapes, combined_total_voxels = get_active_layers_and_shapes(combined_path)\n",
    "# print(f\"active layers in combined file: {combined_layers}\")\n",
    "# print(f\"layer shapes:\")\n",
    "# for l in combined_layers:\n",
    "#     print(f\"Layer {l:2d}: {combined_shapes[l]}\")\n",
    "# print(f\"total voxels in combined file: {combined_total_voxels}\")\n",
    "\n",
    "# # now loop through split files\n",
    "# split_layers_total = set()\n",
    "# split_shapes_total = {}\n",
    "# split_total_voxels = 0\n",
    "\n",
    "# for i in range(20):\n",
    "#     split_path = os.path.join(regular_dir, str(i), file_name)\n",
    "#     if not os.path.exists(split_path):\n",
    "#         continue\n",
    "#     layers, shapes, voxels = get_active_layers_and_shapes(split_path)\n",
    "#     split_layers_total.update(layers)\n",
    "#     for l in layers:\n",
    "#         split_shapes_total[l] = shapes[l]  # look at las tshape\n",
    "#     split_total_voxels += sum(shapes[l][1] for l in layers)\n",
    "\n",
    "# print(f\"\\nChecking split files 0–19\")\n",
    "# print(f\"union of active layers across split files: {sorted(split_layers_total)}\")\n",
    "# print(f\"layer shape:\")\n",
    "# for l in sorted(split_layers_total):\n",
    "#     print(f\"Layer {l:02d}: {split_shapes_total[l]}\")\n",
    "# print(f\"Total voxels summed over all split layers: {split_total_voxels}\")\n",
    "\n",
    "# print(\"\\nSummary Comparison\")\n",
    "# print(f\"number of active layers — combined: {len(combined_layers)}, split: {len(split_layers_total)}\")\n",
    "# print(f\"total voxels — combined: {combined_total_voxels}, split: {split_total_voxels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17224db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file paths:\n",
    "base_dir = '/fast_scratch_1/caloqvae/data/atlas_july31'\n",
    "file_name = \"dataset_combined.hdf5\"\n",
    "\n",
    "##################################################3\n",
    "# uncomment this to see the output:\n",
    "\n",
    "# for eta in range(0, 135 + 1, 5):\n",
    "#     eta_tag = f\"eta_{eta:03d}\"\n",
    "#     print(f\"\\nprocessing {eta_tag}\")\n",
    "#     regular_dir = os.path.join(base_dir, eta_tag, f\"{eta_tag}_regular_binning\")\n",
    "#     combined_path = os.path.join(regular_dir, file_name)\n",
    "\n",
    "#     if not os.path.exists(combined_path):\n",
    "#         print(f\"missing combined file: {combined_path}\")\n",
    "#         continue\n",
    "\n",
    "#     # load combined dataset\n",
    "#     with h5py.File(combined_path, 'r') as file_combined:\n",
    "#         f_combined = {key: torch.tensor(file_combined[key][:]) for key in file_combined.keys()}\n",
    "\n",
    "#     # load and concatenate all split files (0–19)\n",
    "#     split_layers_dict = {f\"energy_layer_{l}\": [] for l in range(24)}\n",
    "#     for i in range(20):\n",
    "#         split_path = os.path.join(regular_dir, str(i), file_name)\n",
    "#         if not os.path.exists(split_path):\n",
    "#             continue\n",
    "#         with h5py.File(split_path, 'r') as file_split:\n",
    "#             for key in file_split.keys():\n",
    "#                 if key.startswith(\"energy_layer_\"):\n",
    "#                     split_layers_dict[key].append(torch.tensor(file_split[key][:]))\n",
    "\n",
    "#     # combine split layers across bins\n",
    "#     f_split_combined = {}\n",
    "#     for key, tensors in split_layers_dict.items():\n",
    "#         if tensors:  # if data\n",
    "#             f_split_combined[key] = torch.cat(tensors, dim=0)\n",
    "\n",
    "#     # active layers\n",
    "#     valid_layers_combined = []\n",
    "#     valid_layers_split = []\n",
    "#     for l in range(24):\n",
    "#         key = f\"energy_layer_{l}\"\n",
    "#         if key in f_combined and (f_combined[key].sum(dim=1) != 0).any():\n",
    "#             valid_layers_combined.append(l)\n",
    "#         if key in f_split_combined and (f_split_combined[key].sum(dim=1) != 0).any():\n",
    "#             valid_layers_split.append(l)\n",
    "\n",
    "#     print(f\"active layers in combined : {valid_layers_combined}\")\n",
    "#     print(f\"active layers in split : {valid_layers_split}\")\n",
    "\n",
    "#     # per-layer comparison\n",
    "#     num_events_combined = []\n",
    "#     num_events_split = []\n",
    "#     for layer in sorted(set(valid_layers_combined) | set(valid_layers_split)):\n",
    "#         key = f'energy_layer_{layer}'\n",
    "#         if key in f_combined and key in f_split_combined:\n",
    "#             shape_combined = f_combined[key].shape\n",
    "#             shape_split = f_split_combined[key].shape\n",
    "#             print(f\"layer {layer:02d} shape — combined: {shape_combined}, split: {shape_split}\")\n",
    "\n",
    "#             nz_combined = (f_combined[key].sum(dim=1) != 0).sum().item()\n",
    "#             nz_split = (f_split_combined[key].sum(dim=1) != 0).sum().item()\n",
    "#             print(f\"non-zero events — combined: {nz_combined}, split: {nz_split}\")\n",
    "\n",
    "#             num_events_combined.append(nz_combined)\n",
    "#             num_events_split.append(nz_split)\n",
    "#         else:\n",
    "#             print(f\"Layer {layer:02d} missing in one of the datasets.\")\n",
    "\n",
    "#     # plot non-zero event counts per layer\n",
    "#     layers_to_plot = sorted(set(valid_layers_combined) | set(valid_layers_split))\n",
    "#     plt.figure(figsize=(10, 6))\n",
    "#     plt.bar(np.array(layers_to_plot) - 0.15, num_events_combined, width=0.3, label='Combined File')\n",
    "#     plt.bar(np.array(layers_to_plot) + 0.15, num_events_split, width=0.3, label='Concat Split Files')\n",
    "#     plt.xlabel('Layer')\n",
    "#     plt.ylabel('Number of Non-Zero Events')\n",
    "#     plt.title(f'Non-Zero Events per Layer for {eta_tag}')\n",
    "#     plt.yscale('log')\n",
    "#     plt.xticks(layers_to_plot)\n",
    "#     plt.legend()\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "41110bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking how many voxels are hit and looking at events for each layer:\n",
    "# file paths:\n",
    "data_paths = {\n",
    "    \"combined\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/dataset_combined.hdf5\",\n",
    "    \"positive\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/dataset_combined_positive.hdf5\",\n",
    "    \"fine\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/dataset_combined_fine.hdf5\",\n",
    "}\n",
    "\n",
    "# uncomment this to see output:\n",
    "\n",
    "# for label, path in data_paths.items():\n",
    "#     print(f\"\\n File: {label} ({path})\")\n",
    "#     with h5py.File(path, 'r') as f:\n",
    "#         total_events = f[\"incident_energy\"].shape[0]\n",
    "#         for layer in range(24):\n",
    "#             energy_key = f\"energy_layer_{layer}\"\n",
    "#             if energy_key in f:\n",
    "#                 energy = f[energy_key][:]  # shape: (n_events, n_voxels)\n",
    "#                 hit_mask = energy > 0\n",
    "#                 n_hits = np.count_nonzero(hit_mask)\n",
    "#                 active_voxels = np.any(hit_mask, axis=0).sum()\n",
    "#                 active_events = np.any(hit_mask, axis=1).sum()\n",
    "#                 print(f\"layer {layer:02d}: {n_hits} total hits, \"\n",
    "#                       f\"{active_voxels} voxels hit at least once, \"\n",
    "#                       f\"{active_events} events with energy > 0\")\n",
    "#             else:\n",
    "#                 print(f\"Layer {layer:02d}: missing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bfe0add8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at incident energy distributions:\n",
    "# file paths\n",
    "data_paths = {\n",
    "    \"combined\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/dataset_combined.hdf5\",\n",
    "    \"positive\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/dataset_combined_positive.hdf5\",\n",
    "}\n",
    "\n",
    "############################################\n",
    "# uncomment to show the output:\n",
    "\n",
    "# # process the files\n",
    "# for label, path in data_paths.items():\n",
    "#     with h5py.File(path, 'r') as f:\n",
    "#         incident_energies = np.array(f[\"incident_energy\"])\n",
    "#     incident_energies_rounded = np.round(incident_energies)\n",
    "#     unique_energies, counts = np.unique(incident_energies_rounded, return_counts=True) # count unique energies\n",
    "\n",
    "#     # plot histogram\n",
    "#     plt.figure(figsize=(8, 4))\n",
    "#     plt.hist(incident_energies_rounded, bins=np.logspace(np.log10(256), np.log10(1e5), 50), alpha=0.6, label=label, histtype='step')\n",
    "#     plt.xscale('log')\n",
    "#     #plt.yscale('log')\n",
    "#     plt.xlabel(\"Incident Energy (MeV)\")\n",
    "#     plt.ylabel(\"Number of Events\")\n",
    "#     plt.title(f\"Incident Energy Distribution — {label}\")\n",
    "#     plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "#     plt.tight_layout()\n",
    "#     plt.legend()\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3601b82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " COMBINED — using layers: [0, 1, 2, 3, 12]\n",
      "\n",
      " POSITIVE — using layers: [0, 1, 2, 3, 12]\n"
     ]
    }
   ],
   "source": [
    "# investigating a specific file:\n",
    "data_paths = {\n",
    "    \"combined\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/dataset_combined.hdf5\",\n",
    "    \"positive\": \"/fast_scratch_1/caloqvae/data/atlas_july31/eta_000/eta_000_regular_binning/dataset_combined_positive.hdf5\",\n",
    "}\n",
    "\n",
    "def process_file(path, label):\n",
    "    \"\"\"\n",
    "    gets incident energy and the showers from a file\n",
    "    \"\"\"\n",
    "    with h5py.File(path, 'r') as file:\n",
    "        f = {key: torch.tensor(np.array(file[key])) for key in file.keys()}\n",
    "\n",
    "        valid_layers = []\n",
    "        for layer in range(24):\n",
    "            key = f'energy_layer_{layer}'\n",
    "            if key in f and (f[key].sum(dim=1) != 0).any():\n",
    "                valid_layers.append(layer)\n",
    "\n",
    "        print(f\"\\n{label.upper()} — using layers: {valid_layers}\")\n",
    "\n",
    "        en_sizes = np.concatenate([[0], np.array([\n",
    "            torch.where(f[\"incident_energy\"].log2() == i, 1, 0).sum()\n",
    "            for i in range(8, 23)\n",
    "        ])])\n",
    "        idx = list(np.concatenate([\n",
    "            [j + en_sizes[:i].sum() for j in range(int(np.floor(0.8 * en_sizes[i])))]\n",
    "            for i in range(1, len(en_sizes))\n",
    "        ]))\n",
    "        idxTest = list(set(range(f['incident_energy'].shape[0])) - set(idx))\n",
    "        idxSort = idx + idxTest\n",
    "\n",
    "        # now get showers and incident energies:\n",
    "        combined = torch.cat([f[f'energy_layer_{l}'] for l in valid_layers], dim=1)\n",
    "        showers = (combined * f[\"incident_energy\"].unsqueeze(1))[idxSort, :]\n",
    "        incident_energies = f[\"incident_energy\"].unsqueeze(1)[idxSort, :]\n",
    "\n",
    "        return showers.numpy(), incident_energies.numpy()\n",
    "\n",
    "# store results\n",
    "results = {}\n",
    "for label, path in data_paths.items():\n",
    "    showers, incident_energies = process_file(path, label)\n",
    "    results[label] = {\"showers\": showers, \"incident_energies\": incident_energies}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf42606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now plotting results:\n",
    "# uncomment this to show the output:\n",
    "\n",
    "# # plot total deposited energy (log)\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# for label in results:\n",
    "#     plt.hist(results[label][\"showers\"].sum(1),\n",
    "#              bins=100, alpha=0.6, label=label, histtype='step')\n",
    "# plt.yscale('log')\n",
    "# plt.xlabel(\"Total Deposited Energy (MeV)\")\n",
    "# plt.ylabel(\"Events (log scale)\")\n",
    "# plt.title(\"Total Energy Deposited per Event\")\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # plot incident energy distributions\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# for label in results:\n",
    "#     plt.hist(results[label][\"incident_energies\"].flatten(),\n",
    "#              bins=100, alpha=0.6, label=label, histtype='step')\n",
    "# plt.yscale('log')\n",
    "# plt.xscale('log')\n",
    "# plt.xlabel(\"Incident Energy (MeV)\")\n",
    "# plt.ylabel(\"Events\")\n",
    "# plt.title(\"Incident Energy Distribution\")\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # plot deposited / incident\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# for label in results:\n",
    "#     efficiency = results[label][\"showers\"].sum(1) / results[label][\"incident_energies\"].flatten()\n",
    "#     plt.hist(efficiency, bins=100, alpha=0.6, label=label, histtype='step')\n",
    "# plt.xlabel(\"Deposited / Incident Energy\")\n",
    "# plt.ylabel(\"Events\")\n",
    "# plt.title(\"Ratio\")\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
