{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83206f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[02:28:07.561]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mCaloQuVAE                                         \u001b[0mLoading configuration.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "from hydra.utils import instantiate\n",
    "from hydra import initialize, compose\n",
    "import hydra\n",
    "import wandb\n",
    "from omegaconf import OmegaConf\n",
    "import copy\n",
    "\n",
    "from model.rbm.rbm import RBM\n",
    "from model.rbm.rbm_torch import RBMtorch\n",
    "from model.rbm.rbm_fulltorch import RBMTorchFull\n",
    "from scripts.run import setup_model, load_model_instance\n",
    "\n",
    "import dwave_networkx as dnx\n",
    "from dwave.system import DWaveSampler\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3254aaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/1878xxru?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f296bb165a0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=\"config\")\n",
    "config=compose(config_name=\"config.yaml\")\n",
    "wandb.init(tags = [config.data.dataset_name], project=config.wandb.project, entity=config.wandb.entity, config=OmegaConf.to_container(config, resolve=True), mode='disabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb1ec89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[02:33:34.770]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mFetching definitions of all available solvers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[02:33:34.817]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mReceived solver data for 7 solver(s).\n",
      "\u001b[1m[02:33:34.869]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(id='Advantage_system4.1')\n",
      "\u001b[1m[02:33:34.919]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(id='Advantage_system6.4')\n",
      "\u001b[1m[02:33:34.971]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(id='Advantage2_system1.6')\n",
      "\u001b[1m[02:33:35.209]\u001b[0m \u001b[1;93mWARN \u001b[1;0m  \u001b[1mmodel.rbm.zephyr                                  \u001b[0mCould not use chip layout for qubit selection. Falling back to index-based selection. Error: 'RBM' object has no attribute 'nodelist'\n",
      "\u001b[1m[02:33:36.009]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mFetching definitions of all available solvers\n",
      "\u001b[1m[02:33:36.046]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mReceived solver data for 7 solver(s).\n",
      "\u001b[1m[02:33:36.448]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(id='Advantage_system4.1')\n",
      "\u001b[1m[02:33:36.500]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(id='Advantage_system6.4')\n",
      "\u001b[1m[02:33:36.571]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(id='Advantage2_system1.6')\n",
      "\u001b[1m[02:33:36.811]\u001b[0m \u001b[1;93mWARN \u001b[1;0m  \u001b[1mmodel.rbm.zephyr                                  \u001b[0mCould not use chip layout for qubit selection. Falling back to index-based selection. Error: 'RBMTorchFull' object has no attribute 'nodelist'\n",
      "\u001b[1m[02:33:37.559]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mmodel.rbm.rbm_fulltorch                           \u001b[0mRBMTorchFull initialized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, 01, max diff: 1.1920928955078125e-07\n",
      "Step 0, 02, max diff: 1.1920928955078125e-07\n",
      "Step 0, 03, max diff: 1.1920928955078125e-07\n",
      "Step 0, 12, max diff: 1.1920928955078125e-07\n",
      "Step 0, 13, max diff: 1.1920928955078125e-07\n",
      "Step 0, 23, max diff: 1.1920928955078125e-07\n",
      "Step 1, 01, max diff: 1.1920928955078125e-07\n",
      "Step 1, 02, max diff: 1.1920928955078125e-07\n",
      "Step 1, 03, max diff: 1.1920928955078125e-07\n",
      "Step 1, 12, max diff: 2.384185791015625e-07\n",
      "Step 1, 13, max diff: 2.384185791015625e-07\n",
      "Step 1, 23, max diff: 1.1920928955078125e-07\n",
      "Step 2, 01, max diff: 1.7881393432617188e-07\n",
      "Step 2, 02, max diff: 1.1920928955078125e-07\n",
      "Step 2, 03, max diff: 1.7881393432617188e-07\n",
      "Step 2, 12, max diff: 2.980232238769531e-07\n",
      "Step 2, 13, max diff: 2.384185791015625e-07\n",
      "Step 2, 23, max diff: 2.384185791015625e-07\n",
      "Step 3, 01, max diff: 1.7881393432617188e-07\n",
      "Step 3, 02, max diff: 1.7881393432617188e-07\n",
      "Step 3, 03, max diff: 1.7881393432617188e-07\n",
      "Step 3, 12, max diff: 2.980232238769531e-07\n",
      "Step 3, 13, max diff: 2.384185791015625e-07\n",
      "Step 3, 23, max diff: 2.384185791015625e-07\n",
      "Step 4, 01, max diff: 1.7881393432617188e-07\n",
      "Step 4, 02, max diff: 1.7881393432617188e-07\n",
      "Step 4, 03, max diff: 2.384185791015625e-07\n",
      "Step 4, 12, max diff: 2.980232238769531e-07\n",
      "Step 4, 13, max diff: 2.384185791015625e-07\n",
      "Step 4, 23, max diff: 2.384185791015625e-07\n",
      "Step 5, 01, max diff: 2.384185791015625e-07\n",
      "Step 5, 02, max diff: 2.384185791015625e-07\n",
      "Step 5, 03, max diff: 2.384185791015625e-07\n",
      "Step 5, 12, max diff: 2.980232238769531e-07\n",
      "Step 5, 13, max diff: 2.384185791015625e-07\n",
      "Step 5, 23, max diff: 2.384185791015625e-07\n",
      "Step 6, 01, max diff: 2.980232238769531e-07\n",
      "Step 6, 02, max diff: 2.980232238769531e-07\n",
      "Step 6, 03, max diff: 2.384185791015625e-07\n",
      "Step 6, 12, max diff: 3.5762786865234375e-07\n",
      "Step 6, 13, max diff: 2.384185791015625e-07\n",
      "Step 6, 23, max diff: 2.384185791015625e-07\n",
      "Step 7, 01, max diff: 2.980232238769531e-07\n",
      "Step 7, 02, max diff: 2.980232238769531e-07\n",
      "Step 7, 03, max diff: 2.384185791015625e-07\n",
      "Step 7, 12, max diff: 3.5762786865234375e-07\n",
      "Step 7, 13, max diff: 2.384185791015625e-07\n",
      "Step 7, 23, max diff: 2.384185791015625e-07\n",
      "Step 8, 01, max diff: 3.5762786865234375e-07\n",
      "Step 8, 02, max diff: 2.980232238769531e-07\n",
      "Step 8, 03, max diff: 2.980232238769531e-07\n",
      "Step 8, 12, max diff: 3.5762786865234375e-07\n",
      "Step 8, 13, max diff: 2.980232238769531e-07\n",
      "Step 8, 23, max diff: 2.980232238769531e-07\n",
      "Step 9, 01, max diff: 3.5762786865234375e-07\n",
      "Step 9, 02, max diff: 3.5762786865234375e-07\n",
      "Step 9, 03, max diff: 2.980232238769531e-07\n",
      "Step 9, 12, max diff: 3.5762786865234375e-07\n",
      "Step 9, 13, max diff: 2.980232238769531e-07\n",
      "Step 9, 23, max diff: 2.980232238769531e-07\n",
      "Step 10, 01, max diff: 3.5762786865234375e-07\n",
      "Step 10, 02, max diff: 3.5762786865234375e-07\n",
      "Step 10, 03, max diff: 2.980232238769531e-07\n",
      "Step 10, 12, max diff: 3.5762786865234375e-07\n",
      "Step 10, 13, max diff: 3.5762786865234375e-07\n",
      "Step 10, 23, max diff: 2.980232238769531e-07\n",
      "Step 11, 01, max diff: 3.5762786865234375e-07\n",
      "Step 11, 02, max diff: 3.5762786865234375e-07\n",
      "Step 11, 03, max diff: 2.980232238769531e-07\n",
      "Step 11, 12, max diff: 3.5762786865234375e-07\n",
      "Step 11, 13, max diff: 3.5762786865234375e-07\n",
      "Step 11, 23, max diff: 2.980232238769531e-07\n",
      "Step 12, 01, max diff: 3.5762786865234375e-07\n",
      "Step 12, 02, max diff: 3.5762786865234375e-07\n",
      "Step 12, 03, max diff: 2.980232238769531e-07\n",
      "Step 12, 12, max diff: 3.5762786865234375e-07\n",
      "Step 12, 13, max diff: 4.172325134277344e-07\n",
      "Step 12, 23, max diff: 2.980232238769531e-07\n",
      "Step 13, 01, max diff: 3.5762786865234375e-07\n",
      "Step 13, 02, max diff: 3.5762786865234375e-07\n",
      "Step 13, 03, max diff: 2.980232238769531e-07\n",
      "Step 13, 12, max diff: 3.5762786865234375e-07\n",
      "Step 13, 13, max diff: 4.76837158203125e-07\n",
      "Step 13, 23, max diff: 3.5762786865234375e-07\n",
      "Step 14, 01, max diff: 3.5762786865234375e-07\n",
      "Step 14, 02, max diff: 3.5762786865234375e-07\n",
      "Step 14, 03, max diff: 3.5762786865234375e-07\n",
      "Step 14, 12, max diff: 3.5762786865234375e-07\n",
      "Step 14, 13, max diff: 4.76837158203125e-07\n",
      "Step 14, 23, max diff: 3.5762786865234375e-07\n",
      "Step 15, 01, max diff: 3.5762786865234375e-07\n",
      "Step 15, 02, max diff: 3.5762786865234375e-07\n",
      "Step 15, 03, max diff: 3.5762786865234375e-07\n",
      "Step 15, 12, max diff: 3.5762786865234375e-07\n",
      "Step 15, 13, max diff: 5.364418029785156e-07\n",
      "Step 15, 23, max diff: 3.5762786865234375e-07\n",
      "Step 16, 01, max diff: 3.5762786865234375e-07\n",
      "Step 16, 02, max diff: 3.5762786865234375e-07\n",
      "Step 16, 03, max diff: 3.5762786865234375e-07\n",
      "Step 16, 12, max diff: 3.5762786865234375e-07\n",
      "Step 16, 13, max diff: 5.364418029785156e-07\n",
      "Step 16, 23, max diff: 3.5762786865234375e-07\n",
      "Step 17, 01, max diff: 3.5762786865234375e-07\n",
      "Step 17, 02, max diff: 3.5762786865234375e-07\n",
      "Step 17, 03, max diff: 3.5762786865234375e-07\n",
      "Step 17, 12, max diff: 3.5762786865234375e-07\n",
      "Step 17, 13, max diff: 5.364418029785156e-07\n",
      "Step 17, 23, max diff: 3.5762786865234375e-07\n",
      "Step 18, 01, max diff: 3.5762786865234375e-07\n",
      "Step 18, 02, max diff: 3.5762786865234375e-07\n",
      "Step 18, 03, max diff: 3.5762786865234375e-07\n",
      "Step 18, 12, max diff: 3.5762786865234375e-07\n",
      "Step 18, 13, max diff: 5.364418029785156e-07\n",
      "Step 18, 23, max diff: 3.5762786865234375e-07\n",
      "Step 19, 01, max diff: 3.5762786865234375e-07\n",
      "Step 19, 02, max diff: 3.5762786865234375e-07\n",
      "Step 19, 03, max diff: 3.5762786865234375e-07\n",
      "Step 19, 12, max diff: 3.5762786865234375e-07\n",
      "Step 19, 13, max diff: 5.364418029785156e-07\n",
      "Step 19, 23, max diff: 3.5762786865234375e-07\n",
      "Step 20, 01, max diff: 3.5762786865234375e-07\n",
      "Step 20, 02, max diff: 3.5762786865234375e-07\n",
      "Step 20, 03, max diff: 3.5762786865234375e-07\n",
      "Step 20, 12, max diff: 3.5762786865234375e-07\n",
      "Step 20, 13, max diff: 5.364418029785156e-07\n",
      "Step 20, 23, max diff: 3.5762786865234375e-07\n",
      "Step 21, 01, max diff: 3.5762786865234375e-07\n",
      "Step 21, 02, max diff: 3.5762786865234375e-07\n",
      "Step 21, 03, max diff: 3.5762786865234375e-07\n",
      "Step 21, 12, max diff: 3.5762786865234375e-07\n",
      "Step 21, 13, max diff: 4.76837158203125e-07\n",
      "Step 21, 23, max diff: 3.5762786865234375e-07\n",
      "Step 22, 01, max diff: 3.5762786865234375e-07\n",
      "Step 22, 02, max diff: 3.5762786865234375e-07\n",
      "Step 22, 03, max diff: 3.5762786865234375e-07\n",
      "Step 22, 12, max diff: 4.172325134277344e-07\n",
      "Step 22, 13, max diff: 4.76837158203125e-07\n",
      "Step 22, 23, max diff: 3.5762786865234375e-07\n",
      "Step 23, 01, max diff: 4.172325134277344e-07\n",
      "Step 23, 02, max diff: 3.5762786865234375e-07\n",
      "Step 23, 03, max diff: 3.5762786865234375e-07\n",
      "Step 23, 12, max diff: 4.172325134277344e-07\n",
      "Step 23, 13, max diff: 4.76837158203125e-07\n",
      "Step 23, 23, max diff: 3.5762786865234375e-07\n",
      "Step 24, 01, max diff: 4.172325134277344e-07\n",
      "Step 24, 02, max diff: 3.5762786865234375e-07\n",
      "Step 24, 03, max diff: 3.5762786865234375e-07\n",
      "Step 24, 12, max diff: 4.172325134277344e-07\n",
      "Step 24, 13, max diff: 4.76837158203125e-07\n",
      "Step 24, 23, max diff: 3.5762786865234375e-07\n",
      "Step 25, 01, max diff: 4.172325134277344e-07\n",
      "Step 25, 02, max diff: 3.5762786865234375e-07\n",
      "Step 25, 03, max diff: 3.5762786865234375e-07\n",
      "Step 25, 12, max diff: 4.172325134277344e-07\n",
      "Step 25, 13, max diff: 4.76837158203125e-07\n",
      "Step 25, 23, max diff: 3.5762786865234375e-07\n",
      "Step 26, 01, max diff: 4.172325134277344e-07\n",
      "Step 26, 02, max diff: 4.172325134277344e-07\n",
      "Step 26, 03, max diff: 3.5762786865234375e-07\n",
      "Step 26, 12, max diff: 4.172325134277344e-07\n",
      "Step 26, 13, max diff: 4.76837158203125e-07\n",
      "Step 26, 23, max diff: 4.172325134277344e-07\n",
      "Step 27, 01, max diff: 4.172325134277344e-07\n",
      "Step 27, 02, max diff: 4.172325134277344e-07\n",
      "Step 27, 03, max diff: 4.172325134277344e-07\n",
      "Step 27, 12, max diff: 4.172325134277344e-07\n",
      "Step 27, 13, max diff: 4.76837158203125e-07\n",
      "Step 27, 23, max diff: 4.172325134277344e-07\n",
      "Step 28, 01, max diff: 4.172325134277344e-07\n",
      "Step 28, 02, max diff: 4.172325134277344e-07\n",
      "Step 28, 03, max diff: 4.172325134277344e-07\n",
      "Step 28, 12, max diff: 4.172325134277344e-07\n",
      "Step 28, 13, max diff: 4.76837158203125e-07\n",
      "Step 28, 23, max diff: 4.172325134277344e-07\n",
      "Step 29, 01, max diff: 4.172325134277344e-07\n",
      "Step 29, 02, max diff: 4.172325134277344e-07\n",
      "Step 29, 03, max diff: 4.172325134277344e-07\n",
      "Step 29, 12, max diff: 4.172325134277344e-07\n",
      "Step 29, 13, max diff: 4.76837158203125e-07\n",
      "Step 29, 23, max diff: 4.172325134277344e-07\n",
      "Step 30, 01, max diff: 4.172325134277344e-07\n",
      "Step 30, 02, max diff: 4.172325134277344e-07\n",
      "Step 30, 03, max diff: 4.172325134277344e-07\n",
      "Step 30, 12, max diff: 4.172325134277344e-07\n",
      "Step 30, 13, max diff: 5.364418029785156e-07\n",
      "Step 30, 23, max diff: 4.172325134277344e-07\n",
      "Step 31, 01, max diff: 4.172325134277344e-07\n",
      "Step 31, 02, max diff: 4.172325134277344e-07\n",
      "Step 31, 03, max diff: 4.172325134277344e-07\n",
      "Step 31, 12, max diff: 4.172325134277344e-07\n",
      "Step 31, 13, max diff: 5.364418029785156e-07\n",
      "Step 31, 23, max diff: 4.172325134277344e-07\n",
      "Step 32, 01, max diff: 4.172325134277344e-07\n",
      "Step 32, 02, max diff: 4.172325134277344e-07\n",
      "Step 32, 03, max diff: 4.172325134277344e-07\n",
      "Step 32, 12, max diff: 4.76837158203125e-07\n",
      "Step 32, 13, max diff: 5.364418029785156e-07\n",
      "Step 32, 23, max diff: 4.172325134277344e-07\n",
      "Step 33, 01, max diff: 4.172325134277344e-07\n",
      "Step 33, 02, max diff: 4.172325134277344e-07\n",
      "Step 33, 03, max diff: 4.172325134277344e-07\n",
      "Step 33, 12, max diff: 4.76837158203125e-07\n",
      "Step 33, 13, max diff: 5.364418029785156e-07\n",
      "Step 33, 23, max diff: 4.172325134277344e-07\n",
      "Step 34, 01, max diff: 4.172325134277344e-07\n",
      "Step 34, 02, max diff: 4.172325134277344e-07\n",
      "Step 34, 03, max diff: 4.172325134277344e-07\n",
      "Step 34, 12, max diff: 4.76837158203125e-07\n",
      "Step 34, 13, max diff: 5.364418029785156e-07\n",
      "Step 34, 23, max diff: 4.172325134277344e-07\n",
      "Step 35, 01, max diff: 4.172325134277344e-07\n",
      "Step 35, 02, max diff: 4.76837158203125e-07\n",
      "Step 35, 03, max diff: 4.172325134277344e-07\n",
      "Step 35, 12, max diff: 4.76837158203125e-07\n",
      "Step 35, 13, max diff: 5.960464477539062e-07\n",
      "Step 35, 23, max diff: 4.172325134277344e-07\n",
      "Step 36, 01, max diff: 4.172325134277344e-07\n",
      "Step 36, 02, max diff: 4.76837158203125e-07\n",
      "Step 36, 03, max diff: 4.76837158203125e-07\n",
      "Step 36, 12, max diff: 4.76837158203125e-07\n",
      "Step 36, 13, max diff: 5.960464477539062e-07\n",
      "Step 36, 23, max diff: 4.172325134277344e-07\n",
      "Step 37, 01, max diff: 4.172325134277344e-07\n",
      "Step 37, 02, max diff: 4.76837158203125e-07\n",
      "Step 37, 03, max diff: 4.76837158203125e-07\n",
      "Step 37, 12, max diff: 4.76837158203125e-07\n",
      "Step 37, 13, max diff: 5.960464477539062e-07\n",
      "Step 37, 23, max diff: 4.172325134277344e-07\n",
      "Step 38, 01, max diff: 4.172325134277344e-07\n",
      "Step 38, 02, max diff: 4.76837158203125e-07\n",
      "Step 38, 03, max diff: 4.76837158203125e-07\n",
      "Step 38, 12, max diff: 4.76837158203125e-07\n",
      "Step 38, 13, max diff: 5.960464477539062e-07\n",
      "Step 38, 23, max diff: 4.76837158203125e-07\n",
      "Step 39, 01, max diff: 4.172325134277344e-07\n",
      "Step 39, 02, max diff: 4.76837158203125e-07\n",
      "Step 39, 03, max diff: 4.76837158203125e-07\n",
      "Step 39, 12, max diff: 4.76837158203125e-07\n",
      "Step 39, 13, max diff: 5.960464477539062e-07\n",
      "Step 39, 23, max diff: 4.76837158203125e-07\n",
      "Step 40, 01, max diff: 4.172325134277344e-07\n",
      "Step 40, 02, max diff: 4.76837158203125e-07\n",
      "Step 40, 03, max diff: 5.364418029785156e-07\n",
      "Step 40, 12, max diff: 4.76837158203125e-07\n",
      "Step 40, 13, max diff: 5.960464477539062e-07\n",
      "Step 40, 23, max diff: 4.76837158203125e-07\n",
      "Step 41, 01, max diff: 4.172325134277344e-07\n",
      "Step 41, 02, max diff: 4.76837158203125e-07\n",
      "Step 41, 03, max diff: 5.364418029785156e-07\n",
      "Step 41, 12, max diff: 4.76837158203125e-07\n",
      "Step 41, 13, max diff: 5.960464477539062e-07\n",
      "Step 41, 23, max diff: 4.76837158203125e-07\n",
      "Step 42, 01, max diff: 4.172325134277344e-07\n",
      "Step 42, 02, max diff: 4.76837158203125e-07\n",
      "Step 42, 03, max diff: 4.76837158203125e-07\n",
      "Step 42, 12, max diff: 4.76837158203125e-07\n",
      "Step 42, 13, max diff: 5.960464477539062e-07\n",
      "Step 42, 23, max diff: 4.76837158203125e-07\n",
      "Step 43, 01, max diff: 4.76837158203125e-07\n",
      "Step 43, 02, max diff: 4.76837158203125e-07\n",
      "Step 43, 03, max diff: 4.76837158203125e-07\n",
      "Step 43, 12, max diff: 5.364418029785156e-07\n",
      "Step 43, 13, max diff: 5.960464477539062e-07\n",
      "Step 43, 23, max diff: 4.76837158203125e-07\n",
      "Step 44, 01, max diff: 4.76837158203125e-07\n",
      "Step 44, 02, max diff: 4.76837158203125e-07\n",
      "Step 44, 03, max diff: 4.76837158203125e-07\n",
      "Step 44, 12, max diff: 5.364418029785156e-07\n",
      "Step 44, 13, max diff: 5.960464477539062e-07\n",
      "Step 44, 23, max diff: 5.364418029785156e-07\n",
      "Step 45, 01, max diff: 4.76837158203125e-07\n",
      "Step 45, 02, max diff: 4.76837158203125e-07\n",
      "Step 45, 03, max diff: 4.76837158203125e-07\n",
      "Step 45, 12, max diff: 5.364418029785156e-07\n",
      "Step 45, 13, max diff: 5.960464477539062e-07\n",
      "Step 45, 23, max diff: 5.960464477539062e-07\n",
      "Step 46, 01, max diff: 4.76837158203125e-07\n",
      "Step 46, 02, max diff: 5.364418029785156e-07\n",
      "Step 46, 03, max diff: 4.76837158203125e-07\n",
      "Step 46, 12, max diff: 5.364418029785156e-07\n",
      "Step 46, 13, max diff: 6.556510925292969e-07\n",
      "Step 46, 23, max diff: 5.960464477539062e-07\n",
      "Step 47, 01, max diff: 4.76837158203125e-07\n",
      "Step 47, 02, max diff: 5.364418029785156e-07\n",
      "Step 47, 03, max diff: 4.76837158203125e-07\n",
      "Step 47, 12, max diff: 5.364418029785156e-07\n",
      "Step 47, 13, max diff: 6.556510925292969e-07\n",
      "Step 47, 23, max diff: 6.556510925292969e-07\n",
      "Step 48, 01, max diff: 4.76837158203125e-07\n",
      "Step 48, 02, max diff: 5.364418029785156e-07\n",
      "Step 48, 03, max diff: 4.76837158203125e-07\n",
      "Step 48, 12, max diff: 5.364418029785156e-07\n",
      "Step 48, 13, max diff: 6.556510925292969e-07\n",
      "Step 48, 23, max diff: 6.556510925292969e-07\n",
      "Step 49, 01, max diff: 4.76837158203125e-07\n",
      "Step 49, 02, max diff: 5.364418029785156e-07\n",
      "Step 49, 03, max diff: 4.76837158203125e-07\n",
      "Step 49, 12, max diff: 5.364418029785156e-07\n",
      "Step 49, 13, max diff: 6.556510925292969e-07\n",
      "Step 49, 23, max diff: 6.556510925292969e-07\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "manual_RBM = RBM(config)\n",
    "torch.manual_seed(0)\n",
    "torch_RBM = RBMTorchFull(config)\n",
    "\n",
    "n_steps = 50\n",
    "n_samples = 10\n",
    "n_nodes_p = config.rbm.latent_nodes_per_p\n",
    "\n",
    "for step in range(n_steps):\n",
    "    post_samples = [\n",
    "        torch.randint(0, 2, (n_samples, n_nodes_p), dtype=torch.float32)\n",
    "        for _ in range(4)\n",
    "    ]\n",
    "    # ensure same randomness inside both\n",
    "    with torch.random.fork_rng():\n",
    "        torch.manual_seed(step)  # sync RNG\n",
    "        manual_RBM.gradient_rbm(post_samples)\n",
    "        manual_RBM.update_params_SGD()\n",
    "\n",
    "    with torch.random.fork_rng():\n",
    "        torch.manual_seed(step)\n",
    "        torch_RBM.step_on_batch(post_samples)\n",
    "\n",
    "    # compare weights after this step\n",
    "    for key in manual_RBM._weight_dict.keys():\n",
    "        diff = (manual_RBM._weight_dict[key] - torch_RBM.weight_dict[key]).abs().max()\n",
    "        print(f\"Step {step}, {key}, max diff: {diff}\")\n",
    "        assert diff < 1e-5, f\"Mismatch at step {step} for {key}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0fab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RBM = RBM(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9903e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configure this list with your chosen qubits (indices from the sampler.nodelist) ---\n",
    "chosen_qubits = [100, 101, 250]  # Example qubit indices; replace with your list\n",
    "\n",
    "# Get a live Zephyr sampler\n",
    "sampler = DWaveSampler(solver={'topology__type': 'zephyr'})\n",
    "m, t = sampler.properties['topology']['shape']\n",
    "\n",
    "# Build a graph of active hardware\n",
    "G = dnx.zephyr_graph(m, t,\n",
    "                     node_list=sampler.nodelist,\n",
    "                     edge_list=sampler.edgelist)\n",
    "\n",
    "# Get 2D coordinates for plotting\n",
    "pos = dnx.zephyr_layout(G)\n",
    "\n",
    "# Color nodes: red if in chosen_qubits, gray otherwise\n",
    "node_colors = ['red' if n in chosen_qubits else 'lightgray' for n in G.nodes]\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "nx.draw_networkx_edges(G, pos=pos, alpha=0.3, width=0.5)\n",
    "nx.draw_networkx_nodes(G, pos=pos,\n",
    "                       node_color=node_colors,\n",
    "                       node_size=40,\n",
    "                       linewidths=0.5,\n",
    "                       edgecolors='black')\n",
    "\n",
    "plt.title(f\"Zephyr topology ({sampler.solver.name}) with highlighted qubits\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e38c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config.wandb.watch = 0\n",
    "new_model = True\n",
    "if new_model:\n",
    "    self = setup_model(cfg)\n",
    "else:\n",
    "    config = OmegaConf.load(cfg.config_path)\n",
    "    config.gpu_list = cfg.gpu_list\n",
    "    config.load_state = cfg.load_state\n",
    "    self = setup_model(config)\n",
    "    self._model_creator.load_state(config.run_path, self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48600a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.autograd.set_detect_anomaly(True)\n",
    "self.fit_vae(0)\n",
    "self._save_model(name=str(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1cb147",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.model_creator.load_state(\"/tmp/autoencoderbase_0.pth\", self.device, self.optimiser, self.model.prior.opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e183ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "manual_RBM = RBM(config)\n",
    "manual_weights = manual_RBM.weight_dict\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch_RBM = RBMtorch(config)\n",
    "torch_weights = torch_RBM.weight_dict\n",
    "\n",
    "#check if two weight dicts are equal\n",
    "assert manual_weights.keys() == torch_weights.keys()\n",
    "for key in manual_weights.keys():\n",
    "    assert torch.all(manual_weights[key] == torch_weights[key])\n",
    "# print top values in both weight dicts\n",
    "# print(\"Manual RBM weights:\")\n",
    "# print(manual_weights['02'][0])\n",
    "# print(\"Torch RBM weights:\")\n",
    "# print(torch_weights['02'][0])\n",
    "\n",
    "# calculate mock gradients\n",
    "# manual_RBM.calculate_mock_gradient()\n",
    "# torch_RBM.calculate_mock_gradient()\n",
    "\n",
    "#generate dummy post_samples\n",
    "torch.manual_seed(0)\n",
    "\n",
    "n_samples = 10\n",
    "n_nodes_p = config.rbm.latent_nodes_per_p\n",
    "post_samples = [\n",
    "    torch.randint(0, 2, (n_samples, n_nodes_p), dtype=torch.float32)\n",
    "    for _ in range(4)\n",
    "]\n",
    "\n",
    "# Compute gradients with identical RNG sequences\n",
    "with torch.random.fork_rng():\n",
    "    torch.manual_seed(123)\n",
    "    manual_RBM.gradient_rbm_centered(post_samples)\n",
    "\n",
    "with torch.random.fork_rng():\n",
    "    torch.manual_seed(123)\n",
    "    torch_RBM.gradient_rbm_centered(post_samples)\n",
    "\n",
    "# Compare with tolerances + diagnostics\n",
    "for kind in (\"bias\", \"weight\"):\n",
    "    for key in manual_RBM.grad[kind]:\n",
    "        a = manual_RBM.grad[kind][key]\n",
    "        b = torch_RBM.grad[kind][key]\n",
    "        if not torch.allclose(a, b, rtol=1e-5, atol=1e-8):\n",
    "            diff = (a - b).abs().max().item()\n",
    "            print(f\"{kind}.{key} differ, max|Î”|={diff}\")\n",
    "\n",
    "# use different optimizers\n",
    "manual_RBM.initOpt()\n",
    "manual_RBM.update_params()\n",
    "\n",
    "torch_RBM.initOpt_torch()\n",
    "torch_RBM.update_params_torch()\n",
    "\n",
    "# print top values in both weight dicts\n",
    "manual_weights = manual_RBM._weight_dict\n",
    "torch_weights = torch_RBM.weight_dict\n",
    "print(\"Manual RBM weights:\")\n",
    "print(manual_weights['02'][0])\n",
    "print(\"Torch RBM weights:\")\n",
    "print(torch_weights['02'][0])\n",
    "\n",
    "for i, group in enumerate(torch_RBM.opt.param_groups):\n",
    "    print(f\"Param group {i}\")\n",
    "    for p in group['params']:\n",
    "        print(\"  id:\", id(p), \" shape:\", p.shape, \" requires_grad:\", p.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "# check if weights are equal\n",
    "assert manual_weights.keys() == torch_weights.keys()\n",
    "for key in manual_weights.keys():\n",
    "    diff = (manual_weights[key] - torch_weights[key]).abs().max()\n",
    "    print(f\"Max diff for {key}: {diff}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487d12d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "rbm_torch = RBMtorch(config)\n",
    "# Clone weights before update\n",
    "before_update = {k: v.clone() for k, v in rbm_torch._weight_dict.items()}\n",
    "\n",
    "# before step\n",
    "p = rbm_torch._weight_dict['01']\n",
    "print(id(p), p.storage().data_ptr())\n",
    "\n",
    "\n",
    "# Perform optimizer step\n",
    "rbm_torch.initOpt_torch()\n",
    "rbm_torch.calculate_mock_gradient()\n",
    "rbm_torch.update_params_torch()\n",
    "\n",
    "\n",
    "rbm_torch.update_params_torch()\n",
    "\n",
    "# Clone weights after update\n",
    "after_update = {k: v.clone() for k, v in rbm_torch._weight_dict.items()}\n",
    "# after step: these must be identical\n",
    "q = rbm_torch._weight_dict['01']\n",
    "print(id(q), q.storage().data_ptr())\n",
    "\n",
    "# Check for changes\n",
    "for k in before_update:\n",
    "    changed = not torch.allclose(before_update[k], after_update[k])\n",
    "    print(f\"Weights for '{k}' updated: {changed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e69243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class testRBM(RBM):\n",
    "    def calculate_mock_gradient(self):\n",
    "        \"\"\"Creates a deterministic mock gradient for testing.\"\"\"\n",
    "        self.grad = {\"bias\": {}, \"weight\": {}}\n",
    "        for key, param in self.bias_dict.items():\n",
    "            self.grad[\"bias\"][key] = torch.ones_like(param) * 0.1\n",
    "        for key, param in self.weight_dict.items():\n",
    "            self.grad[\"weight\"][key] = torch.ones_like(param) * -0.05\n",
    "RBMinitial = testRBM(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b22fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm_custom = copy.deepcopy(RBMinitial)\n",
    "rbm_custom.initOpt()\n",
    "rbm_custom.calculate_mock_gradient()\n",
    "\n",
    "initial_weight_01 = rbm_custom.weight_dict['01'].data.clone()\n",
    "rbm_custom.update_params()\n",
    "final_weight_custom = rbm_custom.weight_dict['01'].data.clone()\n",
    "\n",
    "print(f\"Initial Weight (Top 5 values): {initial_weight_01.flatten()[:5].tolist()}\")\n",
    "print(f\"Final Weight (Top 5 values):   {final_weight_custom.flatten()[:5].tolist()}\\n\")\n",
    "\n",
    "rbm_torch = copy.deepcopy(RBMinitial)\n",
    "rbm_torch.initOpt_torch()\n",
    "rbm_torch.calculate_mock_gradient()\n",
    "\n",
    "initial_weight_01_torch = rbm_torch.weight_dict['01'].data.clone()\n",
    "rbm_torch.update_params_torch()\n",
    "final_weight_torch = rbm_torch.weight_dict['01'].data.clone()\n",
    "\n",
    "print(f\"Initial Weight (Top 5 values): {initial_weight_01_torch.flatten()[:5].tolist()}\")\n",
    "print(f\"Final Weight (Top 5 values):   {final_weight_torch.flatten()[:5].tolist()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Running Test 1: Custom AdamOpt (Adam with L2 Regularization) ---\")\n",
    "rbm_custom = copy.deepcopy(rbm_initial)\n",
    "rbm_custom.initOpt()\n",
    "rbm_custom.calculate_mock_gradient()\n",
    "\n",
    "# Store initial weights for comparison\n",
    "initial_weight_01 = rbm_custom.weight_dict['01'].data.clone()\n",
    "rbm_custom.update_params()\n",
    "final_weight_custom = rbm_custom.weight_dict['01'].data.clone()\n",
    "\n",
    "print(f\"Initial Weight (Top 5 values): {initial_weight_01.flatten()[:5].tolist()}\")\n",
    "print(f\"Final Weight (Top 5 values):   {final_weight_custom.flatten()[:5].tolist()}\\n\")\n",
    "\n",
    "\n",
    "# --- Test 2: PyTorch's Adam (AdamW) Implementation ---\n",
    "print(\"--- Running Test 2: PyTorch `torch.optim.Adam` (AdamW) ---\")\n",
    "rbm_torch = copy.deepcopy(rbm_initial)\n",
    "rbm_torch.initOpt_torch()\n",
    "rbm_torch.calculate_mock_gradient()\n",
    "\n",
    "# Store initial weights (should be identical to the other test)\n",
    "initial_weight_01_torch = rbm_torch.weight_dict['01'].data.clone()\n",
    "rbm_torch.update_params_torch()\n",
    "final_weight_torch = rbm_torch.weight_dict['01'].data.clone()\n",
    "\n",
    "print(f\"Initial Weight (Top 5 values): {initial_weight_01_torch.flatten()[:5].tolist()}\")\n",
    "print(f\"Final Weight (Top 5 values):   {final_weight_torch.flatten()[:5].tolist()}\\n\")\n",
    "\n",
    "# --- Comparison and Conclusion ---\n",
    "print(\"--- Comparison ---\")\n",
    "# Check if initial weights are identical\n",
    "are_initial_weights_same = torch.allclose(initial_weight_01, initial_weight_01_torch)\n",
    "print(f\"Initial weights were identical: {are_initial_weights_same}\")\n",
    "\n",
    "# Check if final weights are identical\n",
    "are_final_weights_same = torch.allclose(final_weight_custom, final_weight_torch)\n",
    "print(f\"Final weights are identical: {are_final_weights_same}\")\n",
    "\n",
    "if not are_final_weights_same:\n",
    "    difference = torch.mean(torch.abs(final_weight_custom - final_weight_torch))\n",
    "    print(f\"Average absolute difference between final weights: {difference.item():.8f}\")\n",
    "    print(\"\\nConclusion: The final weights are different, confirming the optimizer behaviors are not identical.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
