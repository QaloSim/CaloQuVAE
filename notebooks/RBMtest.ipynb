{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83206f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[18:21:40.680]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mCaloQuVAE                                         \u001b[0mLoading configuration.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "from hydra.utils import instantiate\n",
    "from hydra import initialize, compose\n",
    "import hydra\n",
    "import wandb\n",
    "from omegaconf import OmegaConf\n",
    "import copy\n",
    "\n",
    "from model.rbm.rbm import RBM\n",
    "from model.rbm.rbm_torch import RBMtorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3254aaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/06fr5gjy?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fc6fa115790>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=\"config\")\n",
    "config=compose(config_name=\"config.yaml\")\n",
    "wandb.init(tags = [config.data.dataset_name], project=config.wandb.project, entity=config.wandb.entity, config=OmegaConf.to_container(config, resolve=True), mode='disabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e183ed94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[18:21:48.393]\u001b[0m \u001b[1;93mWARN \u001b[1;0m  \u001b[1mmodel.rbm.zephyr                                  \u001b[0mQPU is offline. Setting a hard-coded zephyr. Check to see you're pinging the correct chip_id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leozhu/CaloQuVAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[18:21:49.361]\u001b[0m \u001b[1;93mWARN \u001b[1;0m  \u001b[1mmodel.rbm.zephyr                                  \u001b[0mQPU is offline. Setting a hard-coded zephyr. Check to see you're pinging the correct chip_id\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/leozhu/CaloQuVAE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/random.py:187: UserWarning: CUDA reports that you have 6 available devices, and you have used fork_rng without explicitly specifying which devices are being used. For safety, we initialize *every* CUDA device by default, which can be quite slow if you have a lot of CUDAs. If you know that you are only making use of a few CUDA devices, set the environment variable CUDA_VISIBLE_DEVICES or the 'devices' keyword argument of fork_rng with the set of devices you are actually using. For example, if you are using CPU only, set device.upper()_VISIBLE_DEVICES= or devices=[]; if you are using device 0 only, set CUDA_VISIBLE_DEVICES=0 or devices=[0].  To initialize all devices and suppress this warning, set the 'devices' keyword argument to `range(torch.cuda.device_count())`.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manual RBM weights:\n",
      "tensor([-2.0857, -0.7343,  0.0906, -0.6137, -2.3928,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Torch RBM weights:\n",
      "tensor([-2.0857, -0.7343,  0.0906, -0.6137, -2.3928, -0.0000,  0.0000,  0.0000,\n",
      "        -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "        -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "        -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "         0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
      "        -0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
      "        -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "        -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
      "        -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
      "        -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "        -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,\n",
      "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
      "        -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
      "        -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
      "         0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,\n",
      "        -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
      "        -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
      "        -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
      "        -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
      "        -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "        -0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,\n",
      "         0.0000, -0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "        -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000,  0.0000,\n",
      "        -0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
      "        -0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,\n",
      "         0.0000, -0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,\n",
      "         0.0000, -0.0000, -0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000, -0.0000,\n",
      "        -0.0000, -0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,\n",
      "        -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
      "        -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000, -0.0000,\n",
      "         0.0000,  0.0000, -0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000,\n",
      "         0.0000,  0.0000,  0.0000, -0.0000, -0.0000, -0.0000,  0.0000, -0.0000],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "Param group 0\n",
      "  id: 140492162147312  shape: torch.Size([512, 512])  requires_grad: True\n",
      "  id: 140492162147632  shape: torch.Size([512, 512])  requires_grad: True\n",
      "  id: 140492162147552  shape: torch.Size([512, 512])  requires_grad: True\n",
      "  id: 140492162147712  shape: torch.Size([512, 512])  requires_grad: True\n",
      "  id: 140492162147792  shape: torch.Size([512, 512])  requires_grad: True\n",
      "  id: 140492162147872  shape: torch.Size([512, 512])  requires_grad: True\n",
      "  id: 140492586485312  shape: torch.Size([512])  requires_grad: True\n",
      "  id: 140492585873920  shape: torch.Size([512])  requires_grad: True\n",
      "  id: 140492162147952  shape: torch.Size([512])  requires_grad: True\n",
      "  id: 140492162148032  shape: torch.Size([512])  requires_grad: True\n",
      "Max diff for 01: 1.1920928955078125e-07\n",
      "Max diff for 02: 1.1920928955078125e-07\n",
      "Max diff for 03: 1.1920928955078125e-07\n",
      "Max diff for 12: 1.1920928955078125e-07\n",
      "Max diff for 13: 1.1920928955078125e-07\n",
      "Max diff for 23: 1.1920928955078125e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_tensor.py:1128: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /opt/pytorch/pytorch/aten/src/ATen/native/Scalar.cpp:22.)\n",
      "  return self.item().__format__(format_spec)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "manual_RBM = RBM(config)\n",
    "manual_weights = manual_RBM.weight_dict\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch_RBM = RBMtorch(config)\n",
    "torch_weights = torch_RBM.weight_dict\n",
    "\n",
    "#check if two weight dicts are equal\n",
    "assert manual_weights.keys() == torch_weights.keys()\n",
    "for key in manual_weights.keys():\n",
    "    assert torch.all(manual_weights[key] == torch_weights[key])\n",
    "# print top values in both weight dicts\n",
    "# print(\"Manual RBM weights:\")\n",
    "# print(manual_weights['02'][0])\n",
    "# print(\"Torch RBM weights:\")\n",
    "# print(torch_weights['02'][0])\n",
    "\n",
    "# calculate mock gradients\n",
    "# manual_RBM.calculate_mock_gradient()\n",
    "# torch_RBM.calculate_mock_gradient()\n",
    "\n",
    "#generate dummy post_samples\n",
    "torch.manual_seed(0)\n",
    "\n",
    "n_samples = 10\n",
    "n_nodes_p = config.rbm.latent_nodes_per_p\n",
    "post_samples = [\n",
    "    torch.randint(0, 2, (n_samples, n_nodes_p), dtype=torch.float32)\n",
    "    for _ in range(4)\n",
    "]\n",
    "\n",
    "# Compute gradients with identical RNG sequences\n",
    "with torch.random.fork_rng():\n",
    "    torch.manual_seed(123)\n",
    "    manual_RBM.gradient_rbm_centered(post_samples)\n",
    "\n",
    "with torch.random.fork_rng():\n",
    "    torch.manual_seed(123)\n",
    "    torch_RBM.gradient_rbm_centered(post_samples)\n",
    "\n",
    "# Compare with tolerances + diagnostics\n",
    "for kind in (\"bias\", \"weight\"):\n",
    "    for key in manual_RBM.grad[kind]:\n",
    "        a = manual_RBM.grad[kind][key]\n",
    "        b = torch_RBM.grad[kind][key]\n",
    "        if not torch.allclose(a, b, rtol=1e-5, atol=1e-8):\n",
    "            diff = (a - b).abs().max().item()\n",
    "            print(f\"{kind}.{key} differ, max|Î”|={diff}\")\n",
    "\n",
    "# use different optimizers\n",
    "manual_RBM.initOpt()\n",
    "manual_RBM.update_params()\n",
    "\n",
    "torch_RBM.initOpt_torch()\n",
    "torch_RBM.update_params_torch()\n",
    "\n",
    "# print top values in both weight dicts\n",
    "manual_weights = manual_RBM._weight_dict\n",
    "torch_weights = torch_RBM.weight_dict\n",
    "print(\"Manual RBM weights:\")\n",
    "print(manual_weights['02'][0])\n",
    "print(\"Torch RBM weights:\")\n",
    "print(torch_weights['02'][0])\n",
    "\n",
    "for i, group in enumerate(torch_RBM.opt.param_groups):\n",
    "    print(f\"Param group {i}\")\n",
    "    for p in group['params']:\n",
    "        print(\"  id:\", id(p), \" shape:\", p.shape, \" requires_grad:\", p.requires_grad)\n",
    "\n",
    "\n",
    "\n",
    "# check if weights are equal\n",
    "assert manual_weights.keys() == torch_weights.keys()\n",
    "for key in manual_weights.keys():\n",
    "    diff = (manual_weights[key] - torch_weights[key]).abs().max()\n",
    "    print(f\"Max diff for {key}: {diff}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487d12d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "rbm_torch = RBMtorch(config)\n",
    "# Clone weights before update\n",
    "before_update = {k: v.clone() for k, v in rbm_torch._weight_dict.items()}\n",
    "\n",
    "# before step\n",
    "p = rbm_torch._weight_dict['01']\n",
    "print(id(p), p.storage().data_ptr())\n",
    "\n",
    "\n",
    "# Perform optimizer step\n",
    "rbm_torch.initOpt_torch()\n",
    "rbm_torch.calculate_mock_gradient()\n",
    "rbm_torch.update_params_torch()\n",
    "\n",
    "\n",
    "rbm_torch.update_params_torch()\n",
    "\n",
    "# Clone weights after update\n",
    "after_update = {k: v.clone() for k, v in rbm_torch._weight_dict.items()}\n",
    "# after step: these must be identical\n",
    "q = rbm_torch._weight_dict['01']\n",
    "print(id(q), q.storage().data_ptr())\n",
    "\n",
    "# Check for changes\n",
    "for k in before_update:\n",
    "    changed = not torch.allclose(before_update[k], after_update[k])\n",
    "    print(f\"Weights for '{k}' updated: {changed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e69243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class testRBM(RBM):\n",
    "    def calculate_mock_gradient(self):\n",
    "        \"\"\"Creates a deterministic mock gradient for testing.\"\"\"\n",
    "        self.grad = {\"bias\": {}, \"weight\": {}}\n",
    "        for key, param in self.bias_dict.items():\n",
    "            self.grad[\"bias\"][key] = torch.ones_like(param) * 0.1\n",
    "        for key, param in self.weight_dict.items():\n",
    "            self.grad[\"weight\"][key] = torch.ones_like(param) * -0.05\n",
    "RBMinitial = testRBM(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b22fc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbm_custom = copy.deepcopy(RBMinitial)\n",
    "rbm_custom.initOpt()\n",
    "rbm_custom.calculate_mock_gradient()\n",
    "\n",
    "initial_weight_01 = rbm_custom.weight_dict['01'].data.clone()\n",
    "rbm_custom.update_params()\n",
    "final_weight_custom = rbm_custom.weight_dict['01'].data.clone()\n",
    "\n",
    "print(f\"Initial Weight (Top 5 values): {initial_weight_01.flatten()[:5].tolist()}\")\n",
    "print(f\"Final Weight (Top 5 values):   {final_weight_custom.flatten()[:5].tolist()}\\n\")\n",
    "\n",
    "rbm_torch = copy.deepcopy(RBMinitial)\n",
    "rbm_torch.initOpt_torch()\n",
    "rbm_torch.calculate_mock_gradient()\n",
    "\n",
    "initial_weight_01_torch = rbm_torch.weight_dict['01'].data.clone()\n",
    "rbm_torch.update_params_torch()\n",
    "final_weight_torch = rbm_torch.weight_dict['01'].data.clone()\n",
    "\n",
    "print(f\"Initial Weight (Top 5 values): {initial_weight_01_torch.flatten()[:5].tolist()}\")\n",
    "print(f\"Final Weight (Top 5 values):   {final_weight_torch.flatten()[:5].tolist()}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Running Test 1: Custom AdamOpt (Adam with L2 Regularization) ---\")\n",
    "rbm_custom = copy.deepcopy(rbm_initial)\n",
    "rbm_custom.initOpt()\n",
    "rbm_custom.calculate_mock_gradient()\n",
    "\n",
    "# Store initial weights for comparison\n",
    "initial_weight_01 = rbm_custom.weight_dict['01'].data.clone()\n",
    "rbm_custom.update_params()\n",
    "final_weight_custom = rbm_custom.weight_dict['01'].data.clone()\n",
    "\n",
    "print(f\"Initial Weight (Top 5 values): {initial_weight_01.flatten()[:5].tolist()}\")\n",
    "print(f\"Final Weight (Top 5 values):   {final_weight_custom.flatten()[:5].tolist()}\\n\")\n",
    "\n",
    "\n",
    "# --- Test 2: PyTorch's Adam (AdamW) Implementation ---\n",
    "print(\"--- Running Test 2: PyTorch `torch.optim.Adam` (AdamW) ---\")\n",
    "rbm_torch = copy.deepcopy(rbm_initial)\n",
    "rbm_torch.initOpt_torch()\n",
    "rbm_torch.calculate_mock_gradient()\n",
    "\n",
    "# Store initial weights (should be identical to the other test)\n",
    "initial_weight_01_torch = rbm_torch.weight_dict['01'].data.clone()\n",
    "rbm_torch.update_params_torch()\n",
    "final_weight_torch = rbm_torch.weight_dict['01'].data.clone()\n",
    "\n",
    "print(f\"Initial Weight (Top 5 values): {initial_weight_01_torch.flatten()[:5].tolist()}\")\n",
    "print(f\"Final Weight (Top 5 values):   {final_weight_torch.flatten()[:5].tolist()}\\n\")\n",
    "\n",
    "# --- Comparison and Conclusion ---\n",
    "print(\"--- Comparison ---\")\n",
    "# Check if initial weights are identical\n",
    "are_initial_weights_same = torch.allclose(initial_weight_01, initial_weight_01_torch)\n",
    "print(f\"Initial weights were identical: {are_initial_weights_same}\")\n",
    "\n",
    "# Check if final weights are identical\n",
    "are_final_weights_same = torch.allclose(final_weight_custom, final_weight_torch)\n",
    "print(f\"Final weights are identical: {are_final_weights_same}\")\n",
    "\n",
    "if not are_final_weights_same:\n",
    "    difference = torch.mean(torch.abs(final_weight_custom - final_weight_torch))\n",
    "    print(f\"Average absolute difference between final weights: {difference.item():.8f}\")\n",
    "    print(\"\\nConclusion: The final weights are different, confirming the optimizer behaviors are not identical.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
