{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24277d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "from hydra.utils import instantiate\n",
    "from hydra import initialize, compose\n",
    "import hydra\n",
    "import wandb\n",
    "from omegaconf import OmegaConf\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model.rbm.rbm import RBM\n",
    "from model.rbm.rbm_torch import RBMtorch\n",
    "from model.rbm.rbm_fulltorch import RBMTorchFull\n",
    "from scripts.run import setup_model, load_model_instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5771b056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize MNIST transform (pixels to {0,1})\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: (x > 0.5).float())\n",
    "])\n",
    "\n",
    "# Download MNIST\n",
    "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
    "test_dataset  = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "#print number of batches\n",
    "print(f\"Number of training batches: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ee4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(x, dev):\n",
    "    x = x.to(dev)\n",
    "    x = x.view(x.size(0), -1)   # flatten -> (batch, 784)\n",
    "    return torch.chunk(x, 4, dim=1)  # four partitions, each (batch, 196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df4fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=\"config\")\n",
    "config=compose(config_name=\"config.yaml\")\n",
    "\n",
    "# Pick device from config or fall back to auto-detect\n",
    "if config.device == \"gpu\" and torch.cuda.is_available():\n",
    "    dev = torch.device(f\"cuda:{config.gpu_list[0]}\")\n",
    "else:\n",
    "    dev = torch.device(\"cpu\")\n",
    "\n",
    "# Move RBM to device\n",
    "\n",
    "RBM = RBMTorchFull(config).to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edbf7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        post_samples = preprocess_batch(x, dev)\n",
    "        loss = RBM.step_on_batch(post_samples)\n",
    "        losses.append(loss)\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch {epoch} Batch {batch_idx}: Loss = {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9e3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_flat_samples(p0, p1, p2, p3, n=8):\n",
    "    \"\"\"\n",
    "    Stitch partitions from a flat-split RBM into 28x28 images.\n",
    "    p0..p3: (batch, 196) binary samples\n",
    "    n: number of images to display\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    for i in range(n):\n",
    "        flat = torch.cat([p0[i], p1[i], p2[i], p3[i]], dim=0)  # length 784\n",
    "        img = flat.view(28, 28).cpu().numpy()\n",
    "        samples.append(img)\n",
    "\n",
    "    fig, axes = plt.subplots(1, n, figsize=(n*2, 2))\n",
    "    for ax, img in zip(axes, samples):\n",
    "        ax.imshow(img, cmap=\"gray\")\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3a8b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unconditional samples\n",
    "p0, p1, p2, p3 = .block_gibbs_sampling(batch_size=16)\n",
    "\n",
    "# Visualize\n",
    "visualize_flat_samples(p0, p1, p2, p3, n=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efae6cc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
