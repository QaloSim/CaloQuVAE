{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dfae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "from hydra.utils import instantiate\n",
    "from hydra import initialize, compose\n",
    "import hydra\n",
    "import wandb\n",
    "\n",
    "from data.dataManager import DataManager\n",
    "from model.modelCreator import ModelCreator\n",
    "from omegaconf import OmegaConf\n",
    "from scripts.run import setup_model, load_model_instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1b7b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=\"config\")\n",
    "config=compose(config_name=\"config.yaml\")\n",
    "wandb.init(tags = [config.data.dataset_name], project=config.wandb.project, entity=config.wandb.entity, config=OmegaConf.to_container(config, resolve=True), mode='disabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a17fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "devids = [\"cuda:{0}\".format(x) for x in list(config.gpu_list)]\n",
    "dev = torch.device(devids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c978bf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = True\n",
    "if new_model:\n",
    "    self = setup_model(config)\n",
    "    # self.model = self.model.double()  # sets all model parameters to float64\n",
    "else:\n",
    "    self = load_model_instance(config.config_path)\n",
    "    # self.model = self.model.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f15764",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.evaluate_vae(self.data_mgr.val_loader, epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3832f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import utils.plots\n",
    "importlib.reload(utils.plots)\n",
    "from utils.plots import vae_plots\n",
    "vae_plots(\n",
    "    cfg = self._config,\n",
    "    incident_energies = self.incident_energy,\n",
    "    target_showers = self.showers,\n",
    "    recon_showers = self.showers_recon,\n",
    "    sampled_showers = self.showers_prior\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816b2cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.evaluate(engine.data_mgr.val_loader, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7f27cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(engine.data_mgr.train_loader))\n",
    "# next(iter(dataMgr.train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63edf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    enc_data = engine.model.encoder(x[0].to(dev, dtype=torch.float32), x[1].to(dev, dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86af05f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(enc_data)\n",
    "len(enc_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e516f0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    enc_data = engine.model((x[0].to(dev, dtype=torch.float32), x[1].to(dev, dtype=torch.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a48a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    l = engine.model.loss(x[0].to(dev, dtype=torch.float32), enc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3085d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d7d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_samples = enc_data[2]\n",
    "# post_samples = [torch.rand(128,302, device=dev) * 10 for _ in range(4)]\n",
    "post_samples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b546df",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.model.prior.energy_exp(\n",
    "    post_samples[0].to(dtype=torch.float),\n",
    "    post_samples[1],\n",
    "    post_samples[2],\n",
    "    post_samples[3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23023527",
   "metadata": {},
   "outputs": [],
   "source": [
    "(- (post_samples[0].to(dtype=torch.float) @ engine.model.prior.weight_dict[\"01\"] @ post_samples[1].T).diagonal() - \\\n",
    "(post_samples[0].to(dtype=torch.float) @ engine.model.prior.weight_dict[\"02\"] @ post_samples[2].T.to(dtype=torch.float)).diagonal() - \\\n",
    "(post_samples[0].to(dtype=torch.float) @ engine.model.prior.weight_dict[\"03\"] @ post_samples[3].T.to(dtype=torch.float)).diagonal() - \\\n",
    "(post_samples[1].to(dtype=torch.float) @ engine.model.prior.weight_dict[\"12\"] @ post_samples[2].T.to(dtype=torch.float)).diagonal() - \\\n",
    "(post_samples[1].to(dtype=torch.float) @ engine.model.prior.weight_dict[\"13\"] @ post_samples[3].T.to(dtype=torch.float)).diagonal() - \\\n",
    "(post_samples[2].to(dtype=torch.float) @ engine.model.prior.weight_dict[\"23\"] @ post_samples[3].T.to(dtype=torch.float)).diagonal() \\\n",
    "    - post_samples[1] @ engine.model.prior.bias_dict[\"1\"] - \\\n",
    "post_samples[2] @ engine.model.prior.bias_dict[\"2\"] - \\\n",
    "post_samples[3] @ engine.model.prior.bias_dict[\"3\"]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b935620",
   "metadata": {},
   "outputs": [],
   "source": [
    "(- post_samples[1] @ engine.model.prior.bias_dict[\"1\"].to(dtype=torch.float) - \\\n",
    "post_samples[2] @ engine.model.prior.bias_dict[\"2\"].to(dtype=torch.float) - \\\n",
    "post_samples[3] @ engine.model.prior.bias_dict[\"3\"].to(dtype=torch.float)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e755b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_samples[0].to(dtype=torch.float) @ engine.model.prior.bias_dict[\"0\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a853fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(engine.model.prior.weight_dict['01'] + torch.zeros((128,) +\n",
    "                                                    (302,302),\n",
    "                                                    device=engine.model.prior.weight_dict['01'].device)).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f37f7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.zeros((128,) + 302,device=engine.model.prior.weight_dict[key].device)\n",
    "post_samples[0].size(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a515866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.rbm.zephyr import ZephyrRBM, ZephyrRBM_Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dac95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.rbm.latent_nodes_per_p=512\n",
    "config.rbm.optimize_partition = True\n",
    "new = ZephyrRBM(config)\n",
    "old = ZephyrRBM_Old(config)\n",
    "\n",
    "binwidth = 1.0\n",
    "new_data, old_data = {}, {}\n",
    "for key in new.weight_dict.keys():\n",
    "    # data[key] = engine.model.prior.weight_dict[key].sign().abs().sum(dim=0).detach().cpu().numpy()\n",
    "    new_data[key] = new._weight_mask_dict[key].abs().sum(dim=0).cpu().numpy()\n",
    "for key in old.weight_dict.keys():\n",
    "    old_data[key] = old._weight_mask_dict[key].abs().sum(dim=0).cpu().numpy()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c80d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights(data, label):\n",
    "    # Create 2x2 grid of subplots\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(10, 7), sharey=True, sharex=True, tight_layout=True)  # 2x2 grid, figure size 10x10\n",
    "    fig.text(0.5, 0.0, 'Couplings/qubit', ha='center', fontsize=15)\n",
    "    fig.text(0.0, 0.4, 'Histogram', rotation=90, ha='center', fontsize=15)\n",
    "\n",
    "    # Plot data on each subplot\n",
    "    labels, counts = np.unique(data['01'], return_counts=True)\n",
    "    axs[0,0].bar(labels, counts, align='center', color=\"b\", alpha=0.8)\n",
    "    # axs[0, 0].hist(data['01'], bins=np.arange(min(data['01']), max(data['01']) + binwidth, binwidth), histtype='stepfilled', linewidth=2.5, color=\"b\", alpha=0.7, align='center')\n",
    "    axs[0,0].grid(\"True\")\n",
    "    axs[0,0].legend([\"v to h\"], fontsize=18)\n",
    "\n",
    "    labels, counts = np.unique(data['02'], return_counts=True)\n",
    "    axs[0,1].bar(labels, counts, align='center', color=\"b\", alpha=0.9)\n",
    "    axs[0,1].grid(\"True\")\n",
    "    axs[0,1].legend([\"v to s\"], fontsize=18)\n",
    "\n",
    "    labels, counts = np.unique(data['03'], return_counts=True)\n",
    "    axs[0,2].bar(labels, counts, align='center', color=\"b\", alpha=0.9)\n",
    "    axs[0,2].grid(\"True\")\n",
    "    axs[0,2].legend([\"v to t\"], fontsize=18)\n",
    "\n",
    "    labels, counts = np.unique(data['12'], return_counts=True)\n",
    "    axs[1,0].bar(labels, counts, align='center', color=\"b\", alpha=0.9)\n",
    "    axs[1,0].grid(\"True\")\n",
    "    axs[1,0].legend([\"h to s\"], fontsize=18)\n",
    "\n",
    "    labels, counts = np.unique(data['13'], return_counts=True)\n",
    "    axs[1,1].bar(labels, counts, align='center', color=\"b\", alpha=0.9)\n",
    "    axs[1,1].grid(\"True\")\n",
    "    axs[1,1].legend([\"h to t\"], fontsize=18)\n",
    "\n",
    "    labels, counts = np.unique(data['23'], return_counts=True)\n",
    "    axs[1,2].bar(labels, counts, align='center', color=\"b\", alpha=0.9)\n",
    "    axs[1,2].grid(\"True\")\n",
    "    axs[1,2].legend([\"s to t\"], fontsize=18)\n",
    "\n",
    "    # plt.savefig(f'/home/javier/Projects/CaloQVAE/figs/weights_plot_zephyr.png', bbox_inches=\"tight\")\n",
    "    plt.suptitle(f\"Weight mask histogram for {label}\", fontsize=20)\n",
    "\n",
    "    plt.show()\n",
    "plot_weights(new_data, \"new algorithm\")\n",
    "plot_weights(old_data, \"old algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d9557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "zph = new\n",
    "# plt.hist(torch.cat((zph._weight_mask_dict['01'],zph._weight_mask_dict['02'],zph._weight_mask_dict['03']),1).sum(dim=1).numpy(), bins=20)\n",
    "# plt.show()\n",
    "# plt.hist(torch.cat((zph._weight_mask_dict['01'].transpose(1,0),zph._weight_mask_dict['12'],zph._weight_mask_dict['13']),1).sum(dim=1), bins=20)\n",
    "# plt.show()\n",
    "# plt.hist(torch.cat((zph._weight_mask_dict['02'].transpose(1,0),zph._weight_mask_dict['12'].transpose(1,0),zph._weight_mask_dict['23']),1).sum(dim=1), bins=20)\n",
    "# plt.show()\n",
    "# plt.hist(torch.cat((zph._weight_mask_dict['03'].transpose(1,0),zph._weight_mask_dict['13'].transpose(1,0),zph._weight_mask_dict['23'].transpose(1,0)),1).sum(dim=1), bins=20)\n",
    "# plt.show()\n",
    "def plot_total_weights(zeph, label):\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(10, 7), sharey=True, sharex=True, tight_layout=True)  # 2x2 grid, figure size 10x10\n",
    "    fig.text(0.5, 0.0, 'Couplings/qubit', ha='center', fontsize=15)\n",
    "    fig.text(0.0, 0.4, 'Histogram', rotation=90, ha='center', fontsize=15)\n",
    "    ax[0, 0].hist(torch.cat((zeph._weight_mask_dict['01'], zeph._weight_mask_dict['02'], zeph._weight_mask_dict['03']), 1).sum(dim=1).numpy(), bins=20)\n",
    "    ax[0, 0].set_title(\"v to h, s, t\")\n",
    "    ax[0, 1].hist(torch.cat((zeph._weight_mask_dict['01'].transpose(1,0), zeph._weight_mask_dict['12'], zeph._weight_mask_dict['13']), 1).sum(dim=1), bins=20)\n",
    "    ax[0, 1].set_title(\"h to v, s, t\")\n",
    "    ax[1, 0].hist(torch.cat((zeph._weight_mask_dict['02'].transpose(1,0), zeph._weight_mask_dict['12'].transpose(1,0), zeph._weight_mask_dict['23']), 1).sum(dim=1), bins=20)\n",
    "    ax[1, 0].set_title(\"s to v, h, t\")\n",
    "    ax[1, 1].hist(torch.cat((zeph._weight_mask_dict['03'].transpose(1,0), zeph._weight_mask_dict['13'].transpose(1,0), zeph._weight_mask_dict['23'].transpose(1,0)), 1).sum(dim=1), bins=20)\n",
    "    ax[1, 1].set_title(\"t to v, h, s\")\n",
    "    fig.suptitle(f\"Total weight mask histogram for {label}\", fontsize=20)\n",
    "\n",
    "plot_total_weights(new, \"new algorithm\")\n",
    "plot_total_weights(old, \"old algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb2ab03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights_overlay(new_data, old_data):\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(10, 7), sharey=True, sharex=True, tight_layout=True)\n",
    "    fig.text(0.5, 0.0, 'Couplings/qubit', ha='center', fontsize=15)\n",
    "    fig.text(0.0, 0.4, 'Histogram', rotation=90, ha='center', fontsize=15)\n",
    "\n",
    "    keys = ['01', '02', '03', '12', '13', '23']\n",
    "    titles = [\"v to h\", \"v to s\", \"v to t\", \"h to s\", \"h to t\", \"s to t\"]\n",
    "    \n",
    "    for i, key in enumerate(keys):\n",
    "        ax = axs[i // 3, i % 3]\n",
    "        data_new = new_data[key]\n",
    "        data_old = old_data[key]\n",
    "        # Determine bin edges to align histograms, centering on integers\n",
    "        min_val = min(data_new.min(), data_old.min())\n",
    "        max_val = max(data_new.max(), data_old.max())\n",
    "        bins = np.arange(min_val, max_val + 2) - 0.5  # Shift by 0.5 to center bins on integers\n",
    "        ax.hist(data_new, bins=bins, alpha=0.5, label='new', color='blue')\n",
    "        ax.hist(data_old, bins=bins, alpha=0.5, label='old', color='red')\n",
    "        ax.legend()\n",
    "        ax.set_title(titles[i])\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.suptitle(\"Weight mask histogram comparison\", fontsize=20)\n",
    "    plt.show()\n",
    "plot_weights_overlay(new_data, old_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a50d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_total_connections(zeph):\n",
    "    total_v = torch.cat((zeph._weight_mask_dict['01'], zeph._weight_mask_dict['02'], zeph._weight_mask_dict['03']), 1).sum(dim=1).cpu().numpy()\n",
    "    total_h = torch.cat((zeph._weight_mask_dict['01'].transpose(1,0), zeph._weight_mask_dict['12'], zeph._weight_mask_dict['13']), 1).sum(dim=1).cpu().numpy()\n",
    "    total_s = torch.cat((zeph._weight_mask_dict['02'].transpose(1,0), zeph._weight_mask_dict['12'].transpose(1,0), zeph._weight_mask_dict['23']), 1).sum(dim=1).cpu().numpy()\n",
    "    total_t = torch.cat((zeph._weight_mask_dict['03'].transpose(1,0), zeph._weight_mask_dict['13'].transpose(1,0), zeph._weight_mask_dict['23'].transpose(1,0)), 1).sum(dim=1).cpu().numpy()\n",
    "    return total_v, total_h, total_s, total_t\n",
    "\n",
    "def plot_total_weights_overlay(new, old):\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 7), sharey=True, sharex=True, tight_layout=True)\n",
    "    fig.text(0.5, 0.0, 'Couplings/qubit', ha='center', fontsize=15)\n",
    "    fig.text(0.0, 0.4, 'Histogram', rotation=90, ha='center', fontsize=15)\n",
    "\n",
    "    total_new = compute_total_connections(new)\n",
    "    total_old = compute_total_connections(old)\n",
    "\n",
    "    all_data = np.concatenate((total_new[0], total_new[1], total_new[2], total_new[3],\n",
    "                               total_old[0], total_old[1], total_old[2], total_old[3]))\n",
    "    min_val = int(all_data.min())\n",
    "    max_val = int(all_data.max())\n",
    "    bins = np.arange(min_val, max_val + 1, 1)  #\n",
    "\n",
    "    titles = [\"v to h, s, t\", \"h to v, s, t\", \"s to v, h, t\", \"t to v, h, s\"]\n",
    "    \n",
    "    for i in range(len(titles)):\n",
    "        ax = axs[i // 2, i % 2]\n",
    "        data_new = total_new[i]\n",
    "        data_old = total_old[i]\n",
    "        ax.hist(data_new, bins=bins, alpha=0.5, label='new', color='blue', align='mid')\n",
    "        ax.hist(data_old, bins=bins, alpha=0.5, label='old', color='red', align='mid')\n",
    "        ax.set_xticks(bins)  # Integer ticks\n",
    "        ax.legend()\n",
    "        ax.set_title(titles[i])\n",
    "        ax.grid(True)\n",
    "\n",
    "    plt.suptitle(\"Total weight mask histogram comparison\", fontsize=20)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_total_weights_overlay(new, old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b3772a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
