{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d0e1478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[06:25:15.230]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mCaloQuVAE                                         \u001b[0mLoading configuration.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "from hydra.utils import instantiate\n",
    "from hydra import initialize, compose\n",
    "import hydra\n",
    "\n",
    "import wandb\n",
    "\n",
    "from data.dataManager import DataManager\n",
    "from model.modelCreator import ModelCreator\n",
    "from omegaconf import OmegaConf\n",
    "from scripts.run import setup_model, load_model_instance\n",
    "\n",
    "from utils.plots import vae_plots\n",
    "from utils.rbm_plots import plot_rbm_histogram, plot_rbm_params\n",
    "\n",
    "from scripts.run import set_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e48f8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/3gaufl5s?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fd812fa7380>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=\"config\")\n",
    "config=compose(config_name=\"config.yaml\")\n",
    "wandb.init(tags = [config.data.dataset_name], project=config.wandb.project, entity=config.wandb.entity, config=OmegaConf.to_container(config, resolve=True), mode='disabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90abb182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[06:25:21.782]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0mLoading other dataset: CaloChallenge2\n",
      "\u001b[1m[06:25:21.787]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0mKeys: ['incident_energies', 'showers']\n",
      "\u001b[1m[06:25:26.940]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0mdict_keys(['incident_energies', 'showers'])\n",
      "\u001b[1m[06:25:26.943]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0m<torch.utils.data.dataloader.DataLoader object at 0x7fd8154f73e0>: 79999 events, 157 batches\n",
      "\u001b[1m[06:25:26.943]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0m<torch.utils.data.dataloader.DataLoader object at 0x7fd812ff0080>: 10001 events, 10 batches\n",
      "\u001b[1m[06:25:26.944]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0m<torch.utils.data.dataloader.DataLoader object at 0x7fd8136f3470>: 9999 events, 10 batches\n",
      "\u001b[1m[06:25:26.945]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mmodel.modelCreator                                \u001b[0m::Creating Model\n",
      "\u001b[1m[06:25:27.976]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mFetching definitions of all available solvers\n",
      "\u001b[1m[06:25:28.156]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mReceived solver data for 7 solver(s).\n",
      "\u001b[1m[06:25:28.398]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(id='Advantage_system4.1')\n",
      "\u001b[1m[06:25:28.451]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(id='Advantage_system6.4')\n",
      "\u001b[1m[06:25:28.503]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(id='Advantage2_system1.4')\n",
      "\u001b[1m[06:25:29.013]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mscripts.run                                       \u001b[0mRequesting GPUs. GPU list :[4]\n",
      "\u001b[1m[06:25:29.014]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mscripts.run                                       \u001b[0mMain GPU : cuda:4\n",
      "\u001b[1m[06:25:29.190]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mscripts.run                                       \u001b[0mCUDA available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:4\n",
      "encoder._networks.0.seq1.0.conv.weight True\n",
      "encoder._networks.0.seq1.0.conv.bias True\n",
      "encoder._networks.0.seq1.1.weight True\n",
      "encoder._networks.0.seq1.1.bias True\n",
      "encoder._networks.0.seq1.2.weight True\n",
      "encoder._networks.0.seq1.3.conv.weight True\n",
      "encoder._networks.0.seq1.3.conv.bias True\n",
      "encoder._networks.0.seq1.4.weight True\n",
      "encoder._networks.0.seq1.4.bias True\n",
      "encoder._networks.0.seq1.5.weight True\n",
      "encoder._networks.0.seq1.6.conv.weight True\n",
      "encoder._networks.0.seq1.6.conv.bias True\n",
      "encoder._networks.0.seq1.7.weight True\n",
      "encoder._networks.0.seq1.7.bias True\n",
      "encoder._networks.0.seq1.8.weight True\n",
      "encoder._networks.0.seq2.0.conv.weight True\n",
      "encoder._networks.0.seq2.0.conv.bias True\n",
      "encoder._networks.0.seq2.1.weight True\n",
      "encoder._networks.0.seq2.1.bias True\n",
      "encoder._networks.0.seq2.2.weight True\n",
      "encoder._networks.0.seq2.3.conv.weight True\n",
      "encoder._networks.0.seq2.3.conv.bias True\n",
      "encoder._networks.0.seq2.4.weight True\n",
      "encoder._networks.1.seq1.0.conv.weight True\n",
      "encoder._networks.1.seq1.0.conv.bias True\n",
      "encoder._networks.1.seq1.1.weight True\n",
      "encoder._networks.1.seq1.1.bias True\n",
      "encoder._networks.1.seq1.2.weight True\n",
      "encoder._networks.1.seq1.3.conv.weight True\n",
      "encoder._networks.1.seq1.3.conv.bias True\n",
      "encoder._networks.1.seq1.4.weight True\n",
      "encoder._networks.1.seq1.4.bias True\n",
      "encoder._networks.1.seq1.5.weight True\n",
      "encoder._networks.1.seq1.6.conv.weight True\n",
      "encoder._networks.1.seq1.6.conv.bias True\n",
      "encoder._networks.1.seq1.7.weight True\n",
      "encoder._networks.1.seq1.7.bias True\n",
      "encoder._networks.1.seq1.8.weight True\n",
      "encoder._networks.1.seq2.0.conv.weight True\n",
      "encoder._networks.1.seq2.0.conv.bias True\n",
      "encoder._networks.1.seq2.1.weight True\n",
      "encoder._networks.1.seq2.1.bias True\n",
      "encoder._networks.1.seq2.2.weight True\n",
      "encoder._networks.1.seq2.3.conv.weight True\n",
      "encoder._networks.1.seq2.3.conv.bias True\n",
      "encoder._networks.1.seq2.4.weight True\n",
      "encoder._networks.2.seq1.0.conv.weight True\n",
      "encoder._networks.2.seq1.0.conv.bias True\n",
      "encoder._networks.2.seq1.1.weight True\n",
      "encoder._networks.2.seq1.1.bias True\n",
      "encoder._networks.2.seq1.2.weight True\n",
      "encoder._networks.2.seq1.3.conv.weight True\n",
      "encoder._networks.2.seq1.3.conv.bias True\n",
      "encoder._networks.2.seq1.4.weight True\n",
      "encoder._networks.2.seq1.4.bias True\n",
      "encoder._networks.2.seq1.5.weight True\n",
      "encoder._networks.2.seq1.6.conv.weight True\n",
      "encoder._networks.2.seq1.6.conv.bias True\n",
      "encoder._networks.2.seq1.7.weight True\n",
      "encoder._networks.2.seq1.7.bias True\n",
      "encoder._networks.2.seq1.8.weight True\n",
      "encoder._networks.2.seq2.0.conv.weight True\n",
      "encoder._networks.2.seq2.0.conv.bias True\n",
      "encoder._networks.2.seq2.1.weight True\n",
      "encoder._networks.2.seq2.1.bias True\n",
      "encoder._networks.2.seq2.2.weight True\n",
      "encoder._networks.2.seq2.3.conv.weight True\n",
      "encoder._networks.2.seq2.3.conv.bias True\n",
      "encoder._networks.2.seq2.4.weight True\n",
      "decoder.moduleLayers.0._layers.1.conv.weight True\n",
      "decoder.moduleLayers.0._layers.1.conv.bias True\n",
      "decoder.moduleLayers.0._layers.2.weight True\n",
      "decoder.moduleLayers.0._layers.2.bias True\n",
      "decoder.moduleLayers.0._layers.3.weight True\n",
      "decoder.moduleLayers.0._layers.4.conv.weight True\n",
      "decoder.moduleLayers.0._layers.4.conv.bias True\n",
      "decoder.moduleLayers.0._layers.5.weight True\n",
      "decoder.moduleLayers.0._layers.5.bias True\n",
      "decoder.moduleLayers.0._layers.6.weight True\n",
      "decoder.moduleLayers.0._layers2.0.conv.weight True\n",
      "decoder.moduleLayers.0._layers2.0.conv.bias True\n",
      "decoder.moduleLayers.0._layers2.1.weight True\n",
      "decoder.moduleLayers.0._layers2.1.bias True\n",
      "decoder.moduleLayers.0._layers2.2.weight True\n",
      "decoder.moduleLayers.0._layers2.3.conv.weight True\n",
      "decoder.moduleLayers.0._layers2.3.conv.bias True\n",
      "decoder.moduleLayers.0._layers2.4.weight True\n",
      "decoder.moduleLayers.0._layers2.4.bias True\n",
      "decoder.moduleLayers.0._layers2.5.weight True\n",
      "decoder.moduleLayers.0._layers2.6.weight True\n",
      "decoder.moduleLayers.0._layers3.0.conv.weight True\n",
      "decoder.moduleLayers.0._layers3.0.conv.bias True\n",
      "decoder.moduleLayers.0._layers3.1.weight True\n",
      "decoder.moduleLayers.0._layers3.1.bias True\n",
      "decoder.moduleLayers.0._layers3.2.weight True\n",
      "decoder.moduleLayers.0._layers3.3.conv.weight True\n",
      "decoder.moduleLayers.0._layers3.3.conv.bias True\n",
      "decoder.moduleLayers.0._layers3.4.weight True\n",
      "decoder.moduleLayers.0._layers3.4.bias True\n",
      "decoder.moduleLayers.0._layers3.5.weight True\n",
      "decoder.moduleLayers.0._layers3.6.weight True\n",
      "decoder.moduleLayers.1._layers.1.conv.weight True\n",
      "decoder.moduleLayers.1._layers.1.conv.bias True\n",
      "decoder.moduleLayers.1._layers.2.weight True\n",
      "decoder.moduleLayers.1._layers.2.bias True\n",
      "decoder.moduleLayers.1._layers.3.weight True\n",
      "decoder.moduleLayers.1._layers.4.conv.weight True\n",
      "decoder.moduleLayers.1._layers.4.conv.bias True\n",
      "decoder.moduleLayers.1._layers.5.weight True\n",
      "decoder.moduleLayers.1._layers.5.bias True\n",
      "decoder.moduleLayers.1._layers.6.weight True\n",
      "decoder.moduleLayers.1._layers2.0.conv.weight True\n",
      "decoder.moduleLayers.1._layers2.0.conv.bias True\n",
      "decoder.moduleLayers.1._layers2.1.weight True\n",
      "decoder.moduleLayers.1._layers2.1.bias True\n",
      "decoder.moduleLayers.1._layers2.2.weight True\n",
      "decoder.moduleLayers.1._layers2.3.conv.weight True\n",
      "decoder.moduleLayers.1._layers2.3.conv.bias True\n",
      "decoder.moduleLayers.1._layers2.4.weight True\n",
      "decoder.moduleLayers.1._layers2.4.bias True\n",
      "decoder.moduleLayers.1._layers2.5.weight True\n",
      "decoder.moduleLayers.1._layers2.6.weight True\n",
      "decoder.moduleLayers.1._layers3.0.conv.weight True\n",
      "decoder.moduleLayers.1._layers3.0.conv.bias True\n",
      "decoder.moduleLayers.1._layers3.1.weight True\n",
      "decoder.moduleLayers.1._layers3.1.bias True\n",
      "decoder.moduleLayers.1._layers3.2.weight True\n",
      "decoder.moduleLayers.1._layers3.3.conv.weight True\n",
      "decoder.moduleLayers.1._layers3.3.conv.bias True\n",
      "decoder.moduleLayers.1._layers3.4.weight True\n",
      "decoder.moduleLayers.1._layers3.4.bias True\n",
      "decoder.moduleLayers.1._layers3.5.weight True\n",
      "decoder.moduleLayers.1._layers3.6.weight True\n",
      "decoder.moduleLayers.2._layers.1.conv.weight True\n",
      "decoder.moduleLayers.2._layers.1.conv.bias True\n",
      "decoder.moduleLayers.2._layers.2.weight True\n",
      "decoder.moduleLayers.2._layers.2.bias True\n",
      "decoder.moduleLayers.2._layers.3.weight True\n",
      "decoder.moduleLayers.2._layers.4.conv.weight True\n",
      "decoder.moduleLayers.2._layers.4.conv.bias True\n",
      "decoder.moduleLayers.2._layers.5.weight True\n",
      "decoder.moduleLayers.2._layers.5.bias True\n",
      "decoder.moduleLayers.2._layers.6.weight True\n",
      "decoder.moduleLayers.2._layers2.0.conv.weight True\n",
      "decoder.moduleLayers.2._layers2.0.conv.bias True\n",
      "decoder.moduleLayers.2._layers2.1.weight True\n",
      "decoder.moduleLayers.2._layers2.1.bias True\n",
      "decoder.moduleLayers.2._layers2.2.weight True\n",
      "decoder.moduleLayers.2._layers2.3.conv.weight True\n",
      "decoder.moduleLayers.2._layers2.3.conv.bias True\n",
      "decoder.moduleLayers.2._layers2.4.weight True\n",
      "decoder.moduleLayers.2._layers2.4.bias True\n",
      "decoder.moduleLayers.2._layers2.5.weight True\n",
      "decoder.moduleLayers.2._layers2.6.weight True\n",
      "decoder.moduleLayers.2._layers3.0.conv.weight True\n",
      "decoder.moduleLayers.2._layers3.0.conv.bias True\n",
      "decoder.moduleLayers.2._layers3.1.weight True\n",
      "decoder.moduleLayers.2._layers3.1.bias True\n",
      "decoder.moduleLayers.2._layers3.2.weight True\n",
      "decoder.moduleLayers.2._layers3.3.conv.weight True\n",
      "decoder.moduleLayers.2._layers3.3.conv.bias True\n",
      "decoder.moduleLayers.2._layers3.4.weight True\n",
      "decoder.moduleLayers.2._layers3.4.bias True\n",
      "decoder.moduleLayers.2._layers3.5.weight True\n",
      "decoder.moduleLayers.2._layers3.6.weight True\n",
      "decoder.moduleLayers.3._layers.1.conv.weight True\n",
      "decoder.moduleLayers.3._layers.1.conv.bias True\n",
      "decoder.moduleLayers.3._layers.2.weight True\n",
      "decoder.moduleLayers.3._layers.2.bias True\n",
      "decoder.moduleLayers.3._layers.3.weight True\n",
      "decoder.moduleLayers.3._layers.4.conv.weight True\n",
      "decoder.moduleLayers.3._layers.4.conv.bias True\n",
      "decoder.moduleLayers.3._layers.5.weight True\n",
      "decoder.moduleLayers.3._layers.5.bias True\n",
      "decoder.moduleLayers.3._layers.6.weight True\n",
      "decoder.moduleLayers.3._layers2.0.conv.weight True\n",
      "decoder.moduleLayers.3._layers2.0.conv.bias True\n",
      "decoder.moduleLayers.3._layers2.1.weight True\n",
      "decoder.moduleLayers.3._layers2.1.bias True\n",
      "decoder.moduleLayers.3._layers2.2.weight True\n",
      "decoder.moduleLayers.3._layers2.3.conv.weight True\n",
      "decoder.moduleLayers.3._layers2.3.conv.bias True\n",
      "decoder.moduleLayers.3._layers2.4.weight True\n",
      "decoder.moduleLayers.3._layers2.4.bias True\n",
      "decoder.moduleLayers.3._layers2.5.weight True\n",
      "decoder.moduleLayers.3._layers2.6.conv.weight True\n",
      "decoder.moduleLayers.3._layers2.6.conv.bias True\n",
      "decoder.moduleLayers.3._layers2.7.weight True\n",
      "decoder.moduleLayers.3._layers3.0.conv.weight True\n",
      "decoder.moduleLayers.3._layers3.0.conv.bias True\n",
      "decoder.moduleLayers.3._layers3.1.weight True\n",
      "decoder.moduleLayers.3._layers3.1.bias True\n",
      "decoder.moduleLayers.3._layers3.2.weight True\n",
      "decoder.moduleLayers.3._layers3.3.conv.weight True\n",
      "decoder.moduleLayers.3._layers3.3.conv.bias True\n",
      "decoder.moduleLayers.3._layers3.4.weight True\n",
      "decoder.moduleLayers.3._layers3.4.bias True\n",
      "decoder.moduleLayers.3._layers3.5.weight True\n",
      "decoder.moduleLayers.3._layers3.6.conv.weight True\n",
      "decoder.moduleLayers.3._layers3.6.conv.bias True\n",
      "decoder.moduleLayers.3._layers3.7.weight True\n",
      "decoder.subdecs.0.seq.1.conv.weight True\n",
      "decoder.subdecs.0.seq.1.conv.bias True\n",
      "decoder.subdecs.0.query.weight True\n",
      "decoder.subdecs.0.value.weight True\n",
      "decoder.subdecs.0.linear.weight True\n",
      "decoder.subdecs.1.seq.1.conv.weight True\n",
      "decoder.subdecs.1.seq.1.conv.bias True\n",
      "decoder.subdecs.1.query.weight True\n",
      "decoder.subdecs.1.value.weight True\n",
      "decoder.subdecs.1.linear.weight True\n",
      "decoder.subdecs.2.seq.1.conv.weight True\n",
      "decoder.subdecs.2.seq.1.conv.bias True\n",
      "decoder.subdecs.2.query.weight True\n",
      "decoder.subdecs.2.value.weight True\n",
      "decoder.subdecs.2.linear.weight True\n",
      "prior._weight_dict.01 False\n",
      "prior._weight_dict.02 False\n",
      "prior._weight_dict.03 False\n",
      "prior._weight_dict.12 False\n",
      "prior._weight_dict.13 False\n",
      "prior._weight_dict.23 False\n",
      "prior._bias_dict.0 False\n",
      "prior._bias_dict.1 False\n",
      "prior._bias_dict.2 False\n",
      "prior._bias_dict.3 False\n",
      "prior._weight_mask_dict.01 False\n",
      "prior._weight_mask_dict.02 False\n",
      "prior._weight_mask_dict.03 False\n",
      "prior._weight_mask_dict.12 False\n",
      "prior._weight_mask_dict.13 False\n",
      "prior._weight_mask_dict.23 False\n"
     ]
    }
   ],
   "source": [
    "new_model = True\n",
    "if new_model:\n",
    "    self = setup_model(config)\n",
    "    # self.model = self.model.double()  # sets all model parameters to float64\n",
    "else:\n",
    "    self = load_model_instance(config)\n",
    "    # self.model = self.model.double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31643beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model.decoder.transformer import Multiheadv2\n",
    "from model.decoder.decoderhierarchy0 import PeriodicConvTranspose3d\n",
    "import torch.nn as nn\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "33a50412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#D1, D2, D3\n",
    "class DecoderAtt(nn.Module):\n",
    "    def __init__(self, cfg, input_size):\n",
    "        super(DecoderAtt, self).__init__()\n",
    "        self._config = cfg\n",
    "\n",
    "        self.n_latent_hierarchy_lvls=self._config.rbm.partitions\n",
    "\n",
    "        self.n_latent_nodes=self._config.rbm.latent_nodes_per_p * self._config.rbm.partitions\n",
    "\n",
    "        self.z = self._config.data.z\n",
    "        self.r = self._config.data.r\n",
    "        self.phi = self._config.data.phi\n",
    "\n",
    "        # output_size_z = int( output_size / (self.r * self.phi))\n",
    "\n",
    "        self._layers =  nn.Sequential(\n",
    "                   nn.Unflatten(1, (input_size, 1, 1, 1)),\n",
    "\n",
    "                   PeriodicConvTranspose3d(input_size, 512, (3,3,2), (1,1,1), 0),\n",
    "                   nn.BatchNorm3d(512),\n",
    "                   nn.PReLU(512, 0.02),\n",
    "                   \n",
    "\n",
    "                   PeriodicConvTranspose3d(512, 128, (3,3,2), (1,1,1), 0),\n",
    "                   nn.BatchNorm3d(128),\n",
    "                   nn.PReLU(128, 0.02),\n",
    "                                   )\n",
    "        \n",
    "        self._layers2 = nn.Sequential(\n",
    "                   PeriodicConvTranspose3d(129, 64, (3,3,2), (1,1,1), 1),\n",
    "                   nn.BatchNorm3d(64),\n",
    "                   nn.PReLU(64, 0.02),\n",
    "\n",
    "                   PeriodicConvTranspose3d(64, 32, (2,2,2), (1,1,1), 1),\n",
    "                   nn.BatchNorm3d(32),\n",
    "                   nn.PReLU(32, 1.0),\n",
    "\n",
    "                #    PeriodicConvTranspose3d(32, 1, (5,3,3), (1,1,1), 0),\n",
    "                #    PeriodicConv3d(1, 1, (self.z - output_size_z + 1, 1, 1), (1,1,1), 0),\n",
    "                   nn.PReLU(1, 1.0)\n",
    "                                   )\n",
    "        \n",
    "        self._layers3 = nn.Sequential(\n",
    "                   PeriodicConvTranspose3d(129, 64, (3,3,2), (1,1,1), 1),\n",
    "                   nn.BatchNorm3d(64),\n",
    "                   nn.PReLU(64, 0.02),\n",
    "\n",
    "                   PeriodicConvTranspose3d(64, 32, (2,2,2), (1,1,1), 1),\n",
    "                   nn.BatchNorm3d(32),\n",
    "                   nn.PReLU(32, 0.02),\n",
    "\n",
    "                #    PeriodicConvTranspose3d(32, 1, (5,3,3), (1,1,1), 0),\n",
    "                #    PeriodicConv3d(1, 1, (self.z - output_size_z + 1, 1, 1), (1,1,1), 0),\n",
    "                   nn.PReLU(1, 0.02),\n",
    "                                   )\n",
    "        \n",
    "    def forward(self, x, x0):\n",
    "                \n",
    "        x = self._layers(x)\n",
    "        x0 = self.trans_energy(x0)\n",
    "        xx0 = torch.cat((x, x0.unsqueeze(2).unsqueeze(3).unsqueeze(4).repeat(1,1,torch.tensor(x.shape[-3:-2]).item(),torch.tensor(x.shape[-2:-1]).item(), torch.tensor(x.shape[-1:]).item())), 1)\n",
    "        x1 = self._layers2(xx0) #hits\n",
    "        x2 = self._layers3(xx0)\n",
    "        return rearrange(x1, \"b c l h w -> b (l h w) c\"), rearrange(x1, \"b c l h w -> b (l h w) c\")\n",
    "    \n",
    "    def trans_energy(self, x0, log_e_max=14.0, log_e_min=6.0, s_map = 1.0):\n",
    "        return ((torch.log(x0) - log_e_min)/(log_e_max - log_e_min)) * s_map\n",
    "    \n",
    "class Skip(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(Skip, self).__init__()\n",
    "        self._config = cfg\n",
    "        self.head_size = self._config.model.head_size\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Unflatten(1, (self._config.rbm.latent_nodes_per_p*2,1,1,1)),\n",
    "            PeriodicConvTranspose3d(self._config.rbm.latent_nodes_per_p*2, self.head_size,(3,3,3),(1,1,1),0),\n",
    "        )\n",
    "        self.query = nn.Linear(27,self._config.model.skip_output_size, bias=False)\n",
    "        self.value = nn.Linear(27,self._config.model.skip_output_size, bias=False)\n",
    "        self.linear = nn.Linear(self.head_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, keys):\n",
    "        x = self.seq(x)\n",
    "        x = rearrange(x, \"b c l h w -> b c (l h w)\")\n",
    "        x_query = self.query(x).transpose(-2,-1)\n",
    "        x_value = self.value(x).transpose(-2,-1)\n",
    "\n",
    "        wei = x_query @ keys.transpose(-2,-1) * self.head_size**-0.5\n",
    "        wei = F.softmax(wei,dim=-1)\n",
    "        out = self.linear(wei @ x_value).reshape(-1,self._config.model.skip_output_size)\n",
    "\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, cfg, input_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self._config = cfg\n",
    "\n",
    "        self.n_latent_hierarchy_lvls=self._config.rbm.partitions\n",
    "\n",
    "        self.n_latent_nodes=self._config.rbm.latent_nodes_per_p * self._config.rbm.partitions\n",
    "\n",
    "        self.z = self._config.data.z\n",
    "        self.r = self._config.data.r\n",
    "        self.phi = self._config.data.phi\n",
    "        \n",
    "        self._layers =  nn.Sequential(\n",
    "                   nn.Unflatten(1, (input_size, 1, 1, 1)),\n",
    "\n",
    "                   PeriodicConvTranspose3d(input_size, 512, (3,3,2), (2,1,1), 0),\n",
    "                   nn.BatchNorm3d(512),\n",
    "                   nn.PReLU(512, 0.02),\n",
    "                   \n",
    "\n",
    "                   PeriodicConvTranspose3d(512, 128, (5,4,2), (2,1,1), 0),\n",
    "                   nn.BatchNorm3d(128),\n",
    "                   nn.PReLU(128, 0.02),\n",
    "                                   )\n",
    "        \n",
    "        self._layers2 = nn.Sequential(\n",
    "                   PeriodicConvTranspose3d(129, 64, (3,3,2), (2,1,1), 1),\n",
    "                   nn.BatchNorm3d(64),\n",
    "                   nn.PReLU(64, 0.02),\n",
    "\n",
    "                   PeriodicConvTranspose3d(64, 32, (5,3,2), (2,1,1), 1),\n",
    "                   nn.BatchNorm3d(32),\n",
    "                   nn.PReLU(32, 1.0),\n",
    "\n",
    "                   PeriodicConvTranspose3d(32, 1, (5,3,3), (1,1,1), 0),\n",
    "                   nn.PReLU(1, 1.0)\n",
    "                                   )\n",
    "        \n",
    "        self._layers3 = nn.Sequential(\n",
    "                   PeriodicConvTranspose3d(129, 64, (3,3,2), (2,1,1), 1),\n",
    "                   nn.BatchNorm3d(64),\n",
    "                   nn.PReLU(64, 0.02),\n",
    "\n",
    "                   PeriodicConvTranspose3d(64, 32, (5,3,2), (2,1,1), 1),\n",
    "                   nn.BatchNorm3d(32),\n",
    "                   nn.PReLU(32, 0.02),\n",
    "\n",
    "                   PeriodicConvTranspose3d(32, 1, (5,3,3), (1,1,1), 0),\n",
    "                   nn.PReLU(1, 0.02),\n",
    "                                   )\n",
    "        \n",
    "    def forward(self, x, x0):\n",
    "                \n",
    "        x = self._layers(x)\n",
    "        x0 = self.trans_energy(x0)\n",
    "        xx0 = torch.cat((x, x0.unsqueeze(2).unsqueeze(3).unsqueeze(4).repeat(1,1,torch.tensor(x.shape[-3:-2]).item(),torch.tensor(x.shape[-2:-1]).item(), torch.tensor(x.shape[-1:]).item())), 1)\n",
    "        x1 = self._layers2(xx0) #hits\n",
    "        x2 = self._layers3(xx0)\n",
    "        return x1.reshape(x1.shape[0],self.z*self.r*self.phi), x2.reshape(x1.shape[0],self.z*self.r*self.phi)\n",
    "    \n",
    "    def trans_energy(self, x0, log_e_max=16.0, log_e_min=5.0, s_map = 1.0):\n",
    "        return ((torch.log(x0) - log_e_min)/(log_e_max - log_e_min)) * s_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "613c2253",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[180], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m d \u001b[38;5;241m=\u001b[39m Decoder(config, config\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder_input[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# sk = Skip(config)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43md\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4832\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "d = Decoder(config, config.model.decoder_input[-1])\n",
    "# sk = Skip(config)\n",
    "d(torch.rand(2,4832), torch.rand(2,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f5e9fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(d(torch.rand(2,1208), torch.rand(2,1))[0].shape)\n",
    "# print(sk(torch.rand(2,604))[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96ec0bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = d(torch.rand(2,1208), torch.rand(2,1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f64a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DecoderHierarchyTF(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(DecoderHierarchyTF, self).__init__()\n",
    "        self._config = cfg\n",
    "        self.head_size = self._config.model.head_size\n",
    "        self._create_hierarchy_network()\n",
    "        self._create_skipcon_decoders()\n",
    "\n",
    "    def _create_hierarchy_network(self):\n",
    "        self.latent_nodes = self._config.rbm.latent_nodes_per_p * self._config.rbm.partitions\n",
    "        self.hierarchical_lvls = self._config.rbm.partitions\n",
    "\n",
    "        inp_layers = self._config.model.decoder_input\n",
    "\n",
    "        self.moduleLayers = nn.ModuleList([])\n",
    "        for i in range(self.hierarchical_lvls-1):\n",
    "            self.moduleLayers.append(DecoderAtt(self._config, inp_layers[i])) \n",
    "        self.moduleLayers.append(Decoder(self._config, inp_layers[-1]))   \n",
    "\n",
    "    def _create_skipcon_decoders(self):\n",
    "        self.lnpp = self._config.rbm.latent_nodes_per_p\n",
    "        self.subdecs = nn.ModuleList([])\n",
    "        for i in range(self.hierarchical_lvls-1):\n",
    "            self.subdecs.append(Skip(config))\n",
    "    \n",
    "    def forward(self, z, x0):\n",
    "        z_prime = z\n",
    "        for i in range(len(self.moduleLayers)-1):\n",
    "            x1, x2 = self.moduleLayers[i](z_prime, x0)\n",
    "            keys = x1 * x2\n",
    "            z_skip = torch.cat((z_prime[:,:self.lnpp], z_prime[:,self.lnpp*(3-i):self.lnpp*(4-i)]), dim=1)\n",
    "\n",
    "            out = self.subdecs[i](z_skip, keys)\n",
    "            z_prime = torch.cat((z_prime,z),dim=1)\n",
    "                \n",
    "        x1, x2 = self.moduleLayers[-1](z_prime, x0)\n",
    "        return x1,x2,out,z_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "9301dd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4832"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dev = set_device(config)\n",
    "dh._config.model.decoder_input[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "a4309cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = DecoderHierarchyTF(config)#.to(dev)\n",
    "# dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "4634edf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([2, 672, 32]) torch.Size([2, 672, 32])\n",
      "torch.Size([2, 604]) torch.Size([2, 672, 32])\n",
      "torch.Size([2, 672]) torch.Size([2, 2416])\n",
      "1 torch.Size([2, 672, 32]) torch.Size([2, 672, 32])\n",
      "torch.Size([2, 604]) torch.Size([2, 672, 32])\n",
      "torch.Size([2, 672]) torch.Size([2, 3624])\n",
      "2 torch.Size([2, 672, 32]) torch.Size([2, 672, 32])\n",
      "torch.Size([2, 604]) torch.Size([2, 672, 32])\n",
      "torch.Size([2, 672]) torch.Size([2, 4832])\n",
      "Final z_prime shape: torch.Size([2, 4832])\n"
     ]
    }
   ],
   "source": [
    "x1,x2,out,z_prime = dh(torch.rand(2,1208), torch.rand(2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4e414276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6480]),\n",
       " torch.Size([2, 6480]),\n",
       " torch.Size([2, 672]),\n",
       " torch.Size([2, 4832]))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape, x2.shape, out.shape, z_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ceaf9c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 672, 1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(32,1)(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb64b290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1208])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[:,0:302*4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f0ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
