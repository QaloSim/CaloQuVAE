{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d0e1478",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[19:31:02.440]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mCaloQuVAE                                         \u001b[0mLoading configuration.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "from hydra.utils import instantiate\n",
    "from hydra import initialize, compose\n",
    "import hydra\n",
    "\n",
    "import wandb\n",
    "\n",
    "from data.dataManager import DataManager\n",
    "from model.modelCreator import ModelCreator\n",
    "from omegaconf import OmegaConf\n",
    "from scripts.run import setup_model, load_model_instance\n",
    "\n",
    "from utils.plots import vae_plots\n",
    "from utils.rbm_plots import plot_rbm_histogram, plot_rbm_params\n",
    "\n",
    "from scripts.run import set_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e48f8d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/dummy/dummy/runs/l8gz0xwl?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f16ab7d7aa0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=\"config\")\n",
    "config=compose(config_name=\"config.yaml\")\n",
    "wandb.init(tags = [config.data.dataset_name], project=config.wandb.project, entity=config.wandb.entity, config=OmegaConf.to_container(config, resolve=True), mode='disabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90abb182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m[19:27:35.341]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0mLoading other dataset: CaloChallenge2\n",
      "\u001b[1m[19:27:35.346]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0mKeys: ['incident_energies', 'showers']\n",
      "\u001b[1m[19:27:40.288]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0mdict_keys(['incident_energies', 'showers'])\n",
      "\u001b[1m[19:27:40.291]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0m<torch.utils.data.dataloader.DataLoader object at 0x7f19c3f5d580>: 79999 events, 157 batches\n",
      "\u001b[1m[19:27:40.292]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0m<torch.utils.data.dataloader.DataLoader object at 0x7f191d1698e0>: 10001 events, 10 batches\n",
      "\u001b[1m[19:27:40.292]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdata.dataManager                                  \u001b[0m<torch.utils.data.dataloader.DataLoader object at 0x7f1b9e7dc320>: 9999 events, 10 batches\n",
      "\u001b[1m[19:27:40.293]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mmodel.modelCreator                                \u001b[0m::Creating Model\n",
      "\u001b[1m[19:27:41.363]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mFetching definitions of all available solvers\n",
      "\u001b[1m[19:27:41.547]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mReceived solver data for 7 solver(s).\n",
      "\u001b[1m[19:27:41.814]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(id='Advantage_system4.1')\n",
      "\u001b[1m[19:27:41.866]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(id='Advantage_system6.4')\n",
      "\u001b[1m[19:27:41.917]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mdwave.cloud.client.base                           \u001b[0mAdding solver StructuredSolver(id='Advantage2_system1.4')\n",
      "\u001b[1m[19:27:42.430]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mscripts.run                                       \u001b[0mRequesting GPUs. GPU list :[1]\n",
      "\u001b[1m[19:27:42.431]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mscripts.run                                       \u001b[0mMain GPU : cuda:1\n",
      "\u001b[1m[19:27:42.607]\u001b[0m \u001b[1;95mINFO \u001b[1;0m  \u001b[1mscripts.run                                       \u001b[0mCUDA available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n",
      "encoder._networks.0.seq1.0.conv.weight True\n",
      "encoder._networks.0.seq1.0.conv.bias True\n",
      "encoder._networks.0.seq1.1.weight True\n",
      "encoder._networks.0.seq1.1.bias True\n",
      "encoder._networks.0.seq1.2.weight True\n",
      "encoder._networks.0.seq1.3.conv.weight True\n",
      "encoder._networks.0.seq1.3.conv.bias True\n",
      "encoder._networks.0.seq1.4.weight True\n",
      "encoder._networks.0.seq1.4.bias True\n",
      "encoder._networks.0.seq1.5.weight True\n",
      "encoder._networks.0.seq1.6.conv.weight True\n",
      "encoder._networks.0.seq1.6.conv.bias True\n",
      "encoder._networks.0.seq1.7.weight True\n",
      "encoder._networks.0.seq1.7.bias True\n",
      "encoder._networks.0.seq1.8.weight True\n",
      "encoder._networks.0.seq2.0.conv.weight True\n",
      "encoder._networks.0.seq2.0.conv.bias True\n",
      "encoder._networks.0.seq2.1.weight True\n",
      "encoder._networks.0.seq2.1.bias True\n",
      "encoder._networks.0.seq2.2.weight True\n",
      "encoder._networks.0.seq2.3.conv.weight True\n",
      "encoder._networks.0.seq2.3.conv.bias True\n",
      "encoder._networks.0.seq2.4.weight True\n",
      "encoder._networks.1.seq1.0.conv.weight True\n",
      "encoder._networks.1.seq1.0.conv.bias True\n",
      "encoder._networks.1.seq1.1.weight True\n",
      "encoder._networks.1.seq1.1.bias True\n",
      "encoder._networks.1.seq1.2.weight True\n",
      "encoder._networks.1.seq1.3.conv.weight True\n",
      "encoder._networks.1.seq1.3.conv.bias True\n",
      "encoder._networks.1.seq1.4.weight True\n",
      "encoder._networks.1.seq1.4.bias True\n",
      "encoder._networks.1.seq1.5.weight True\n",
      "encoder._networks.1.seq1.6.conv.weight True\n",
      "encoder._networks.1.seq1.6.conv.bias True\n",
      "encoder._networks.1.seq1.7.weight True\n",
      "encoder._networks.1.seq1.7.bias True\n",
      "encoder._networks.1.seq1.8.weight True\n",
      "encoder._networks.1.seq2.0.conv.weight True\n",
      "encoder._networks.1.seq2.0.conv.bias True\n",
      "encoder._networks.1.seq2.1.weight True\n",
      "encoder._networks.1.seq2.1.bias True\n",
      "encoder._networks.1.seq2.2.weight True\n",
      "encoder._networks.1.seq2.3.conv.weight True\n",
      "encoder._networks.1.seq2.3.conv.bias True\n",
      "encoder._networks.1.seq2.4.weight True\n",
      "encoder._networks.2.seq1.0.conv.weight True\n",
      "encoder._networks.2.seq1.0.conv.bias True\n",
      "encoder._networks.2.seq1.1.weight True\n",
      "encoder._networks.2.seq1.1.bias True\n",
      "encoder._networks.2.seq1.2.weight True\n",
      "encoder._networks.2.seq1.3.conv.weight True\n",
      "encoder._networks.2.seq1.3.conv.bias True\n",
      "encoder._networks.2.seq1.4.weight True\n",
      "encoder._networks.2.seq1.4.bias True\n",
      "encoder._networks.2.seq1.5.weight True\n",
      "encoder._networks.2.seq1.6.conv.weight True\n",
      "encoder._networks.2.seq1.6.conv.bias True\n",
      "encoder._networks.2.seq1.7.weight True\n",
      "encoder._networks.2.seq1.7.bias True\n",
      "encoder._networks.2.seq1.8.weight True\n",
      "encoder._networks.2.seq2.0.conv.weight True\n",
      "encoder._networks.2.seq2.0.conv.bias True\n",
      "encoder._networks.2.seq2.1.weight True\n",
      "encoder._networks.2.seq2.1.bias True\n",
      "encoder._networks.2.seq2.2.weight True\n",
      "encoder._networks.2.seq2.3.conv.weight True\n",
      "encoder._networks.2.seq2.3.conv.bias True\n",
      "encoder._networks.2.seq2.4.weight True\n",
      "decoder.moduleLayers.0._layers.1.conv.weight True\n",
      "decoder.moduleLayers.0._layers.1.conv.bias True\n",
      "decoder.moduleLayers.0._layers.2.weight True\n",
      "decoder.moduleLayers.0._layers.2.bias True\n",
      "decoder.moduleLayers.0._layers.3.weight True\n",
      "decoder.moduleLayers.0._layers.4.conv.weight True\n",
      "decoder.moduleLayers.0._layers.4.conv.bias True\n",
      "decoder.moduleLayers.0._layers.5.weight True\n",
      "decoder.moduleLayers.0._layers.5.bias True\n",
      "decoder.moduleLayers.0._layers.6.weight True\n",
      "decoder.moduleLayers.0._layers2.0.conv.weight True\n",
      "decoder.moduleLayers.0._layers2.0.conv.bias True\n",
      "decoder.moduleLayers.0._layers2.1.weight True\n",
      "decoder.moduleLayers.0._layers2.1.bias True\n",
      "decoder.moduleLayers.0._layers2.2.weight True\n",
      "decoder.moduleLayers.0._layers2.3.conv.weight True\n",
      "decoder.moduleLayers.0._layers2.3.conv.bias True\n",
      "decoder.moduleLayers.0._layers2.4.weight True\n",
      "decoder.moduleLayers.0._layers2.4.bias True\n",
      "decoder.moduleLayers.0._layers2.5.weight True\n",
      "decoder.moduleLayers.0._layers2.6.weight True\n",
      "decoder.moduleLayers.0._layers3.0.conv.weight True\n",
      "decoder.moduleLayers.0._layers3.0.conv.bias True\n",
      "decoder.moduleLayers.0._layers3.1.weight True\n",
      "decoder.moduleLayers.0._layers3.1.bias True\n",
      "decoder.moduleLayers.0._layers3.2.weight True\n",
      "decoder.moduleLayers.0._layers3.3.conv.weight True\n",
      "decoder.moduleLayers.0._layers3.3.conv.bias True\n",
      "decoder.moduleLayers.0._layers3.4.weight True\n",
      "decoder.moduleLayers.0._layers3.4.bias True\n",
      "decoder.moduleLayers.0._layers3.5.weight True\n",
      "decoder.moduleLayers.0._layers3.6.weight True\n",
      "decoder.moduleLayers.1._layers.1.conv.weight True\n",
      "decoder.moduleLayers.1._layers.1.conv.bias True\n",
      "decoder.moduleLayers.1._layers.2.weight True\n",
      "decoder.moduleLayers.1._layers.2.bias True\n",
      "decoder.moduleLayers.1._layers.3.weight True\n",
      "decoder.moduleLayers.1._layers.4.conv.weight True\n",
      "decoder.moduleLayers.1._layers.4.conv.bias True\n",
      "decoder.moduleLayers.1._layers.5.weight True\n",
      "decoder.moduleLayers.1._layers.5.bias True\n",
      "decoder.moduleLayers.1._layers.6.weight True\n",
      "decoder.moduleLayers.1._layers2.0.conv.weight True\n",
      "decoder.moduleLayers.1._layers2.0.conv.bias True\n",
      "decoder.moduleLayers.1._layers2.1.weight True\n",
      "decoder.moduleLayers.1._layers2.1.bias True\n",
      "decoder.moduleLayers.1._layers2.2.weight True\n",
      "decoder.moduleLayers.1._layers2.3.conv.weight True\n",
      "decoder.moduleLayers.1._layers2.3.conv.bias True\n",
      "decoder.moduleLayers.1._layers2.4.weight True\n",
      "decoder.moduleLayers.1._layers2.4.bias True\n",
      "decoder.moduleLayers.1._layers2.5.weight True\n",
      "decoder.moduleLayers.1._layers2.6.weight True\n",
      "decoder.moduleLayers.1._layers3.0.conv.weight True\n",
      "decoder.moduleLayers.1._layers3.0.conv.bias True\n",
      "decoder.moduleLayers.1._layers3.1.weight True\n",
      "decoder.moduleLayers.1._layers3.1.bias True\n",
      "decoder.moduleLayers.1._layers3.2.weight True\n",
      "decoder.moduleLayers.1._layers3.3.conv.weight True\n",
      "decoder.moduleLayers.1._layers3.3.conv.bias True\n",
      "decoder.moduleLayers.1._layers3.4.weight True\n",
      "decoder.moduleLayers.1._layers3.4.bias True\n",
      "decoder.moduleLayers.1._layers3.5.weight True\n",
      "decoder.moduleLayers.1._layers3.6.weight True\n",
      "decoder.moduleLayers.2._layers.1.conv.weight True\n",
      "decoder.moduleLayers.2._layers.1.conv.bias True\n",
      "decoder.moduleLayers.2._layers.2.weight True\n",
      "decoder.moduleLayers.2._layers.2.bias True\n",
      "decoder.moduleLayers.2._layers.3.weight True\n",
      "decoder.moduleLayers.2._layers.4.conv.weight True\n",
      "decoder.moduleLayers.2._layers.4.conv.bias True\n",
      "decoder.moduleLayers.2._layers.5.weight True\n",
      "decoder.moduleLayers.2._layers.5.bias True\n",
      "decoder.moduleLayers.2._layers.6.weight True\n",
      "decoder.moduleLayers.2._layers2.0.conv.weight True\n",
      "decoder.moduleLayers.2._layers2.0.conv.bias True\n",
      "decoder.moduleLayers.2._layers2.1.weight True\n",
      "decoder.moduleLayers.2._layers2.1.bias True\n",
      "decoder.moduleLayers.2._layers2.2.weight True\n",
      "decoder.moduleLayers.2._layers2.3.conv.weight True\n",
      "decoder.moduleLayers.2._layers2.3.conv.bias True\n",
      "decoder.moduleLayers.2._layers2.4.weight True\n",
      "decoder.moduleLayers.2._layers2.4.bias True\n",
      "decoder.moduleLayers.2._layers2.5.weight True\n",
      "decoder.moduleLayers.2._layers2.6.weight True\n",
      "decoder.moduleLayers.2._layers3.0.conv.weight True\n",
      "decoder.moduleLayers.2._layers3.0.conv.bias True\n",
      "decoder.moduleLayers.2._layers3.1.weight True\n",
      "decoder.moduleLayers.2._layers3.1.bias True\n",
      "decoder.moduleLayers.2._layers3.2.weight True\n",
      "decoder.moduleLayers.2._layers3.3.conv.weight True\n",
      "decoder.moduleLayers.2._layers3.3.conv.bias True\n",
      "decoder.moduleLayers.2._layers3.4.weight True\n",
      "decoder.moduleLayers.2._layers3.4.bias True\n",
      "decoder.moduleLayers.2._layers3.5.weight True\n",
      "decoder.moduleLayers.2._layers3.6.weight True\n",
      "decoder.moduleLayers.3._layers.1.conv.weight True\n",
      "decoder.moduleLayers.3._layers.1.conv.bias True\n",
      "decoder.moduleLayers.3._layers.2.weight True\n",
      "decoder.moduleLayers.3._layers.2.bias True\n",
      "decoder.moduleLayers.3._layers.3.weight True\n",
      "decoder.moduleLayers.3._layers.4.conv.weight True\n",
      "decoder.moduleLayers.3._layers.4.conv.bias True\n",
      "decoder.moduleLayers.3._layers.5.weight True\n",
      "decoder.moduleLayers.3._layers.5.bias True\n",
      "decoder.moduleLayers.3._layers.6.weight True\n",
      "decoder.moduleLayers.3._layers2.0.conv.weight True\n",
      "decoder.moduleLayers.3._layers2.0.conv.bias True\n",
      "decoder.moduleLayers.3._layers2.1.weight True\n",
      "decoder.moduleLayers.3._layers2.1.bias True\n",
      "decoder.moduleLayers.3._layers2.2.weight True\n",
      "decoder.moduleLayers.3._layers2.3.conv.weight True\n",
      "decoder.moduleLayers.3._layers2.3.conv.bias True\n",
      "decoder.moduleLayers.3._layers2.4.weight True\n",
      "decoder.moduleLayers.3._layers2.4.bias True\n",
      "decoder.moduleLayers.3._layers2.5.weight True\n",
      "decoder.moduleLayers.3._layers2.6.conv.weight True\n",
      "decoder.moduleLayers.3._layers2.6.conv.bias True\n",
      "decoder.moduleLayers.3._layers2.7.weight True\n",
      "decoder.moduleLayers.3._layers3.0.conv.weight True\n",
      "decoder.moduleLayers.3._layers3.0.conv.bias True\n",
      "decoder.moduleLayers.3._layers3.1.weight True\n",
      "decoder.moduleLayers.3._layers3.1.bias True\n",
      "decoder.moduleLayers.3._layers3.2.weight True\n",
      "decoder.moduleLayers.3._layers3.3.conv.weight True\n",
      "decoder.moduleLayers.3._layers3.3.conv.bias True\n",
      "decoder.moduleLayers.3._layers3.4.weight True\n",
      "decoder.moduleLayers.3._layers3.4.bias True\n",
      "decoder.moduleLayers.3._layers3.5.weight True\n",
      "decoder.moduleLayers.3._layers3.6.conv.weight True\n",
      "decoder.moduleLayers.3._layers3.6.conv.bias True\n",
      "decoder.moduleLayers.3._layers3.7.weight True\n",
      "decoder.subdecs.0.seq.1.conv.weight True\n",
      "decoder.subdecs.0.seq.1.conv.bias True\n",
      "decoder.subdecs.0.query.weight True\n",
      "decoder.subdecs.0.value.weight True\n",
      "decoder.subdecs.0.linear.weight True\n",
      "decoder.subdecs.1.seq.1.conv.weight True\n",
      "decoder.subdecs.1.seq.1.conv.bias True\n",
      "decoder.subdecs.1.query.weight True\n",
      "decoder.subdecs.1.value.weight True\n",
      "decoder.subdecs.1.linear.weight True\n",
      "decoder.subdecs.2.seq.1.conv.weight True\n",
      "decoder.subdecs.2.seq.1.conv.bias True\n",
      "decoder.subdecs.2.query.weight True\n",
      "decoder.subdecs.2.value.weight True\n",
      "decoder.subdecs.2.linear.weight True\n",
      "prior._weight_dict.01 False\n",
      "prior._weight_dict.02 False\n",
      "prior._weight_dict.03 False\n",
      "prior._weight_dict.12 False\n",
      "prior._weight_dict.13 False\n",
      "prior._weight_dict.23 False\n",
      "prior._bias_dict.0 False\n",
      "prior._bias_dict.1 False\n",
      "prior._bias_dict.2 False\n",
      "prior._bias_dict.3 False\n",
      "prior._weight_mask_dict.01 False\n",
      "prior._weight_mask_dict.02 False\n",
      "prior._weight_mask_dict.03 False\n",
      "prior._weight_mask_dict.12 False\n",
      "prior._weight_mask_dict.13 False\n",
      "prior._weight_mask_dict.23 False\n"
     ]
    }
   ],
   "source": [
    "new_model = True\n",
    "if new_model:\n",
    "    self = setup_model(config)\n",
    "    # self.model = self.model.double()  # sets all model parameters to float64\n",
    "else:\n",
    "    self = load_model_instance(config)\n",
    "    # self.model = self.model.double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddf1ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    self.model.decoder(torch.rand(2,1208), torch.rand(2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31643beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model.decoder.transformer import Multiheadv2\n",
    "from model.decoder.decoderhierarchy0 import PeriodicConvTranspose3d\n",
    "import torch.nn as nn\n",
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33a50412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#D1, D2, D3\n",
    "class DecoderAtt(nn.Module):\n",
    "    def __init__(self, cfg, input_size):\n",
    "        super(DecoderAtt, self).__init__()\n",
    "        self._config = cfg\n",
    "\n",
    "        self.n_latent_hierarchy_lvls=self._config.rbm.partitions\n",
    "\n",
    "        self.n_latent_nodes=self._config.rbm.latent_nodes_per_p * self._config.rbm.partitions\n",
    "\n",
    "        self.z = self._config.data.z\n",
    "        self.r = self._config.data.r\n",
    "        self.phi = self._config.data.phi\n",
    "\n",
    "        # output_size_z = int( output_size / (self.r * self.phi))\n",
    "\n",
    "        self._layers =  nn.Sequential(\n",
    "                   nn.Unflatten(1, (input_size, 1, 1, 1)),\n",
    "\n",
    "                   PeriodicConvTranspose3d(input_size, 512, (3,3,2), (1,1,1), 0),\n",
    "                   nn.BatchNorm3d(512),\n",
    "                   nn.PReLU(512, 0.02),\n",
    "                   \n",
    "\n",
    "                   PeriodicConvTranspose3d(512, 128, (3,3,2), (1,1,1), 0),\n",
    "                   nn.BatchNorm3d(128),\n",
    "                   nn.PReLU(128, 0.02),\n",
    "                                   )\n",
    "        \n",
    "        self._layers2 = nn.Sequential(\n",
    "                   PeriodicConvTranspose3d(129, 64, (3,3,2), (1,1,1), 1),\n",
    "                   nn.BatchNorm3d(64),\n",
    "                   nn.PReLU(64, 0.02),\n",
    "\n",
    "                   PeriodicConvTranspose3d(64, 32, (2,2,2), (1,1,1), 1),\n",
    "                   nn.BatchNorm3d(32),\n",
    "                   nn.PReLU(32, 1.0),\n",
    "\n",
    "                #    PeriodicConvTranspose3d(32, 1, (5,3,3), (1,1,1), 0),\n",
    "                #    PeriodicConv3d(1, 1, (self.z - output_size_z + 1, 1, 1), (1,1,1), 0),\n",
    "                   nn.PReLU(1, 1.0)\n",
    "                                   )\n",
    "        \n",
    "        self._layers3 = nn.Sequential(\n",
    "                   PeriodicConvTranspose3d(129, 64, (3,3,2), (1,1,1), 1),\n",
    "                   nn.BatchNorm3d(64),\n",
    "                   nn.PReLU(64, 0.02),\n",
    "\n",
    "                   PeriodicConvTranspose3d(64, 32, (2,2,2), (1,1,1), 1),\n",
    "                   nn.BatchNorm3d(32),\n",
    "                   nn.PReLU(32, 0.02),\n",
    "\n",
    "                #    PeriodicConvTranspose3d(32, 1, (5,3,3), (1,1,1), 0),\n",
    "                #    PeriodicConv3d(1, 1, (self.z - output_size_z + 1, 1, 1), (1,1,1), 0),\n",
    "                   nn.PReLU(1, 0.02),\n",
    "                                   )\n",
    "        \n",
    "    def forward(self, x, x0):\n",
    "                \n",
    "        x = self._layers(x)\n",
    "        x0 = self.trans_energy(x0)\n",
    "        xx0 = torch.cat((x, x0.unsqueeze(2).unsqueeze(3).unsqueeze(4).repeat(1,1,torch.tensor(x.shape[-3:-2]).item(),torch.tensor(x.shape[-2:-1]).item(), torch.tensor(x.shape[-1:]).item())), 1)\n",
    "        x1 = self._layers2(xx0) #hits\n",
    "        x2 = self._layers3(xx0)\n",
    "        return rearrange(x1, \"b c l h w -> b (l h w) c\"), rearrange(x1, \"b c l h w -> b (l h w) c\")\n",
    "    \n",
    "    def trans_energy(self, x0, log_e_max=14.0, log_e_min=6.0, s_map = 1.0):\n",
    "        return ((torch.log(x0) - log_e_min)/(log_e_max - log_e_min)) * s_map\n",
    "    \n",
    "class Skip(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(Skip, self).__init__()\n",
    "        self._config = cfg\n",
    "        self.head_size = self._config.model.head_size\n",
    "        self.seq = nn.Sequential(\n",
    "            nn.Unflatten(1, (self._config.rbm.latent_nodes_per_p*2,1,1,1)),\n",
    "            PeriodicConvTranspose3d(self._config.rbm.latent_nodes_per_p*2, self.head_size,(3,3,3),(1,1,1),0),\n",
    "        )\n",
    "        self.query = nn.Linear(27,self._config.model.skip_output_size, bias=False)\n",
    "        self.value = nn.Linear(27,self._config.model.skip_output_size, bias=False)\n",
    "        self.linear = nn.Linear(self.head_size, 1, bias=False)\n",
    "\n",
    "    def forward(self, x, keys):\n",
    "        x = self.seq(x)\n",
    "        x = rearrange(x, \"b c l h w -> b c (l h w)\")\n",
    "        x_query = self.query(x).transpose(-2,-1)\n",
    "        x_value = self.value(x).transpose(-2,-1)\n",
    "\n",
    "        wei = x_query @ keys.transpose(-2,-1) * self.head_size**-0.5\n",
    "        wei = F.softmax(wei,dim=-1)\n",
    "        out = self.linear(wei @ x_value).reshape(-1,self._config.model.skip_output_size)\n",
    "\n",
    "        \n",
    "        return out\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, cfg, input_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self._config = cfg\n",
    "\n",
    "        self.n_latent_hierarchy_lvls=self._config.rbm.partitions\n",
    "\n",
    "        self.n_latent_nodes=self._config.rbm.latent_nodes_per_p * self._config.rbm.partitions\n",
    "\n",
    "        self.z = self._config.data.z\n",
    "        self.r = self._config.data.r\n",
    "        self.phi = self._config.data.phi\n",
    "        \n",
    "        self._layers =  nn.Sequential(\n",
    "                   nn.Unflatten(1, (input_size, 1, 1, 1)),\n",
    "\n",
    "                   PeriodicConvTranspose3d(input_size, 512, (3,3,2), (2,1,1), 0),\n",
    "                   nn.BatchNorm3d(512),\n",
    "                   nn.PReLU(512, 0.02),\n",
    "                   \n",
    "\n",
    "                   PeriodicConvTranspose3d(512, 128, (5,4,2), (2,1,1), 0),\n",
    "                   nn.BatchNorm3d(128),\n",
    "                   nn.PReLU(128, 0.02),\n",
    "                                   )\n",
    "        \n",
    "        self._layers2 = nn.Sequential(\n",
    "                   PeriodicConvTranspose3d(129, 64, (3,3,2), (2,1,1), 1),\n",
    "                   nn.BatchNorm3d(64),\n",
    "                   nn.PReLU(64, 0.02),\n",
    "\n",
    "                   PeriodicConvTranspose3d(64, 32, (5,3,2), (2,1,1), 1),\n",
    "                   nn.BatchNorm3d(32),\n",
    "                   nn.PReLU(32, 1.0),\n",
    "\n",
    "                   PeriodicConvTranspose3d(32, 1, (5,3,3), (1,1,1), 0),\n",
    "                   nn.PReLU(1, 1.0)\n",
    "                                   )\n",
    "        \n",
    "        self._layers3 = nn.Sequential(\n",
    "                   PeriodicConvTranspose3d(129, 64, (3,3,2), (2,1,1), 1),\n",
    "                   nn.BatchNorm3d(64),\n",
    "                   nn.PReLU(64, 0.02),\n",
    "\n",
    "                   PeriodicConvTranspose3d(64, 32, (5,3,2), (2,1,1), 1),\n",
    "                   nn.BatchNorm3d(32),\n",
    "                   nn.PReLU(32, 0.02),\n",
    "\n",
    "                   PeriodicConvTranspose3d(32, 1, (5,3,3), (1,1,1), 0),\n",
    "                   nn.PReLU(1, 0.02),\n",
    "                                   )\n",
    "        \n",
    "    def forward(self, x, x0):\n",
    "                \n",
    "        x = self._layers(x)\n",
    "        x0 = self.trans_energy(x0)\n",
    "        xx0 = torch.cat((x, x0.unsqueeze(2).unsqueeze(3).unsqueeze(4).repeat(1,1,torch.tensor(x.shape[-3:-2]).item(),torch.tensor(x.shape[-2:-1]).item(), torch.tensor(x.shape[-1:]).item())), 1)\n",
    "        x1 = self._layers2(xx0) #hits\n",
    "        x2 = self._layers3(xx0)\n",
    "        return x1.reshape(x1.shape[0],self.z*self.r*self.phi), x2.reshape(x1.shape[0],self.z*self.r*self.phi)\n",
    "    \n",
    "    def trans_energy(self, x0, log_e_max=16.0, log_e_min=5.0, s_map = 1.0):\n",
    "        return ((torch.log(x0) - log_e_min)/(log_e_max - log_e_min)) * s_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "613c2253",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[180], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m d \u001b[38;5;241m=\u001b[39m Decoder(config, config\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdecoder_input[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# sk = Skip(config)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43md\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4832\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "d = Decoder(config, config.model.decoder_input[-1])\n",
    "# sk = Skip(config)\n",
    "d(torch.rand(2,4832), torch.rand(2,1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f5e9fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(d(torch.rand(2,1208), torch.rand(2,1))[0].shape)\n",
    "# print(sk(torch.rand(2,604))[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "96ec0bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = d(torch.rand(2,1208), torch.rand(2,1))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f64a82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class DecoderHierarchyTF(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(DecoderHierarchyTF, self).__init__()\n",
    "        self._config = cfg\n",
    "        self.head_size = self._config.model.head_size\n",
    "        self._create_hierarchy_network()\n",
    "        self._create_skipcon_decoders()\n",
    "\n",
    "    def _create_hierarchy_network(self):\n",
    "        self.latent_nodes = self._config.rbm.latent_nodes_per_p * self._config.rbm.partitions\n",
    "        self.hierarchical_lvls = self._config.rbm.partitions\n",
    "\n",
    "        inp_layers = self._config.model.decoder_input\n",
    "\n",
    "        self.moduleLayers = nn.ModuleList([])\n",
    "        for i in range(self.hierarchical_lvls-1):\n",
    "            self.moduleLayers.append(DecoderAtt(self._config, inp_layers[i])) \n",
    "        self.moduleLayers.append(Decoder(self._config, inp_layers[-1]))   \n",
    "\n",
    "    def _create_skipcon_decoders(self):\n",
    "        self.lnpp = self._config.rbm.latent_nodes_per_p\n",
    "        self.subdecs = nn.ModuleList([])\n",
    "        for i in range(self.hierarchical_lvls-1):\n",
    "            self.subdecs.append(Skip(config))\n",
    "    \n",
    "    def forward(self, z, x0):\n",
    "        z_prime = z\n",
    "        for i in range(len(self.moduleLayers)-1):\n",
    "            print(i)\n",
    "            x1, x2 = self.moduleLayers[i](z_prime, x0)\n",
    "            keys = x1 * x2\n",
    "            z_skip = torch.cat((z_prime[:,:self.lnpp], z_prime[:,self.lnpp*(3-i):self.lnpp*(4-i)]), dim=1)\n",
    "\n",
    "            out = self.subdecs[i](z_skip, keys)\n",
    "            z_prime = torch.cat((out,z),dim=1)\n",
    "            print(out.shape, z_prime.shape, z.shape)\n",
    "                \n",
    "        x1, x2 = self.moduleLayers[-1](z_prime, x0)\n",
    "        return x1,x2,out,z_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9301dd96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4832"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dev = set_device(config)\n",
    "dh._config.model.decoder_input[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4309cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = DecoderHierarchyTF(config)#.to(dev)\n",
    "# dh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4634edf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([2, 672]) torch.Size([2, 1208]) torch.Size([2, 1208])\n",
      "1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "unflatten: Provided sizes [2416, 1, 1, 1] don't multiply up to the size of dim 1 (1880) in the input tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x1,x2,out,z_prime \u001b[38;5;241m=\u001b[39m \u001b[43mdh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1208\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[18], line 32\u001b[0m, in \u001b[0;36mDecoderHierarchyTF.forward\u001b[0;34m(self, z, x0)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoduleLayers)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m---> 32\u001b[0m     x1, x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmoduleLayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_prime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     keys \u001b[38;5;241m=\u001b[39m x1 \u001b[38;5;241m*\u001b[39m x2\n\u001b[1;32m     34\u001b[0m     z_skip \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((z_prime[:,:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlnpp], z_prime[:,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlnpp\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m3\u001b[39m\u001b[38;5;241m-\u001b[39mi):\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlnpp\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m4\u001b[39m\u001b[38;5;241m-\u001b[39mi)]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[5], line 60\u001b[0m, in \u001b[0;36mDecoderAtt.forward\u001b[0;34m(self, x, x0)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, x0):\n\u001b[0;32m---> 60\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     x0 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrans_energy(x0)\n\u001b[1;32m     62\u001b[0m     xx0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((x, x0\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m,torch\u001b[38;5;241m.\u001b[39mtensor(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m.\u001b[39mitem(),torch\u001b[38;5;241m.\u001b[39mtensor(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mitem(), torch\u001b[38;5;241m.\u001b[39mtensor(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:])\u001b[38;5;241m.\u001b[39mitem())), \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/flatten.py:155\u001b[0m, in \u001b[0;36mUnflatten.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflattened_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py:1432\u001b[0m, in \u001b[0;36mTensor.unflatten\u001b[0;34m(self, dim, sizes)\u001b[0m\n\u001b[1;32m   1430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39munflatten(dim, sizes, names)\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1432\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msizes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: unflatten: Provided sizes [2416, 1, 1, 1] don't multiply up to the size of dim 1 (1880) in the input tensor"
     ]
    }
   ],
   "source": [
    "x1,x2,out,z_prime = dh(torch.rand(2,1208), torch.rand(2,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4e414276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 6480]),\n",
       " torch.Size([2, 6480]),\n",
       " torch.Size([2, 672]),\n",
       " torch.Size([2, 4832]))"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape, x2.shape, out.shape, z_prime.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ceaf9c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 672, 1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Linear(32,1)(out).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bb64b290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1208])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z[:,0:302*4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f0ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
