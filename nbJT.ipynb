{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e1478",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import torch\n",
    "from hydra.utils import instantiate\n",
    "from hydra import initialize, compose\n",
    "import hydra\n",
    "\n",
    "import wandb\n",
    "\n",
    "from data.dataManager import DataManager\n",
    "from model.modelCreator import ModelCreator\n",
    "from omegaconf import OmegaConf\n",
    "from scripts.run import setup_model, load_model_instance\n",
    "\n",
    "from utils.plots import vae_plots\n",
    "from utils.rbm_plots import plot_rbm_histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e48f8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize(version_base=None, config_path=\"config\")\n",
    "config=compose(config_name=\"config.yaml\")\n",
    "wandb.init(tags = [config.data.dataset_name], project=config.wandb.project, entity=config.wandb.entity, config=OmegaConf.to_container(config, resolve=True), mode='disabled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90abb182",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = True\n",
    "if new_model:\n",
    "    self = setup_model(config)\n",
    "    # self.model = self.model.double()  # sets all model parameters to float64\n",
    "else:\n",
    "    self = load_model_instance(config.config_path)\n",
    "    # self.model = self.model.double()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df06c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.model.double()\n",
    "self.fit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3633a775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.model.float()\n",
    "x,x0 = torch.rand(2,6480).to(self._device), torch.rand(2,1).to(self._device)\n",
    "beta, post_logits, post_samples = self.model.encoder(x,x0, 5.0)\n",
    "self.model.decode(post_samples, x, x0, beta, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914fabf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.fit(0)\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DecoderHierarchy0(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(DecoderHierarchy0, self).__init__()\n",
    "        self._config = cfg\n",
    "        self._create_hierarchy_network()\n",
    "        self._create_skipcon_decoders()\n",
    "\n",
    "    def _create_hierarchy_network(self):\n",
    "        self.latent_nodes = self._config.rbm.latent_nodes_per_p * 4\n",
    "        # change these variables for different HE decoder structures\n",
    "        # FOR THE MIRROR HD, LET 3 SUBDECODERS GENERATE z1', z2', z3', \n",
    "        # THEN LAST SUBDECODER GENERATES THE ENTIRE SHOWER\n",
    "        # self.n_layers_per_subdec = 11\n",
    "        # self.layer_step = self._config.model.n_layers_per_subdec*144\n",
    "         # varies depending on if last layer is > or < layer step\n",
    "        self.hierarchical_lvls = 4\n",
    "\n",
    "        inp_layers = self._config.model.decoder_input\n",
    "        out_layers = self._config.model.decoder_output\n",
    "\n",
    "        self.moduleLayers = nn.ModuleList([])\n",
    "        for i in range(len(inp_layers)):\n",
    "            self.moduleLayers.append(Decoder(self._config, inp_layers[i], out_layers[i]))\n",
    "\n",
    "        \n",
    "\n",
    "    def _create_skipcon_decoders(self):\n",
    "        latent_inp = 2 * self._config.rbm.latent_nodes_per_p\n",
    "        self.subdecs = nn.ModuleList([])\n",
    "        for i in range(len(self._config.model.decoder_output)-1):\n",
    "            recon_out = self.latent_nodes + self._config.model.decoder_output[i]\n",
    "            self.subdecs.append(nn.Conv3d(latent_inp, recon_out, kernel_size=1, stride=1, padding=0))\n",
    "    \n",
    "    def forward(self, x, x0):\n",
    "        x_lat = x\n",
    "        self.x1, self.x2 = torch.tensor([]).to(x.device), torch.tensor([]).to(x.device) # store hits and activation tensors\n",
    "        for lvl in range(len(self.moduleLayers)):\n",
    "            cur_net = self.moduleLayers[lvl]\n",
    "            output_hits, output_activations = cur_net(x, x0)\n",
    "            outputs = output_hits * output_activations\n",
    "            z = outputs\n",
    "            if lvl == len(self.moduleLayers) - 1:\n",
    "                self.x1 = output_hits\n",
    "                self.x2 = output_activations\n",
    "            else:\n",
    "                partition_ind_start = (len(self.moduleLayers) - 1 - lvl) * self._config.rbm.latent_nodes_per_p\n",
    "                partition_ind_end = (len(self.moduleLayers) - lvl) * self._config.rbm.latent_nodes_per_p\n",
    "                enc_z = torch.cat((x[:,0:self._config.rbm.latent_nodes_per_p], x[:,partition_ind_start:partition_ind_end]), dim=1)\n",
    "                # enc_z = x[:,partition_ind_start:partition_ind_end]\n",
    "                enc_z = torch.unflatten(enc_z, 1, (2 * self._config.rbm.latent_nodes_per_p, 1, 1, 1))\n",
    "                # enc_z = torch.unflatten(enc_z, 1, (self._config.model.n_latent_nodes_per_p, 1, 1, 1))\n",
    "                enc_z = self.subdecs[lvl](enc_z).view(enc_z.size(0), -1)\n",
    "                # print(enc_z.shape)\n",
    "                xz = torch.cat((x_lat, z), dim=1)\n",
    "                # print(xz.shape)\n",
    "                x = enc_z + xz\n",
    "                # print(\"ins 1: \", x.shape)\n",
    "        return self.x1, self.x2\n",
    "\n",
    "class Decoder(nn.Module): #use this one\n",
    "    def __init__(self, cfg, input_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self._config = cfg\n",
    "\n",
    "        self.n_latent_hierarchy_lvls=self._config.rbm.partitions\n",
    "\n",
    "        self.n_latent_nodes=self._config.rbm.latent_nodes_per_p * self._config.rbm.partitions\n",
    "\n",
    "        self.z = self._config.data.z\n",
    "        self.r = self._config.data.r\n",
    "        self.phi = self._config.data.phi\n",
    "\n",
    "        output_size_z = int( output_size / ( self.r * self.phi))\n",
    "\n",
    "        self._layers =  nn.Sequential(\n",
    "                   nn.Unflatten(1, (input_size, 1, 1, 1)),\n",
    "\n",
    "                   PeriodicConvTranspose3d(input_size, 512, (3,3,2), (2,1,1), 0),\n",
    "                   nn.BatchNorm3d(512),\n",
    "                   nn.PReLU(512, 0.02),\n",
    "                   \n",
    "\n",
    "                   PeriodicConvTranspose3d(512, 128, (5,4,2), (2,1,1), 0),\n",
    "                   nn.BatchNorm3d(128),\n",
    "                   nn.PReLU(128, 0.02),\n",
    "                                   )\n",
    "        \n",
    "        self._layers2 = nn.Sequential(\n",
    "                   PeriodicConvTranspose3d(129, 64, (3,3,2), (2,1,1), 1),\n",
    "                   nn.BatchNorm3d(64),\n",
    "                   nn.PReLU(64, 0.02),\n",
    "\n",
    "                   PeriodicConvTranspose3d(64, 32, (5,3,2), (2,1,1), 1),\n",
    "                   nn.BatchNorm3d(32),\n",
    "                   nn.PReLU(32, 1.0),\n",
    "\n",
    "                   PeriodicConvTranspose3d(32, 1, (5,3,3), (1,1,1), 0),\n",
    "                   PeriodicConv3d(1, 1, (self.z - output_size_z + 1, 1, 1), (1,1,1), 0),\n",
    "                   nn.PReLU(1, 1.0)\n",
    "                                   )\n",
    "        \n",
    "        self._layers3 = nn.Sequential(\n",
    "                   PeriodicConvTranspose3d(129, 64, (3,3,2), (2,1,1), 1),\n",
    "                   nn.BatchNorm3d(64),\n",
    "                   nn.PReLU(64, 0.02),\n",
    "\n",
    "                   PeriodicConvTranspose3d(64, 32, (5,3,2), (2,1,1), 1),\n",
    "                   nn.BatchNorm3d(32),\n",
    "                   nn.PReLU(32, 0.02),\n",
    "\n",
    "                   PeriodicConvTranspose3d(32, 1, (5,3,3), (1,1,1), 0),\n",
    "                   PeriodicConv3d(1, 1, (self.z - output_size_z + 1, 1, 1), (1,1,1), 0),\n",
    "                   nn.PReLU(1, 0.02),\n",
    "                                   )\n",
    "        \n",
    "    def forward(self, x, x0):\n",
    "                \n",
    "        x = self._layers(x)\n",
    "        x0 = self.trans_energy(x0)\n",
    "        xx0 = torch.cat((x, x0.unsqueeze(2).unsqueeze(3).unsqueeze(4).repeat(1,1,torch.tensor(x.shape[-3:-2]).item(),torch.tensor(x.shape[-2:-1]).item(), torch.tensor(x.shape[-1:]).item())), 1)\n",
    "        x1 = self._layers2(xx0) #hits\n",
    "        x2 = self._layers3(xx0)\n",
    "        return x1.reshape(x1.shape[0],-1), x2.reshape(x1.shape[0],-1)\n",
    "        # return x1, x2\n",
    "    \n",
    "    def trans_energy(self, x0, log_e_max=14.0, log_e_min=6.0, s_map = 1.0):\n",
    "        return ((torch.log(x0) - log_e_min)/(log_e_max - log_e_min)) * s_map\n",
    "    \n",
    "\n",
    "\n",
    "class PeriodicConvTranspose3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(PeriodicConvTranspose3d, self).__init__()\n",
    "        self.padding = padding\n",
    "        self.conv = nn.ConvTranspose3d(in_channels, out_channels, kernel_size, stride=stride, padding=0, dilation=dilation, groups=groups, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply convolution\n",
    "        x = self.conv(x)\n",
    "        # Pad input tensor with periodic boundary conditions\n",
    "        if self.padding == 1:\n",
    "            mid = x.shape[-2] // 2\n",
    "            shift = torch.cat((x[..., mid:, [0]], x[..., :mid, [0]]), -2)\n",
    "            x = torch.cat((shift,x), dim=-1)\n",
    "            x = F.pad(x, (0, 0, self.padding, self.padding, 0, 0), mode='circular')\n",
    "        return x\n",
    "    \n",
    "class PeriodicConv3d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        super(PeriodicConv3d, self).__init__()\n",
    "        self.padding = padding\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=0, dilation=dilation, groups=groups, bias=bias)\n",
    "    def forward(self, x):\n",
    "        # Pad input tensor with periodic boundary and circle-center conditions\n",
    "        if self.padding == 1:\n",
    "            mid = x.shape[-1] // 2\n",
    "            shift = torch.cat((x[..., [-1], mid:], x[..., [-1], :mid]), -1)\n",
    "            x = torch.cat((x, shift), dim=-2)\n",
    "        x = F.pad(x, (self.padding, self.padding, 0, 0, 0, 0), mode='circular')\n",
    "        # Apply convolution\n",
    "        x = self.conv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f74306",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.data.z = 45\n",
    "config.data.r = 9\n",
    "config.data.phi = 16\n",
    "d = Decoder(config, 1208, 2160)\n",
    "print(d(torch.rand(2,1208), torch.rand(2,1))[0].shape, d(torch.rand(2,1208), torch.rand(2,1))[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643dfb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "self.model.decoder(torch.rand(2,1208).to(self.device), torch.rand(2,1).to(self.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38fb06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.model.decoder_input\n",
    "self.model.decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f8e1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = d(torch.rand(2,1208), torch.rand(2,1))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e99e929",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.reshape(x.shape[0], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85448963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
