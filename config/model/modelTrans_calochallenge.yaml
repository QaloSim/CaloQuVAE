# This model configuration uses the hierarchical decoder instead of the standard decoder.
# This allows for more complex decoding processes, potentially improving the model's performance on certain tasks.
# The rest of the configuration remains unchanged from model2.yaml.
model_name: autoencoderbase
encoder: hierachicalencoder
encoderblock: CaloChallenge2
decoder: decoderhierachytf

decoder_input:
  - 1208
  - 1880
  - 1880
  - 1880

skip_output_size: 672
head_size: 32

# output_smoothing_fct: 60
mse_weight: 0.5
coefficient: 0.01

loss_coeff:
  ae_loss: 1.0
  kl_loss: 0.0
  hit_loss: 1.0
  entropy: 1.0
  pos_energy: 1.0
  neg_energy: 1.0